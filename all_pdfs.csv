"pdf","text"
"./01_DataScientistToolbox/lectures/01_01_seriesMotivation.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_01a_toolBoxOverview.pdf"," 
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_02_gettingHelp.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_03_findingAnswers.pdf"," 
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_04_RProgramming.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_05_gettingData.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_06_exploratoryAnalysis.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_07_reproducibleResearch.pdf"," 
  
  
 "
"./01_DataScientistToolbox/lectures/01_08_statisticalInference.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_09_regressionModels.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_10_practicalMachineLearning.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/01_11_buildingDataProducts.pdf"," 
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_03_commandLineInterface.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_04_git.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_05_github.pdf"," 
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_06_creatingRepos.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_07_basicGitCommands.pdf"," 
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_08_basicMarkdown.pdf"," 
  
  
 "
"./01_DataScientistToolbox/lectures/02_09_installingRPackages.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/02_10_rtools.pdf"," 
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/03_01_typesOfQuestions.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/03_02_whatIsData.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/03_03_whatAboutBigData.pdf"," 
  
  
  
  
  
  
  
 "
"./01_DataScientistToolbox/lectures/03_04_experimentalDesign.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./02_RProgramming/apply/apply.pdf","7/30/13                                                                       Introduction to the R Language
                          Introduction to the R Language
                          Loop Functions - apply
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                1/7
 
 7/30/13                                                                       Introduction to the R Language
              apply
              applyis used to a evaluate a function (often an anonymous one) over the margins of an array.
                 · It is most often used to apply a function to the rows or columns of a matrix
                 · It can be used with general arrays, e.g. taking the average of an array of matrices
                 · It is not really faster than writing a loop, but it works in one line!
                                                                                                             2/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                    2/7
 
 7/30/13                                                                       Introduction to the R Language
              apply
                 > str(apply)
                 function (X, MARGIN, FUN, ...)
                 · Xis an array
                 · MARGINis an integer vector indicating which margins should be “retained”.
                 · FUNis a function to be applied
                 · ... is for other arguments to be passed to FUN
                                                                                                             3/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                    3/7
 
 7/30/13                                                                       Introduction to the R Language
              apply
                 > x <- matrix(rnorm(200), 20, 10)
                 > apply(x, 2, mean)
                  [1] 0.04868268 0.35743615 -0.09104379
                  [4] -0.05381370 -0.16552070 -0.18192493
                  [7] 0.10285727 0.36519270 0.14898850
                 [10] 0.26767260
                 > apply(x, 1, sum)
                  [1] -1.94843314 2.60601195 1.51772391
                  [4] -2.80386816 3.73728682 -1.69371360
                  [7] 0.02359932 3.91874808 -2.39902859
                 [10] 0.48685925 -1.77576824 -3.34016277
                 [13] 4.04101009 0.46515429 1.83687755
                 [16] 4.36744690 2.21993789 2.60983764
                 [19] -1.48607630 3.58709251
                                                                                                             4/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                    4/7
 
 7/30/13                                                                       Introduction to the R Language
              col/row sums and means
              For sums and means of matrix dimensions, we have some shortcuts.
                 · rowSums= apply(x, 1, sum)
                 · rowMeans= apply(x, 1, mean)
                 · colSums= apply(x, 2, sum)
                 · colMeans= apply(x, 2, mean)
              The shortcut functions are much faster, but you won’t notice unless you’re using a large matrix.
                                                                                                               5/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                      5/7
 
 7/30/13                                                                              Introduction to the R Language
              Other Ways to Apply
              Quantiles of the rows of a matrix.
                 > x <- matrix(rnorm(200), 20, 10)
                 > apply(x, 1, quantile, probs = c(0.25, 0.75))
                                   [,1]                  [,2]                 [,3]       [,4]
                 25% -0.3304284 -0.99812467 -0.9186279 -0.49711686
                 75% 0.9258157 0.07065724 0.3050407 -0.06585436
                                     [,5]                [,6]                [,7]    [,8]
                 25% -0.05999553 -0.6588380 -0.653250 0.01749997
                 75% 0.52928743 0.3727449 1.255089 0.72318419
                                   [,9]               [,10]                [,11]    [,12]
                 25% -1.2467955 -0.8378429 -1.0488430 -0.7054902
                 75% 0.3352377 0.7297176 0.3113434 0.4581150
                                  [,13]               [,14]                [,15]    [,16]
                 25% -0.1895108 -0.5729407 -0.5968578 -0.9517069
                 75% 0.5326299 0.5064267 0.4933852 0.8868922
                                  [,17]               [,18]                [,19]   [,20]
                                                                                                                    6/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                           6/7
 
 7/30/13                                                                       Introduction to the R Language
              apply
              Average matrix in an array
                 > a <- array(rnorm(2 * 2 * 10), c(2, 2, 10))
                 > apply(a, c(1, 2), mean)
                                     [,1]                   [,2]
                 [1,] -0.2353245 -0.03980211
                 [2,] -0.3339748 0.04364908
                 > rowMeans(a, dims = 2)
                                     [,1]                   [,2]
                 [1,] -0.2353245 -0.03980211
                 [2,] -0.3339748 0.04364908
                                                                                                             7/7
file://localhost/Users/sean/Developer/GitHub/modules/roger/apply/index.html#1                                    7/7
"
"./02_RProgramming/assets/img/example10.pdf","             10
                        ●
             9                              ●
                            ●
                                ● ● ●
Expression
                  ● ●                                               ● ●
                                                ● ● ●
                                                        ●
                                                                                                                                      ●
             8
                                                                ●
                                        ●
                                                                              ●
                                                            ●                                                 ●
                                                                                                  ● ●                     ●
                                                                          ●                                       ●
                                                                                  ●                     ● ●                   ●
                                                                                                                                              ●
                                                                                      ●       ●                       ●                   ●
             7                                                                                                                    ●
                                                                                          ●
             6
                                                                      Array
"
"./02_RProgramming/assets/img/figure1final.pdf","                                      Expression
                                    Expression                           c                                         Expression
                                                                                                                 Expression          a
                 4     5        6      7           8    9       10         11                    6       8          10     12   14
                               !               !
                            !                      !
                           !                       !                 Batch 1
                           !                   !
Array                                                                      Sample
                                       !                    !
        Sample                                                                          Sample
                                      !                 !
                                           !                !
                                                                     Batch 2
                                               !                !
                                                                         d                                                           b
                                                                                                                   Expression
                                                                                                                 Expression
                                                                                                     4       6       8     10   12       14
                           Normal
                     Normal
                 Normal
                 Normal
                                                                               Sample   Sample
                                               Normal
                                Normal
            Normal
            Normal
"
"./02_RProgramming/ControlStructures/Introduction to the R Language.pdf","7/30/13                                                                           Introduction to the R Language
                          Introduction to the R Language
                          Control Structures
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                1/14
 
 7/30/13                                                                           Introduction to the R Language
              Control Structures
              Control structures in R allow you to control the flow of execution of the program, depending on runtime
              conditions. Common structures are
                 · if, else: testing a condition
                 · for: execute a loop a fixed number of times
                 · while: execute a loop while a condition is true
                 · repeat: execute an infinite loop
                 · break: break the execution of a loop
                 · next: skip an interation of a loop
                 · return: exit a function
              Most control structures are not used in interactive sessions, but rather when writing functions or longer
              expresisons.
                                                                                                                    2/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                        2/14
 
 7/30/13                                                                           Introduction to the R Language
              Control Structures: if
                 if(<condition>) {
                               ## do something
                 } else {
                               ## do something else
                 }
                 if(<condition1>) {
                               ## do something
                 } else if(<condition2>) {
                               ## do something different
                 } else {
                               ## do something different
                 }
                                                                                                                 3/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                     3/14
 
 7/30/13                                                                           Introduction to the R Language
              if
              This is a valid if/else structure.
                 if(x > 3) {
                               y <- 10
                 } else {
                               y <- 0
                 }
              So is this one.
                 y <- if(x > 3) {
                               10
                 } else {
                               0
                 }
                                                                                                                 4/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                     4/14
 
 7/30/13                                                                           Introduction to the R Language
              if
              Of course, the else clause is not necessary.
                 if(<condition1>) {
                 }
                 if(<condition2>) {
                 }
                                                                                                                 5/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                     5/14
 
 7/30/13                                                                           Introduction to the R Language
              for
              forloops take an interator variable and assign it successive values from a sequence or vector. For
              loops are most commonly used for iterating over the elements of an object (list, vector, etc.)
                 for(i in 1:10) {
                               print(i)
                 }
              This loop takes the ivariable and in each iteration of the loop gives it values 1, 2, 3, ..., 10, and then
              exits.
                                                                                                                     6/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                         6/14
 
 7/30/13                                                                           Introduction to the R Language
              for
              These three loops have the same behavior.
                 x <- c(""a"", ""b"", ""c"", ""d"")
                 for(i in 1:4) {
                               print(x[i])
                 }
                 for(i in seq_along(x)) {
                               print(x[i])
                 }
                 for(letter in x) {
                               print(letter)
                 }
                 for(i in 1:4) print(x[i])
                                                                                                                 7/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                     7/14
 
 7/30/13                                                                           Introduction to the R Language
              Nested for loops
              forloops can be nested.
                 x <- matrix(1:6, 2, 3)
                 for(i in seq_len(nrow(x))) {
                               for(j in seq_len(ncol(x))) {
                                              print(x[i, j])
                               }
                 }
              Be careful with nesting though. Nesting beyond 2–3 levels is often very difficult to read/understand.
                                                                                                                   8/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                       8/14
 
 7/30/13                                                                           Introduction to the R Language
              while
              While loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop
              body is executed, the condition is tested again, and so forth.
                 count <- 0
                 while(count < 10) {
                               print(count)
                               count <- count + 1
                 }
              While loops can potentially result in infinite loops if not written properly. Use with care!
                                                                                                                  9/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                      9/14
 
 7/30/13                                                                           Introduction to the R Language
              while
              Sometimes there will be more than one condition in the test.
                 z <- 5
                 while(z >= 3 && z <= 10) {
                               print(z)
                               coin <- rbinom(1, 1, 0.5)
                               if(coin == 1) { ## random walk
                                              z <- z + 1
                               } else {
                                              z <- z - 1
                               }
                 }
              Conditions are always evaluated from left to right.
                                                                                                                 10/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                      10/14
 
 7/30/13                                                                           Introduction to the R Language
              repeat
              Repeat initiates an infinite loop; these are not commonly used in statistical applications but they do
              have their uses. The only way to exit a repeatloop is to call break.
                 x0 <- 1
                 tol <- 1e-8
                 repeat {
                               x1 <- computeEstimate()
                               if(abs(x1 - x0) < tol) {
                                              break
                               } else {
                                              x0 <- x1
                               }
                 }
                                                                                                                 11/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                      11/14
 
 7/30/13                                                                           Introduction to the R Language
              repeat
              The loop in the previous slide is a bit dangerous because there’s no guarantee it will stop. Better to
              set a hard limit on the number of iterations (e.g. using a for loop) and then report whether convergence
              was achieved or not.
                                                                                                                  12/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                       12/14
 
 7/30/13                                                                           Introduction to the R Language
              next, return
              nextis used to skip an iteration of a loop
                 for(i in 1:100) {
                               if(i <= 20) {
                                              ## Skip the first 20 iterations
                                              next
                               }
                               ## Do something here
                 }
              returnsignals that a function should exit and return a given value
                                                                                                                 13/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                      13/14
 
 7/30/13                                                                           Introduction to the R Language
              Control Structures
              Summary
                 · Control structures like if, while, and forallow you to control the flow of an R program
                 · Infinite loops should generally be avoided, even if they are theoretically correct.
                 · Control structures mentiond here are primarily useful for writing programs; for command-line
                    interactive work, the *apply functions are more useful.
                                                                                                                 14/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 3/index.html#1                                      14/14
"
"./02_RProgramming/DataTypes/Introduction to the R Language.pdf","7/29/13                                                                            Introduction to the R Language
                          Introduction to the R Language
                          Data Types and Basic Operations
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                1/27
 
 7/29/13                                                                            Introduction to the R Language
              Objects
              R has five basic or “atomic” classes of objects:
                 · character
                 · numeric (real numbers)
                 · integer
                 · complex
                 · logical (True/False)
              The most basic object is a vector
                 · A vector can only contain objects of the same class
                 · BUT: The one exception is a list, which is represented as a vector but can contain objects of
                    different classes (indeed, that’s usually why we use them)
              Empty vectors can be created with the vector()function.
                                                                                                                  2/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     2/27
 
 7/29/13                                                                            Introduction to the R Language
              Numbers
                 · Numbers in R a generally treated as numeric objects (i.e. double precision real numbers)
                 · If you explicitly want an integer, you need to specify the Lsuffix
                 · Ex: Entering 1gives you a numeric object; entering 1Lexplicitly gives you an integer.
                 · There is also a special number Inf which represents infinity; e.g. 1 / 0; Inf can be used in
                    ordinary calculations; e.g. 1 / Infis 0
                 · The value NaNrepresents an undefined value (“not a number”); e.g. 0 / 0; NaNcan also be thought
                    of as a missing value (more on that later)
                                                                                                                  3/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     3/27
 
 7/29/13                                                                            Introduction to the R Language
              Attributes
              R objects can have attributes
                 · names, dimnames
                 · dimensions (e.g. matrices, arrays)
                 · class
                 · length
                 · other user-defined attributes/metadata
              Attributes of an object can be accessed using the attributes()function.
                                                                                                                  4/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     4/27
 
 7/29/13                                                                            Introduction to the R Language
              Entering Input
              At the R prompt we type expressions. The <-symbol is the assignment operator.
                 > x <- 1
                 > print(x)
                 [1] 1
                 >x
                 [1] 1
                 > msg <- ""hello""
              The grammar of the language determines whether an expression is complete or not.
                 > x <- ## Incomplete expression
              The # character indicates a comment. Anything to the right of the # (including the # itself) is ignored.
                                                                                                                     5/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                        5/27
 
 7/29/13                                                                            Introduction to the R Language
              Evaluation
              When a complete expression is entered at the prompt, it is evaluated and the result of the evaluated
              expression is returned. The result may be auto-printed.
                 > x <- 5 ## nothing printed
                 >x                ## auto-printing occurs
                 [1] 5
                 > print(x) ## explicit printing
                 [1] 5
              The [1]indicates that xis a vector and 5is the first element.
                                                                                                                  6/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     6/27
 
 7/29/13                                                                            Introduction to the R Language
              Printing
                 > x <- 1:20
                 >x
                  [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
                 [16] 16 17 18 19 20
              The :operator is used to create integer sequences.
                                                                                                                  7/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     7/27
 
 7/29/13                                                                            Introduction to the R Language
              Creating Vectors
              The c()function can be used to create vectors of objects.
                 > x <- c(0.5, 0.6)                            ## numeric
                 > x <- c(TRUE, FALSE)                         ## logical
                 > x <- c(T, F)                                ## logical
                 > x <- c(""a"", ""b"", ""c"")                       ## character
                 > x <- 9:29                                   ## integer
                 > x <- c(1+0i, 2+4i)                          ## complex
              Using the vector()function
                 > x <- vector(""numeric"", length = 10)
                 >x
                  [1] 0 0 0 0 0 0 0 0 0 0
                                                                                                                  8/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     8/27
 
 7/29/13                                                                            Introduction to the R Language
              Mixing Objects
              What about the following?
                 > y <- c(1.7, ""a"") ## character
                 > y <- c(TRUE, 2)                      ## numeric
                 > y <- c(""a"", TRUE) ## character
              When different objects are mixed in a vector, coercion occurs so that every element in the vector is of
              the same class.
                                                                                                                  9/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                     9/27
 
 7/29/13                                                                            Introduction to the R Language
              Explicit Coercion
              Objects can be explicitly coerced from one class to another using the as.*functions, if available.
                 > x <- 0:6
                 > class(x)
                 [1] ""integer""
                 > as.numeric(x)
                 [1] 0 1 2 3 4 5 6
                 > as.logical(x)
                 [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE
                 > as.character(x)
                 [1] ""0"" ""1"" ""2"" ""3"" ""4"" ""5"" ""6""
                                                                                                                  10/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      10/27
 
 7/29/13                                                                            Introduction to the R Language
              Explicit Coercion
              Nonsensical coercion results in NAs.
                 > x <- c(""a"", ""b"", ""c"")
                 > as.numeric(x)
                 [1] NA NA NA
                 Warning message:
                 NAs introduced by coercion
                 > as.logical(x)
                 [1] NA NA NA
                 > as.complex(x)
                 [1] 0+0i 1+0i 2+0i 3+0i 4+0i 5+0i 6+0i
                                                                                                                  11/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      11/27
 
 7/29/13                                                                            Introduction to the R Language
              Matrices
              Matrices are vectors with a dimension attribute. The dimension attribute is itself an integer vector of
              length 2 (nrow, ncol)
                 > m <- matrix(nrow = 2, ncol = 3)
                 >m
                          [,1] [,2] [,3]
                 [1,] NA NA NA
                 [2,] NA NA NA
                 > dim(m)
                 [1] 2 3
                 > attributes(m)
                 $dim
                 [1] 2 3
                                                                                                                  12/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      12/27
 
 7/29/13                                                                            Introduction to the R Language
              Matrices (cont’d)
              Matrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner
              and running down the columns.
                 > m <- matrix(1:6, nrow = 2, ncol = 3)
                 >m
                          [,1] [,2] [,3]
                 [1,]          1         3       5
                 [2,]          2         4       6
                                                                                                                  13/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      13/27
 
 7/29/13                                                                            Introduction to the R Language
              Matrices (cont’d)
              Matrices can also be created directly from vectors by adding a dimension attribute.
                 > m <- 1:10
                 >m
                 [1] 1 2 3 4 5 6 7 8 9 10
                 > dim(m) <- c(2, 5)
                 >m
                          [,1] [,2] [,3] [,4] [,5]
                 [1,]          1         3       5         7          9
                 [2,]          2         4       6         8 10
                                                                                                                  14/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      14/27
 
 7/29/13                                                                            Introduction to the R Language
              cbind-ing and rbind-ing
              Matrices can be created by column-binding or row-binding with cbind()and rbind().
                 > x <- 1:3
                 > y <- 10:12
                 > cbind(x, y)
                          x y
                 [1,] 1 10
                 [2,] 2 11
                 [3,] 3 12
                 > rbind(x, y)
                    [,1] [,2] [,3]
                 x        1        2        3
                 y 10 11 12
                                                                                                                  15/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      15/27
 
 7/29/13                                                                            Introduction to the R Language
              Lists
              Lists are a special type of vector that can contain elements of different classes. Lists are a very
              important data type in R and you should get to know them well.
                 > x <- list(1, ""a"", TRUE, 1 + 4i)
                 >x
                 [[1]]
                 [1] 1
                 [[2]]
                 [1] ""a""
                 [[3]]
                 [1] TRUE
                 [[4]]
                 [1] 1+4i
                                                                                                                  16/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      16/27
 
 7/29/13                                                                            Introduction to the R Language
              Factors
              Factors are used to represent categorical data. Factors can be unordered or ordered. One can think of
              a factor as an integer vector where each integer has a label.
                 · Factors are treated specially by modelling functions like lm()and glm()
                 · Using factors with labels is better than using integers because factors are self-describing; having a
                    variable that has values “Male” and “Female” is better than a variable that has values 1 and 2.
                                                                                                                    17/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                        17/27
 
 7/29/13                                                                            Introduction to the R Language
              Factors
                 > x <- factor(c(""yes"", ""yes"", ""no"", ""yes"", ""no""))
                 >x
                 [1] yes yes no yes no
                 Levels: no yes
                 > table(x)
                 x
                 no yes
                  2 3
                 > unclass(x)
                 [1] 2 2 1 2 1
                 attr(,""levels"")
                 [1] ""no"" ""yes""
                                                                                                                  18/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      18/27
 
 7/29/13                                                                            Introduction to the R Language
              Factors
              The order of the levels can be set using the levelsargument to factor(). This can be important in
              linear modelling because the first level is used as the baseline level.
                 > x <- factor(c(""yes"", ""yes"", ""no"", ""yes"", ""no""),
                                           levels = c(""yes"", ""no""))
                 >x
                 [1] yes yes no yes no
                 Levels: yes no
                                                                                                                  19/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      19/27
 
 7/29/13                                                                            Introduction to the R Language
              Missing Values
              Missing values are denoted by NAor NaNfor undefined mathematical operations.
                 · is.na()is used to test objects if they are NA
                 · is.nan()is used to test for NaN
                 · NAvalues have a class also, so there are integer NA, character NA, etc.
                 · A NaNvalue is also NAbut the converse is not true
                                                                                                                  20/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      20/27
 
 7/29/13                                                                            Introduction to the R Language
              Missing Values
                 > x <- c(1, 2, NA, 10, 3)
                 > is.na(x)
                 [1] FALSE FALSE TRUE FALSE FALSE
                 > is.nan(x)
                 [1] FALSE FALSE FALSE FALSE FALSE
                 > x <- c(1, 2, NaN, NA, 4)
                 > is.na(x)
                 [1] FALSE FALSE TRUE TRUE FALSE
                 > is.nan(x)
                 [1] FALSE FALSE TRUE FALSE FALSE
                                                                                                                  21/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      21/27
 
 7/29/13                                                                            Introduction to the R Language
              Data Frames
              Data frames are used to store tabular data
                 · They are represented as a special type of list where every element of the list has to have the same
                    length
                 · Each element of the list can be thought of as a column and the length of each element of the list is
                    the number of rows
                 · Unlike matrices, data frames can store different classes of objects in each column (just like lists);
                    matrices must have every element be the same class
                 · Data frames also have a special attribute called row.names
                 · Data frames are usually created by calling read.table()or read.csv()
                 · Can be converted to a matrix by calling data.matrix()
                                                                                                                   22/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                       22/27
 
 7/29/13                                                                            Introduction to the R Language
              Data Frames
                 > x <- data.frame(foo = 1:4, bar = c(T, T, F, F))
                 >x
                    foo bar
                 1 1 TRUE
                 2 2 TRUE
                 3 3 FALSE
                 4 4 FALSE
                 > nrow(x)
                 [1] 4
                 > ncol(x)
                 [1] 2
                                                                                                                  23/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      23/27
 
 7/29/13                                                                            Introduction to the R Language
              Names
              R objects can also have names, which is very useful for writing readable code and self-describing
              objects.
                 > x <- 1:3
                 > names(x)
                 NULL
                 > names(x) <- c(""foo"", ""bar"", ""norf"")
                 >x
                 foo bar norf
                    1 2              3
                 > names(x)
                 [1] ""foo"" ""bar"" ""norf""
                                                                                                                  24/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      24/27
 
 7/29/13                                                                            Introduction to the R Language
              Names
              Lists can also have names.
                 > x <- list(a = 1, b = 2, c = 3)
                 >x
                 $a
                 [1] 1
                 $b
                 [1] 2
                 $c
                 [1] 3
                                                                                                                  25/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      25/27
 
 7/29/13                                                                            Introduction to the R Language
              Names
              And matrices.
                 > m <- matrix(1:4, nrow = 2, ncol = 2)
                 > dimnames(m) <- list(c(""a"", ""b""), c(""c"", ""d""))
                 >m
                    cd
                 a13
                 b24
                                                                                                                  26/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      26/27
 
 7/29/13                                                                            Introduction to the R Language
              Summary
              Data Types
                 · atomic classes: numeric, logical, character, integer, complex \
                 · vectors, lists
                 · factors
                 · missing values
                 · data frames
                 · names
                                                                                                                  27/27
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2/index.html#27                                      27/27
"
"./02_RProgramming/Dates/Dates.pdf","Dates and Times in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Dates and Times in R
R has developed a special representation of dates and times
 · Dates are represented by the Date class
 · Times are represented by the POSIXct or the POSIXlt class
 · Dates are stored internally as the number of days since 1970-01-01
 · Tmes are stored internally as the number of seconds since 1970-01-01
                                                                        2/10
 
 Dates in R
Dates are represented by the Date class and can be coerced from a character string using the
as.Date() function.
 x <- as.Date(""1970-01-01"")
 x
 ## [1] ""1970-01-01""
 unclass(x)
 ## [1] 0
 unclass(as.Date(""1970-01-02""))
 ## [1] 1
                                                                                         3/10
 
 Times in R
Times are represented using the POSIXct or the POSIXlt class
 · POSIXct is just a very large integer under the hood; it use a useful class when you want to store
   times in something like a data frame
 · POSIXlt is a list underneath and it stores a bunch of other useful information like the day of the
   week, day of the year, month, day of the month
There are a number of generic functions that work on dates and times
 · weekdays: give the day of the week
 · months: give the month name
 · quarters: give the quarter number (“Q1”, “Q2”, “Q3”, or “Q4”)
                                                                                                  4/10
 
 Times in R
Times can be coerced from a character string using the as.POSIXlt or as.POSIXct function.
 x <- Sys.time()
 x
 ## [1] ""2013-01-24 22:04:14 EST""
 p <- as.POSIXlt(x)
 names(unclass(p))
 ## [1] ""sec""   ""min""   ""hour"" ""mday"" ""mon""
 ## [6] ""year"" ""wday"" ""yday"" ""isdst""
 p$sec
 ## [1] 14.34
                                                                                          5/10
 
 Times in R
You can also use the POSIXct format.
 x <- Sys.time()
 x ## Already in ‘POSIXct’ format
 ## [1] ""2013-01-24 22:04:14 EST""
 unclass(x)
 ## [1] 1359083054
 x$sec
 ## Error: $ operator is invalid for atomic vectors
 p <- as.POSIXlt(x)
 p$sec
 ## [1] 14.37
                                                    6/10
 
 Times in R
Finally, there is the strptime function in case your dates are written in a different format
  datestring <- c(""January 10, 2012 10:40"", ""December 9, 2011 9:10"")
  x <- strptime(datestring, ""%B %d, %Y %H:%M"")
  x
  ## [1] ""2012-01-10 10:40:00 EST"" ""2011-12-09 09:10:00 EST""
  class(x)
  ## [1] ""POSIXlt"" ""POSIXt""
I can never remember the formatting strings. Check ?strptime for details.
                                                                                             7/10
 
 Operations on Dates and Times
You can use mathematical operations on dates and times. Well, really just + and -. You can do
comparisons too (i.e. ==, <=)
 x <- as.Date(""2012-01-01"")
 y <- strptime(""9 Jan 2011 11:34:21"", ""%d %b %Y %H:%M:%S"")
 x-y
 ## Warning: Incompatible methods (""-.Date"",
 ## ""-.POSIXt"") for ""-""
 ## Error: non-numeric argument to binary operator
 x <- as.POSIXlt(x)
 x-y
 ## Time difference of 356.3 days
                                                                                          8/10
 
 Operations on Dates and Times
Even keeps track of leap years, leap seconds, daylight savings, and time zones.
 x <- as.Date(""2012-03-01"") y <- as.Date(""2012-02-28"")
 x-y
 ## Time difference of 2 days
 x <- as.POSIXct(""2012-10-25 01:00:00"")
 y <- as.POSIXct(""2012-10-25 06:00:00"", tz = ""GMT"")
 y-x
 ## Time difference of 1 hours
                                                                                9/10
 
 Summary
· Dates and times have special classes in R that allow for numerical and statistical calculations
· Dates use the Date class
· Times use the POSIXct and POSIXlt class
· Character strings can be coerced to Date/Time classes using the strptime function or the
  as.Date, as.POSIXlt, or as.POSIXct
                                                                                                  10/10
"
"./02_RProgramming/debugging/Debugging.pdf","Debugging
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Something’s Wrong!
Indications that something’s not right
 · message: A generic notification/diagnostic message produced by the message function;
   execution of the function continues
 · warning: An indication that something is wrong but not necessarily fatal; execution of the
   function continues; generated by the warning function
 · error: An indication that a fatal problem has occurred; execution stops; produced by the stop
   function
 · condition: A generic concept for indicating that something unexpected can occur; programmers
   can create their own conditions
                                                                                             2/15
 
 Something’s Wrong!
Warning
 log(-1)
 ## Warning: NaNs produced
 ## [1] NaN
                           3/15
 
 Something’s Wrong
printmessage <- function(x) {
        if(x > 0)
                print(""x is greater than zero"")
        else
                print(""x is less than or equal to zero"")
        invisible(x)
}
                                                         4/15
 
 Something’s Wrong
printmessage <- function(x) {
    if (x > 0)
        print(""x is greater than zero"") else print(""x is less than or equal to zero"")
    invisible(x)
}
printmessage(1)
## [1] ""x is greater than zero""
printmessage(NA)
## Error: missing value where TRUE/FALSE needed
                                                                                      5/15
 
 Something’s Wrong!
printmessage2 <- function(x) {
        if(is.na(x))
                print(""x is a missing value!"")
        else if(x > 0)
                print(""x is greater than zero"")
        else
                print(""x is less than or equal to zero"")
        invisible(x)
}
                                                         6/15
 
 Something’s Wrong!
printmessage2 <- function(x) {
    if (is.na(x))
        print(""x is a missing value!"") else if (x > 0)
        print(""x is greater than zero"") else print(""x is less than or equal to zero"")
    invisible(x)
}
x <- log(-1)
## Warning: NaNs produced
printmessage2(x)
## [1] ""x is a missing value!""
                                                                                      7/15
 
 Something’s Wrong!
How do you know that something is wrong with your function?
 · What was your input? How did you call the function?
 · What were you expecting? Output, messages, other results?
 · What did you get?
 · How does what you get differ from what you were expecting?
 · Were your expectations correct in the first place?
 · Can you reproduce the problem (exactly)?
                                                              8/15
 
 Debugging Tools in R
The primary tools for debugging functions in R are
 · traceback: prints out the function call stack after an error occurs; does nothing if there’s no error
 · debug: flags a function for “debug” mode which allows you to step through execution of a function
   one line at a time
 · browser: suspends the execution of a function wherever it is called and puts the function in
   debug mode
 · trace: allows you to insert debugging code into a function a specific places
 · recover: allows you to modify the error behavior so that you can browse the function call stack
These are interactive tools specifically designed to allow you to pick through a function. There’s also
the more blunt technique of inserting print/cat statements in the function.
                                                                                                    9/15
 
 traceback
 > mean(x)
 Error in mean(x) : object 'x' not found
 > traceback()
 1: mean(x)
 >
                                         10/15
 
 traceback
 > lm(y ~ x)
 Error in eval(expr, envir, enclos) : object ’y’ not found
 > traceback()
 7: eval(expr, envir, enclos)
 6: eval(predvars, data, env)
 5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
 4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
 3: eval(expr, envir, enclos)
 2: eval(mf, parent.frame())
 1: lm(y ~ x)
                                                                    11/15
 
 debug
> debug(lm)
> lm(y ~ x)
debugging in: lm(y ~ x)
debug: {
    ret.x <- x
    ret.y <- y
    cl <- match.call()
    ...
    if (!qr)
        z$qr <- NULL
    z
}
Browse[2]>
                        12/15
 
 debug
Browse[2]> n
debug: ret.x <- x
Browse[2]> n
debug: ret.y <- y
Browse[2]> n
debug: cl <- match.call()
Browse[2]> n
debug: mf <- match.call(expand.dots = FALSE)
Browse[2]> n
debug: m <- match(c(""formula"", ""data"", ""subset"", ""weights"", ""na.action"",
    ""offset""), names(mf), 0L)
                                                                         13/15
 
 recover
 > options(error = recover)
 > read.csv(""nosuchfile"")
 Error in file(file, ""rt"") : cannot open the connection
 In addition: Warning message:
 In file(file, ""rt"") :
   cannot open file ’nosuchfile’: No such file or directory
 Enter a frame number, or 0 to exit
 1: read.csv(""nosuchfile"")
 2: read.table(file = file, header = header, sep = sep, quote = quote, dec =
 3: file(file, ""rt"")
 Selection:
                                                                             14/15
 
 Debugging
Summary
 · There are three main indications of a problem/condition: message, warning, error
      - only an error is fatal
 · When analyzing a function with a problem, make sure you can reproduce the problem, clearly
   state your expectations and how the output differs from your expectation
 · Interactive debugging tools traceback, debug, browser, trace, and recover can be used to
   find problematic code in functions
 · Debugging tools are not a substitute for thinking!
                                                                                        15/15
"
"./02_RProgramming/functions/Functions.pdf","Functions
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Functions
Functions are created using the function() directive and are stored as R objects just like anything
else. In particular, they are R objects of class “function”.
 f <- function(<arguments>) {
          ## Do something interesting
 }
Functions in R are “first class objects”, which means that they can be treated much like any other R
object. Importantly,
 · Functions can be passed as arguments to other functions
 · Functions can be nested, so that you can define a function inside of another function
 · The return value of a function is the last expression in the function body to be evaluated.
                                                                                                 2/13
 
 Function Arguments
Functions have named arguments which potentially have default values.
 · The formal arguments are the arguments included in the function definition
 · The formals function returns a list of all the formal arguments of a function
 · Not every function call in R makes use of all the formal arguments
 · Function arguments can be missing or might have default values
                                                                                 3/13
 
 Argument Matching
R functions arguments can be matched positionally or by name. So the following calls to sd are all
equivalent
 > mydata <- rnorm(100)
 > sd(mydata)
 > sd(x = mydata)
 > sd(x = mydata, na.rm = FALSE)
 > sd(na.rm = FALSE, x = mydata)
 > sd(na.rm = FALSE, mydata)
Even though it’s legal, I don’t recommend messing around with the order of the arguments too much,
since it can lead to some confusion.
                                                                                               4/13
 
 Argument Matching
You can mix positional matching with matching by name. When an argument is matched by name, it
is “taken out” of the argument list and the remaining unnamed arguments are matched in the order
that they are listed in the function definition.
  > args(lm)
  function (formula, data, subset, weights, na.action,
            method = ""qr"", model = TRUE, x = FALSE,
            y = FALSE, qr = TRUE, singular.ok = TRUE,
            contrasts = NULL, offset, ...)
The following two calls are equivalent.
  lm(data = mydata, y ~ x, model = FALSE, 1:100)
  lm(y ~ x, mydata, 1:100, model = FALSE)
                                                                                             5/13
 
 Argument Matching
· Most of the time, named arguments are useful on the command line when you have a long
  argument list and you want to use the defaults for everything except for an argument near the
  end of the list
· Named arguments also help if you can remember the name of the argument and not its position
  on the argument list (plotting is a good example).
                                                                                            6/13
 
 Argument Matching
Function arguments can also be partially matched, which is useful for interactive work. The order of
operations when given an argument is
  1. Check for exact match for a named argument
  2. Check for a partial match
  3. Check for a positional match
                                                                                                7/13
 
 Defining a Function
 f <- function(a, b = 1, c = 2, d = NULL) {
 }
In addition to not specifying a default value, you can also set an argument value to NULL.
                                                                                           8/13
 
 Lazy Evaluation
Arguments to functions are evaluated lazily, so they are evaluated only as needed.
 f <- function(a, b) {
      a^2
 }
 f(2)
 ## [1] 4
This function never actually uses the argument b, so calling f(2) will not produce an error because
the 2 gets positionally matched to a.
                                                                                                9/13
 
 Lazy Evaluation
 f <- function(a, b) {
       print(a)
       print(b)
 }
 f(45)
 ## [1] 45
 ## Error: argument ""b"" is missing, with no default
Notice that “45” got printed first before the error was triggered. This is because b did not have to be
evaluated until after print(a). Once the function tried to evaluate print(b) it had to throw an
error.
                                                                                                   10/13
 
 The “...” Argument
The ... argument indicate a variable number of arguments that are usually passed on to other
functions.
 · ... is often used when extending another function and you don’t want to copy the entire argument
   list of the original function
 myplot <- function(x, y, type = ""l"", ...) {
            plot(x, y, type = type, ...)
 }
 · Generic functions use ... so that extra arguments can be passed to methods (more on this later).
 > mean
 function (x, ...)
 UseMethod(""mean"")
                                                                                               11/13
 
 The “...” Argument
The ... argument is also necessary when the number of arguments passed to the function cannot be
known in advance.
 > args(paste)
 function (..., sep = "" "", collapse = NULL)
 > args(cat)
 function (..., file = """", sep = "" "", fill = FALSE,
     labels = NULL, append = FALSE)
                                                                                            12/13
 
 Arguments Coming After the “...” Argument
One catch with ... is that any arguments that appear after ... on the argument list must be named
explicitly and cannot be partially matched.
 > args(paste)
 function (..., sep = "" "", collapse = NULL)
 > paste(""a"", ""b"", sep = "":"")
 [1] ""a:b""
 > paste(""a"", ""b"", se = "":"")
 [1] ""a b :""
                                                                                              13/13
"
"./02_RProgramming/lapply/lappy.pdf","Introduction to the R Language
Loop Functions
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Looping on the Command Line
Writing for, while loops is useful when programming but not particularly easy when working
interactively on the command line. There are some functions which implement looping to make life
easier.
 · lapply: Loop over a list and evaluate a function on each element
 · sapply: Same as lapply but try to simplify the result
 · apply: Apply a function over the margins of an array
 · tapply: Apply a function over subsets of a vector
 · mapply: Multivariate version of lapply
An auxiliary function split is also useful, particularly in conjunction with lapply.
                                                                                           2/12
 
 lapply
lapply takes three arguments: (1) a list X; (2) a function (or the name of a function) FUN; (3) other
arguments via its ... argument. If X is not a list, it will be coerced to a list using as.list.
 lapply
 ## function (X, FUN, ...)
 ## {
 ##     FUN <- match.fun(FUN)
 ##     if (!is.vector(X) || is.object(X))
 ##          X <- as.list(X)
 ##     .Internal(lapply(X, FUN))
 ## }
 ## <bytecode: 0x7ff7a1951c00>
 ## <environment: namespace:base>
The actual looping is done internally in C code.
                                                                                                  3/12
 
 lapply
lapply always returns a list, regardless of the class of the input.
 x <- list(a = 1:5, b = rnorm(10))
 lapply(x, mean)
 ## $a
 ## [1] 3
 ##
 ## $b
 ## [1] 0.4671
                                                                    4/12
 
 lapply
 x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
 lapply(x, mean)
 ## $a
 ## [1] 2.5
 ##
 ## $b
 ## [1] 0.5261
 ##
 ## $c
 ## [1] 1.421
 ##
 ## $d
 ## [1] 4.927
                                                                        5/12
 
 lapply
 > x <- 1:4
 > lapply(x, runif)
 [[1]]
 [1] 0.2675082
 [[2]]
 [1] 0.2186453 0.5167968
 [[3]]
 [1] 0.2689506 0.1811683 0.5185761
 [[4]]
 [1] 0.5627829 0.1291569 0.2563676 0.7179353
                                             6/12
 
 lapply
 > x <- 1:4
 > lapply(x, runif, min = 0, max = 10)
 [[1]]
 [1] 3.302142
 [[2]]
 [1] 6.848960 7.195282
 [[3]]
 [1] 3.5031416 0.8465707 9.7421014
 [[4]]
 [1] 1.195114 3.594027 2.930794 2.766946
                                         7/12
 
 lapply
lapply and friends make heavy use of anonymous functions.
 > x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2))
 > x
 $a
      [,1] [,2]
 [1,]    1    3
 [2,]    2    4
 $b
      [,1] [,2]
 [1,]    1    4
 [2,]    2    5
 [3,]    3    6
                                                           8/12
 
 lapply
An anonymous function for extracting the first column of each matrix.
 > lapply(x, function(elt) elt[,1])
 $a
 [1] 1 2
 $b
 [1] 1 2 3
                                                                      9/12
 
 sapply
sapply will try to simplify the result of lapply if possible.
 · If the result is a list where every element is length 1, then a vector is returned
 · If the result is a list where every element is a vector of the same length (> 1), a matrix is returned.
 · If it can’t figure things out, a list is returned
                                                                                                      10/12
 
 sapply
 > x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
 > lapply(x, mean)
 $a
 [1] 2.5
 $b
 [1] 0.06082667
 $c
 [1] 1.467083
 $d
 [1] 5.074749
                                                                          11/12
 
 sapply
 > sapply(x, mean)
          a          b          c          d
 2.50000000 0.06082667 1.46708277 5.07474950
 > mean(x)
 [1] NA
 Warning message:
 In mean.default(x) : argument is not numeric or logical: returning NA
                                                                       12/12
"
"./02_RProgramming/lectures/apply.pdf","Introduction to the R Language
Loop Functions - apply
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 apply
apply is used to a evaluate a function (often an anonymous one) over the margins of an array.
 · It is most often used to apply a function to the rows or columns of a matrix
 · It can be used with general arrays, e.g. taking the average of an array of matrices
 · It is not really faster than writing a loop, but it works in one line!
                                                                                              2/7
 
 apply
> str(apply)
function (X, MARGIN, FUN, ...)
· X is an array
· MARGIN is an integer vector indicating which margins should be “retained”.
· FUN is a function to be applied
· ... is for other arguments to be passed to FUN
                                                                             3/7
 
 apply
> x <- matrix(rnorm(200), 20, 10)
> apply(x, 2, mean)
 [1] 0.04868268 0.35743615 -0.09104379
 [4] -0.05381370 -0.16552070 -0.18192493
 [7] 0.10285727 0.36519270 0.14898850
[10] 0.26767260
> apply(x, 1, sum)
 [1] -1.94843314 2.60601195 1.51772391
 [4] -2.80386816 3.73728682 -1.69371360
 [7] 0.02359932 3.91874808 -2.39902859
[10] 0.48685925 -1.77576824 -3.34016277
[13] 4.04101009 0.46515429 1.83687755
[16] 4.36744690 2.21993789 2.60983764
[19] -1.48607630 3.58709251
                                         4/7
 
 col/row sums and means
For sums and means of matrix dimensions, we have some shortcuts.
 · rowSums = apply(x, 1, sum)
 · rowMeans = apply(x, 1, mean)
 · colSums = apply(x, 2, sum)
 · colMeans = apply(x, 2, mean)
The shortcut functions are much faster, but you won’t notice unless you’re using a large matrix.
                                                                                                 5/7
 
 Other Ways to Apply
Quantiles of the rows of a matrix.
 > x <- matrix(rnorm(200), 20, 10)
 > apply(x, 1, quantile, probs = c(0.25, 0.75))
            [,1]         [,2]        [,3]     [,4]
 25% -0.3304284 -0.99812467 -0.9186279 -0.49711686
 75% 0.9258157 0.07065724 0.3050407 -0.06585436
             [,5]        [,6]       [,7]    [,8]
 25% -0.05999553 -0.6588380 -0.653250 0.01749997
 75% 0.52928743 0.3727449 1.255089 0.72318419
            [,9]       [,10]       [,11]   [,12]
 25% -1.2467955 -0.8378429 -1.0488430 -0.7054902
 75% 0.3352377 0.7297176 0.3113434 0.4581150
          [,13]        [,14]       [,15]   [,16]
 25% -0.1895108 -0.5729407 -0.5968578 -0.9517069
 75% 0.5326299 0.5064267 0.4933852 0.8868922
          [,17]        [,18]       [,19]  [,20]
                                                   6/7
 
 apply
Average matrix in an array
 > a <- array(rnorm(2 * 2 * 10), c(2, 2, 10))
 > apply(a, c(1, 2), mean)
            [,1]         [,2]
 [1,] -0.2353245 -0.03980211
 [2,] -0.3339748 0.04364908
 > rowMeans(a, dims = 2)
            [,1]         [,2]
 [1,] -0.2353245 -0.03980211
 [2,] -0.3339748 0.04364908
                                              7/7
"
"./02_RProgramming/lectures/CodingStandard.pdf","Coding Standards for R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Coding Standards for R
 1. Always use text files / text editor
                                        2/6
 
 Coding Standards for R
 1. Always use text files / text editor
 2. Indent your code
                                        3/6
 
 Coding Standards for R
 1. Always use text files / text editor
 2. Indent your code
 3. Limit the width of your code (80 columns?)
                                               4/6
 
 Indenting
 · Indenting improves readability
 · Fixing line length (80 columns) prevents lots of nesting and very long functions
 · Suggested: Indents of 4 spaces at minimum; 8 spaces ideal
                                                                                    5/6
 
 Coding Standards for R
 1. Always use text files / text editor
 2. Indent your code
 3. Limit the width of your code (80 columns?)
 4. Limit the length of individual functions
                                               6/6
"
"./02_RProgramming/lectures/ControlStructures.pdf","Introduction to the R Language
Control Structures
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 Control Structures
Control structures in R allow you to control the flow of execution of the program, depending on
runtime conditions. Common structures are
 · if, else: testing a condition
 · for: execute a loop a fixed number of times
 · while: execute a loop while a condition is true
 · repeat: execute an infinite loop
 · break: break the execution of a loop
 · next: skip an interation of a loop
 · return: exit a function
Most control structures are not used in interactive sessions, but rather when writing functions or
longer expresisons.
                                                                                               2/14
 
 Control Structures: if
if(<condition>) {
        ## do something
} else {
        ## do something else
}
if(<condition1>) {
        ## do something
} else if(<condition2>) {
        ## do something different
} else {
        ## do something different
}
                                  3/14
 
 if
This is a valid if/else structure.
 if(x > 3) {
           y <- 10
 } else {
           y <- 0
 }
So is this one.
 y <- if(x > 3) {
           10
 } else {
           0
 }
                                   4/14
 
 if
Of course, the else clause is not necessary.
 if(<condition1>) {
 }
 if(<condition2>) {
 }
                                             5/14
 
 for
for loops take an interator variable and assign it successive values from a sequence or vector. For
loops are most commonly used for iterating over the elements of an object (list, vector, etc.)
 for(i in 1:10) {
          print(i)
 }
This loop takes the i variable and in each iteration of the loop gives it values 1, 2, 3, ..., 10, and then
exits.
                                                                                                        6/14
 
 for
These three loops have the same behavior.
 x <- c(""a"", ""b"", ""c"", ""d"")
 for(i in 1:4) {
         print(x[i])
 }
 for(i in seq_along(x)) {
         print(x[i])
 }
 for(letter in x) {
         print(letter)
 }
 for(i in 1:4) print(x[i])
                                          7/14
 
 Nested for loops
for loops can be nested.
 x <- matrix(1:6, 2, 3)
 for(i in seq_len(nrow(x))) {
         for(j in seq_len(ncol(x))) {
                  print(x[i, j])
         }
 }
Be careful with nesting though. Nesting beyond 2–3 levels is often very difficult to read/understand.
                                                                                                   8/14
 
 while
While loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop
body is executed, the condition is tested again, and so forth.
 count <- 0
 while(count < 10) {
          print(count)
          count <- count + 1
 }
While loops can potentially result in infinite loops if not written properly. Use with care!
                                                                                                    9/14
 
 while
Sometimes there will be more than one condition in the test.
 z <- 5
 while(z >= 3 && z <= 10) {
         print(z)
         coin <- rbinom(1, 1, 0.5)
         if(coin == 1) { ## random walk
                  z <- z + 1
         } else {
                  z <- z - 1
         }
 }
Conditions are always evaluated from left to right.
                                                             10/14
 
 repeat
Repeat initiates an infinite loop; these are not commonly used in statistical applications but they do
have their uses. The only way to exit a repeat loop is to call break.
 x0 <- 1
 tol <- 1e-8
 repeat {
          x1 <- computeEstimate()
          if(abs(x1 - x0) < tol) {
                  break
          } else {
                  x0 <- x1
          }
 }
                                                                                                  11/14
 
 repeat
The loop in the previous slide is a bit dangerous because there’s no guarantee it will stop. Better to
set a hard limit on the number of iterations (e.g. using a for loop) and then report whether
convergence was achieved or not.
                                                                                                 12/14
 
 next, return
next is used to skip an iteration of a loop
 for(i in 1:100) {
         if(i <= 20) {
                  ## Skip the first 20 iterations
                  next
         }
         ## Do something here
 }
return signals that a function should exit and return a given value
                                                                    13/14
 
 Control Structures
Summary
 · Control structures like if, while, and for allow you to control the flow of an R program
 · Infinite loops should generally be avoided, even if they are theoretically correct.
 · Control structures mentiond here are primarily useful for writing programs; for command-line
   interactive work, the *apply functions are more useful.
                                                                                            14/14
"
"./02_RProgramming/lectures/DataTypes.pdf","Introduction to the R Language
Data Types and Basic Operations
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 Objects
R has five basic or “atomic” classes of objects:
 · character
 · numeric (real numbers)
 · integer
 · complex
 · logical (True/False)
The most basic object is a vector
 · A vector can only contain objects of the same class
 · BUT: The one exception is a list, which is represented as a vector but can contain objects of
   different classes (indeed, that’s usually why we use them)
Empty vectors can be created with the vector() function.
                                                                                            2/27
 
 Numbers
· Numbers in R a generally treated as numeric objects (i.e. double precision real numbers)
· If you explicitly want an integer, you need to specify the L suffix
· Ex: Entering 1 gives you a numeric object; entering 1L explicitly gives you an integer.
· There is also a special number Inf which represents infinity; e.g. 1 / 0; Inf can be used in
  ordinary calculations; e.g. 1 / Inf is 0
· The value NaN represents an undefined value (“not a number”); e.g. 0 / 0; NaN can also be
  thought of as a missing value (more on that later)
                                                                                           3/27
 
 Attributes
R objects can have attributes
 · names, dimnames
 · dimensions (e.g. matrices, arrays)
 · class
 · length
 · other user-defined attributes/metadata
Attributes of an object can be accessed using the attributes() function.
                                                                         4/27
 
 Entering Input
At the R prompt we type expressions. The <- symbol is the assignment operator.
 > x <- 1
 > print(x)
 [1] 1
 > x
 [1] 1
 > msg <- ""hello""
The grammar of the language determines whether an expression is complete or not.
 > x <-   ## Incomplete expression
The # character indicates a comment. Anything to the right of the # (including the # itself) is ignored.
                                                                                                     5/27
 
 Evaluation
When a complete expression is entered at the prompt, it is evaluated and the result of the evaluated
expression is returned. The result may be auto-printed.
 > x <- 5 ## nothing printed
 > x        ## auto-printing occurs
 [1] 5
 > print(x) ## explicit printing
 [1] 5
The [1] indicates that x is a vector and 5 is the first element.
                                                                                                 6/27
 
 Printing
 > x <- 1:20
 > x
  [1] 1 2 3 4 5        6   7  8   9 10 11 12 13 14 15
 [16] 16 17 18 19 20
The : operator is used to create integer sequences.
                                                      7/27
 
 Creating Vectors
The c() function can be used to create vectors of objects.
 > x <- c(0.5, 0.6)       ##  numeric
 > x <- c(TRUE, FALSE)    ##  logical
 > x <- c(T, F)           ##  logical
 > x <- c(""a"", ""b"", ""c"")  ##  character
 > x <- 9:29              ##  integer
 > x <- c(1+0i, 2+4i)     ##  complex
Using the vector() function
 > x <- vector(""numeric"", length = 10)
 > x
  [1] 0 0 0 0 0 0 0 0 0 0
                                                           8/27
 
 Mixing Objects
What about the following?
 > y <- c(1.7, ""a"")    ## character
 > y <- c(TRUE, 2)     ## numeric
 > y <- c(""a"", TRUE)   ## character
When different objects are mixed in a vector, coercion occurs so that every element in the vector is
of the same class.
                                                                                                9/27
 
 Explicit Coercion
Objects can be explicitly coerced from one class to another using the as.* functions, if available.
 > x <- 0:6
 > class(x)
 [1] ""integer""
 > as.numeric(x)
 [1] 0 1 2 3 4 5 6
 > as.logical(x)
 [1] FALSE TRUE TRUE TRUE TRUE         TRUE  TRUE
 > as.character(x)
 [1] ""0"" ""1"" ""2"" ""3"" ""4"" ""5"" ""6""
                                                                                                  10/27
 
 Explicit Coercion
Nonsensical coercion results in NAs.
 > x <- c(""a"", ""b"", ""c"")
 > as.numeric(x)
 [1] NA NA NA
 Warning message:
 NAs introduced by coercion
 > as.logical(x)
 [1] NA NA NA
 > as.complex(x)
 [1] 0+0i 1+0i 2+0i 3+0i 4+0i 5+0i 6+0i
                                        11/27
 
 Matrices
Matrices are vectors with a dimension attribute. The dimension attribute is itself an integer vector of
length 2 (nrow, ncol)
 > m <- matrix(nrow = 2, ncol = 3)
 > m
       [,1] [,2] [,3]
 [1,]    NA   NA   NA
 [2,]    NA   NA   NA
 > dim(m)
 [1] 2 3
 > attributes(m)
 $dim
 [1] 2 3
                                                                                                  12/27
 
 Matrices (cont’d)
Matrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner
and running down the columns.
 > m <- matrix(1:6, nrow = 2, ncol = 3)
 > m
      [,1] [,2] [,3]
 [1,]    1     3    5
 [2,]    2     4    6
                                                                                                  13/27
 
 Matrices (cont’d)
Matrices can also be created directly from vectors by adding a dimension attribute.
 > m <- 1:10
 > m
 [1] 1 2 3 4 5 6 7 8 9 10
 > dim(m) <- c(2, 5)
 > m
      [,1] [,2] [,3] [,4] [,5]
 [1,]    1     3    5    7     9
 [2,]    2     4    6    8   10
                                                                                    14/27
 
 cbind-ing and rbind-ing
Matrices can be created by column-binding or row-binding with cbind() and rbind().
 > x <- 1:3
 > y <- 10:12
 > cbind(x, y)
      x y
 [1,] 1 10
 [2,] 2 11
 [3,] 3 12
 > rbind(x, y)
   [,1] [,2] [,3]
 x    1    2     3
 y   10   11   12
                                                                                   15/27
 
 Lists
Lists are a special type of vector that can contain elements of different classes. Lists are a very
important data type in R and you should get to know them well.
 > x <- list(1, ""a"", TRUE, 1 + 4i)
 > x
 [[1]]
 [1] 1
 [[2]]
 [1] ""a""
 [[3]]
 [1] TRUE
 [[4]]
 [1] 1+4i
                                                                                               16/27
 
 Factors
Factors are used to represent categorical data. Factors can be unordered or ordered. One can think
of a factor as an integer vector where each integer has a label.
 · Factors are treated specially by modelling functions like lm() and glm()
 · Using factors with labels is better than using integers because factors are self-describing; having
    a variable that has values “Male” and “Female” is better than a variable that has values 1 and 2.
                                                                                                  17/27
 
 Factors
> x <- factor(c(""yes"", ""yes"", ""no"", ""yes"", ""no""))
> x
[1] yes yes no yes no
Levels: no yes
> table(x)
x
no yes
 2   3
> unclass(x)
[1] 2 2 1 2 1
attr(,""levels"")
[1] ""no"" ""yes""
                                                  18/27
 
 Factors
The order of the levels can be set using the levels argument to factor(). This can be important
in linear modelling because the first level is used as the baseline level.
 > x <- factor(c(""yes"", ""yes"", ""no"", ""yes"", ""no""),
                 levels = c(""yes"", ""no""))
 > x
 [1] yes yes no yes no
 Levels: yes no
                                                                                          19/27
 
 Missing Values
Missing values are denoted by NA or NaN for undefined mathematical operations.
· is.na() is used to test objects if they are NA
· is.nan() is used to test for NaN
· NA values have a class also, so there are integer NA, character NA, etc.
· A NaN value is also NA but the converse is not true
                                                                               20/27
 
 Missing Values
> x <- c(1, 2, NA, 10, 3)
> is.na(x)
[1] FALSE FALSE TRUE FALSE  FALSE
> is.nan(x)
[1] FALSE FALSE FALSE FALSE FALSE
> x <- c(1, 2, NaN, NA, 4)
> is.na(x)
[1] FALSE FALSE TRUE TRUE   FALSE
> is.nan(x)
[1] FALSE FALSE TRUE FALSE  FALSE
                                  21/27
 
 Data Frames
Data frames are used to store tabular data
 · They are represented as a special type of list where every element of the list has to have the
   same length
 · Each element of the list can be thought of as a column and the length of each element of the list
   is the number of rows
 · Unlike matrices, data frames can store different classes of objects in each column (just like lists);
   matrices must have every element be the same class
 · Data frames also have a special attribute called row.names
 · Data frames are usually created by calling read.table() or read.csv()
 · Can be converted to a matrix by calling data.matrix()
                                                                                                   22/27
 
 Data Frames
> x <- data.frame(foo = 1:4, bar = c(T, T, F, F))
> x
  foo   bar
1   1 TRUE
2   2 TRUE
3   3 FALSE
4   4 FALSE
> nrow(x)
[1] 4
> ncol(x)
[1] 2
                                                  23/27
 
 Names
R objects can also have names, which is very useful for writing readable code and self-describing
objects.
 > x <- 1:3
 > names(x)
 NULL
 > names(x) <- c(""foo"", ""bar"", ""norf"")
 > x
 foo bar norf
   1    2   3
 > names(x)
 [1] ""foo"" ""bar"" ""norf""
                                                                                             24/27
 
 Names
Lists can also have names.
 > x <- list(a = 1, b = 2, c = 3)
 > x
 $a
 [1] 1
 $b
 [1] 2
 $c
 [1] 3
                                  25/27
 
 Names
And matrices.
 > m <- matrix(1:4, nrow = 2, ncol = 2)
 > dimnames(m) <- list(c(""a"", ""b""), c(""c"", ""d""))
 > m
   c d
 a 1 3
 b 2 4
                                                 26/27
 
 Summary
Data Types
 · atomic classes: numeric, logical, character, integer, complex \
 · vectors, lists
 · factors
 · missing values
 · data frames
 · names
                                                                   27/27
"
"./02_RProgramming/lectures/Dates.pdf","Dates and Times in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Dates and Times in R
R has developed a special representation of dates and times
 · Dates are represented by the Date class
 · Times are represented by the POSIXct or the POSIXlt class
 · Dates are stored internally as the number of days since 1970-01-01
 · Tmes are stored internally as the number of seconds since 1970-01-01
                                                                        2/10
 
 Dates in R
Dates are represented by the Date class and can be coerced from a character string using the
as.Date() function.
 x <- as.Date(""1970-01-01"")
 x
 ## [1] ""1970-01-01""
 unclass(x)
 ## [1] 0
 unclass(as.Date(""1970-01-02""))
 ## [1] 1
                                                                                         3/10
 
 Times in R
Times are represented using the POSIXct or the POSIXlt class
 · POSIXct is just a very large integer under the hood; it use a useful class when you want to store
   times in something like a data frame
 · POSIXlt is a list underneath and it stores a bunch of other useful information like the day of the
   week, day of the year, month, day of the month
There are a number of generic functions that work on dates and times
 · weekdays: give the day of the week
 · months: give the month name
 · quarters: give the quarter number (“Q1”, “Q2”, “Q3”, or “Q4”)
                                                                                                  4/10
 
 Times in R
Times can be coerced from a character string using the as.POSIXlt or as.POSIXct function.
 x <- Sys.time()
 x
 ## [1] ""2013-01-24 22:04:14 EST""
 p <- as.POSIXlt(x)
 names(unclass(p))
 ## [1] ""sec""   ""min""   ""hour"" ""mday"" ""mon""
 ## [6] ""year"" ""wday"" ""yday"" ""isdst""
 p$sec
 ## [1] 14.34
                                                                                          5/10
 
 Times in R
You can also use the POSIXct format.
 x <- Sys.time()
 x ## Already in ‘POSIXct’ format
 ## [1] ""2013-01-24 22:04:14 EST""
 unclass(x)
 ## [1] 1359083054
 x$sec
 ## Error: $ operator is invalid for atomic vectors
 p <- as.POSIXlt(x)
 p$sec
 ## [1] 14.37
                                                    6/10
 
 Times in R
Finally, there is the strptime function in case your dates are written in a different format
  datestring <- c(""January 10, 2012 10:40"", ""December 9, 2011
  x <- strptime(datestring, ""%B %d, %Y %H:%M"")
  x
  ## [1] ""2012-01-10 10:40:00"" ""2011-12-09 09:10:00""
  class(x)
  ## [1] ""POSIXlt"" ""POSIXt""
I can never remember the formatting strings. Check ?strptime for details.
                                                                                             7/10
 
 Operations on Dates and Times
You can use mathematical operations on dates and times. Well, really just + and -. You can do
comparisons too (i.e. ==, <=)
 x <- as.Date(""2012-01-01"")
 y <- strptime(""9 Jan 2011 11:34:21"", ""%d %b %Y %H:%M:%S"")
 x-y
 ## Warning: Incompatible methods (""-.Date"",
 ## ""-.POSIXt"") for ""-""
 ## Error: non-numeric argument to binary operator
 x <- as.POSIXlt(x)
 x-y
 ## Time difference of 356.3 days
                                                                                          8/10
 
 Operations on Dates and Times
Even keeps track of leap years, leap seconds, daylight savings, and time zones.
 x <- as.Date(""2012-03-01"") y <- as.Date(""2012-02-28"")
 x-y
 ## Time difference of 2 days
 x <- as.POSIXct(""2012-10-25 01:00:00"")
 y <- as.POSIXct(""2012-10-25 06:00:00"", tz = ""GMT"")
 y-x
 ## Time difference of 1 hours
                                                                                9/10
 
 Summary
· Dates and times have special classes in R that allow for numerical and statistical calculations
· Dates use the Date class
· Times use the POSIXct and POSIXlt class
· Character strings can be coerced to Date/Time classes using the strptime function or the
  as.Date, as.POSIXlt, or as.POSIXct
                                                                                                  10/10
"
"./02_RProgramming/lectures/debugging.pdf","Debugging
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Something’s Wrong!
Indications that something’s not right
 · message: A generic notification/diagnostic message produced by the message function;
   execution of the function continues
 · warning: An indication that something is wrong but not necessarily fatal; execution of the
   function continues; generated by the warning function
 · error: An indication that a fatal problem has occurred; execution stops; produced by the stop
   function
 · condition: A generic concept for indicating that something unexpected can occur; programmers
   can create their own conditions
                                                                                             2/15
 
 Something’s Wrong!
Warning
 log(-1)
 ## Warning: NaNs produced
 ## [1] NaN
                           3/15
 
 Something’s Wrong
printmessage <- function(x) {
        if(x > 0)
                print(""x is greater than zero"")
        else
                print(""x is less than or equal to zero"")
        invisible(x)
}
                                                         4/15
 
 Something’s Wrong
printmessage <- function(x) {
    if (x > 0)
        print(""x is greater than zero"") else print(""x is less than or equal to zero"")
    invisible(x)
}
printmessage(1)
## [1] ""x is greater than zero""
printmessage(NA)
## Error: missing value where TRUE/FALSE needed
                                                                                      5/15
 
 Something’s Wrong!
printmessage2 <- function(x) {
        if(is.na(x))
                print(""x is a missing value!"")
        else if(x > 0)
                print(""x is greater than zero"")
        else
                print(""x is less than or equal to zero"")
        invisible(x)
}
                                                         6/15
 
 Something’s Wrong!
printmessage2 <- function(x) {
    if (is.na(x))
        print(""x is a missing value!"") else if (x > 0)
        print(""x is greater than zero"") else print(""x is less than or equal to zero"")
    invisible(x)
}
x <- log(-1)
## Warning: NaNs produced
printmessage2(x)
## [1] ""x is a missing value!""
                                                                                      7/15
 
 Something’s Wrong!
How do you know that something is wrong with your function?
 · What was your input? How did you call the function?
 · What were you expecting? Output, messages, other results?
 · What did you get?
 · How does what you get differ from what you were expecting?
 · Were your expectations correct in the first place?
 · Can you reproduce the problem (exactly)?
                                                              8/15
 
 Debugging Tools in R
The primary tools for debugging functions in R are
 · traceback: prints out the function call stack after an error occurs; does nothing if there’s no error
 · debug: flags a function for “debug” mode which allows you to step through execution of a function
   one line at a time
 · browser: suspends the execution of a function wherever it is called and puts the function in
   debug mode
 · trace: allows you to insert debugging code into a function a specific places
 · recover: allows you to modify the error behavior so that you can browse the function call stack
These are interactive tools specifically designed to allow you to pick through a function. There’s also
the more blunt technique of inserting print/cat statements in the function.
                                                                                                    9/15
 
 traceback
 > mean(x)
 Error in mean(x) : object 'x' not found
 > traceback()
 1: mean(x)
 >
                                         10/15
 
 traceback
 > lm(y ~ x)
 Error in eval(expr, envir, enclos) : object ’y’ not found
 > traceback()
 7: eval(expr, envir, enclos)
 6: eval(predvars, data, env)
 5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
 4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
 3: eval(expr, envir, enclos)
 2: eval(mf, parent.frame())
 1: lm(y ~ x)
                                                                    11/15
 
 debug
> debug(lm)
> lm(y ~ x)
debugging in: lm(y ~ x)
debug: {
    ret.x <- x
    ret.y <- y
    cl <- match.call()
    ...
    if (!qr)
        z$qr <- NULL
    z
}
Browse[2]>
                        12/15
 
 debug
Browse[2]> n
debug: ret.x <- x
Browse[2]> n
debug: ret.y <- y
Browse[2]> n
debug: cl <- match.call()
Browse[2]> n
debug: mf <- match.call(expand.dots = FALSE)
Browse[2]> n
debug: m <- match(c(""formula"", ""data"", ""subset"", ""weights"", ""na.action"",
    ""offset""), names(mf), 0L)
                                                                         13/15
 
 recover
 > options(error = recover)
 > read.csv(""nosuchfile"")
 Error in file(file, ""rt"") : cannot open the connection
 In addition: Warning message:
 In file(file, ""rt"") :
   cannot open file ’nosuchfile’: No such file or directory
 Enter a frame number, or 0 to exit
 1: read.csv(""nosuchfile"")
 2: read.table(file = file, header = header, sep = sep, quote = quote, dec =
 3: file(file, ""rt"")
 Selection:
                                                                             14/15
 
 Debugging
Summary
 · There are three main indications of a problem/condition: message, warning, error
      - only an error is fatal
 · When analyzing a function with a problem, make sure you can reproduce the problem, clearly
   state your expectations and how the output differs from your expectation
 · Interactive debugging tools traceback, debug, browser, trace, and recover can be used to
   find problematic code in functions
 · Debugging tools are not a substitute for thinking!
                                                                                        15/15
"
"./02_RProgramming/lectures/functions.pdf","Functions
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Functions
Functions are created using the function() directive and are stored as R objects just like anything
else. In particular, they are R objects of class “function”.
 f <- function(<arguments>) {
          ## Do something interesting
 }
Functions in R are “first class objects”, which means that they can be treated much like any other R
object. Importantly,
 · Functions can be passed as arguments to other functions
 · Functions can be nested, so that you can define a function inside of another function
 · The return value of a function is the last expression in the function body to be evaluated.
                                                                                                 2/13
 
 Function Arguments
Functions have named arguments which potentially have default values.
 · The formal arguments are the arguments included in the function definition
 · The formals function returns a list of all the formal arguments of a function
 · Not every function call in R makes use of all the formal arguments
 · Function arguments can be missing or might have default values
                                                                                 3/13
 
 Argument Matching
R functions arguments can be matched positionally or by name. So the following calls to sd are all
equivalent
 > mydata <- rnorm(100)
 > sd(mydata)
 > sd(x = mydata)
 > sd(x = mydata, na.rm = FALSE)
 > sd(na.rm = FALSE, x = mydata)
 > sd(na.rm = FALSE, mydata)
Even though it’s legal, I don’t recommend messing around with the order of the arguments too much,
since it can lead to some confusion.
                                                                                               4/13
 
 Argument Matching
You can mix positional matching with matching by name. When an argument is matched by name, it
is “taken out” of the argument list and the remaining unnamed arguments are matched in the order
that they are listed in the function definition.
  > args(lm)
  function (formula, data, subset, weights, na.action,
            method = ""qr"", model = TRUE, x = FALSE,
            y = FALSE, qr = TRUE, singular.ok = TRUE,
            contrasts = NULL, offset, ...)
The following two calls are equivalent.
  lm(data = mydata, y ~ x, model = FALSE, 1:100)
  lm(y ~ x, mydata, 1:100, model = FALSE)
                                                                                             5/13
 
 Argument Matching
· Most of the time, named arguments are useful on the command line when you have a long
  argument list and you want to use the defaults for everything except for an argument near the
  end of the list
· Named arguments also help if you can remember the name of the argument and not its position
  on the argument list (plotting is a good example).
                                                                                            6/13
 
 Argument Matching
Function arguments can also be partially matched, which is useful for interactive work. The order of
operations when given an argument is
  1. Check for exact match for a named argument
  2. Check for a partial match
  3. Check for a positional match
                                                                                                7/13
 
 Defining a Function
 f <- function(a, b = 1, c = 2, d = NULL) {
 }
In addition to not specifying a default value, you can also set an argument value to NULL.
                                                                                           8/13
 
 Lazy Evaluation
Arguments to functions are evaluated lazily, so they are evaluated only as needed.
 f <- function(a, b) {
      a^2
 }
 f(2)
 ## [1] 4
This function never actually uses the argument b, so calling f(2) will not produce an error because
the 2 gets positionally matched to a.
                                                                                                9/13
 
 Lazy Evaluation
 f <- function(a, b) {
       print(a)
       print(b)
 }
 f(45)
 ## [1] 45
 ## Error: argument ""b"" is missing, with no default
Notice that “45” got printed first before the error was triggered. This is because b did not have to be
evaluated until after print(a). Once the function tried to evaluate print(b) it had to throw an
error.
                                                                                                   10/13
 
 The “...” Argument
The ... argument indicate a variable number of arguments that are usually passed on to other
functions.
 · ... is often used when extending another function and you don’t want to copy the entire argument
   list of the original function
 myplot <- function(x, y, type = ""l"", ...) {
            plot(x, y, type = type, ...)
 }
 · Generic functions use ... so that extra arguments can be passed to methods (more on this later).
 > mean
 function (x, ...)
 UseMethod(""mean"")
                                                                                               11/13
 
 The “...” Argument
The ... argument is also necessary when the number of arguments passed to the function cannot be
known in advance.
 > args(paste)
 function (..., sep = "" "", collapse = NULL)
 > args(cat)
 function (..., file = """", sep = "" "", fill = FALSE,
     labels = NULL, append = FALSE)
                                                                                            12/13
 
 Arguments Coming After the “...” Argument
One catch with ... is that any arguments that appear after ... on the argument list must be named
explicitly and cannot be partially matched.
 > args(paste)
 function (..., sep = "" "", collapse = NULL)
 > paste(""a"", ""b"", sep = "":"")
 [1] ""a:b""
 > paste(""a"", ""b"", se = "":"")
 [1] ""a b :""
                                                                                              13/13
"
"./02_RProgramming/lectures/help.pdf","Getting Help
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Asking Questions
· Asking questions via email is different from asking questions in person
· People on the other side do not have the background information you have
    - they also don’t know you personally (usually)
· Other people are busy; their time is limited
· The instructor (me) is here to help in all circumstances but may not be able to answer all
  questions!
                                                                                        2/14
 
 Finding Answers
· Try to find an answer by searching the archives of the forum you plan to post to.
· Try to find an answer by searching the Web.
· Try to find an answer by reading the manual.
· Try to find an answer by reading a FAQ.
· Try to find an answer by inspection or experimentation.
· Try to find an answer by asking a skilled friend.
· If you're a programmer, try to find an answer by reading the source code.
                                                                                    3/14
 
 Asking Questions
· It’s important to let other people know that you’ve done all of the previous things already
· If the answer is in the documentation, the answer will be “Read the documentation”
      - one email round wasted
                                                                                              4/14
 
 Example: Error Messages
> library(datasets)
> data(airquality)
> cor(airquality)
Error in cor(airquality) : missing observations in cov/cor
                                                           5/14
 
 Google is your friend
                      6/14
 
 Asking Questions
· What steps will reproduce the problem?
· What is the expected output?
· What do you see instead?
· What version of the product (e.g. R, packages, etc.) are you using?
· What operating system?
· Additional information
                                                                      7/14
 
 Subject Headers
· Stupid: ""Help! Can't fit linear model!""
· Smart: ""R 3.0.2 lm() function produces seg fault with large data frame, Mac OS X 10.9.1""
· Smarter: ""R 3.0.2 lm() function on Mac OS X 10.9.1 -- seg fault on large data frame""
                                                                                           8/14
 
 Do
· Describe the goal, not the step
· Be explicit about your question
· Do provide the minimum amount of information necessary (volume is not precision)
· Be courteous (it never hurts)
· Follow up with the solution (if found)
                                                                                   9/14
 
 Don't
· Claim that you’ve found a bug
· Grovel as a substitute for doing your homework
· Post homework questions on mailing lists (we’ve seen them all)
· Email multiple mailing lists at once
· Ask others to debug your broken code without giving a hint as to what sort of problem they should
  be searching for
                                                                                               10/14
 
 Case Study: A Recent Post to the R-devel
Mailing List
Subject: large dataset - confused
Message:
  I'm trying to load a dataset into R, but
    I'm completely lost. This is probably
    due mostly to the fact that I'm a
    complete R newb, but it's got me stuck
    in a research project.
                                           11/14
 
 Response
Yes, you are lost. The R posting guide is
  at http://www.r-project.org/posting-
  guide.html and will point you to the
  right list and also the manuals (at
  e.g. http://cran.r-project.org/
  manuals.html, and one of them seems
  exactly what you need).
                                          12/14
 
 Analysis: What Went Wrong?
· Question was sent to the wrong mailing list (R-devel instead of R-help)
· Email subject was very vague
· Question was very vague
· Problem was not reproducible
· No evidence of any effort made to solve the problem
· RESULT: Recipe for disaster!
                                                                          13/14
 
 Places to Turn
· Class discussion board; your fellow students
· r-help@r-project.org
· Other project-specific mailing lists (This talk inspired by Eric Raymond’s “How to ask questions
  the smart way”)
                                                                                              14/14
"
"./02_RProgramming/lectures/lapply.pdf","Introduction to the R Language
Loop Functions
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Looping on the Command Line
Writing for, while loops is useful when programming but not particularly easy when working
interactively on the command line. There are some functions which implement looping to make life
easier.
 · lapply: Loop over a list and evaluate a function on each element
 · sapply: Same as lapply but try to simplify the result
 · apply: Apply a function over the margins of an array
 · tapply: Apply a function over subsets of a vector
 · mapply: Multivariate version of lapply
An auxiliary function split is also useful, particularly in conjunction with lapply.
                                                                                           2/12
 
 lapply
lapply takes three arguments: (1) a list X; (2) a function (or the name of a function) FUN; (3) other
arguments via its ... argument. If X is not a list, it will be coerced to a list using as.list.
 lapply
 ## function (X, FUN, ...)
 ## {
 ##     FUN <- match.fun(FUN)
 ##     if (!is.vector(X) || is.object(X))
 ##          X <- as.list(X)
 ##     .Internal(lapply(X, FUN))
 ## }
 ## <bytecode: 0x7ff7a1951c00>
 ## <environment: namespace:base>
The actual looping is done internally in C code.
                                                                                                  3/12
 
 lapply
lapply always returns a list, regardless of the class of the input.
 x <- list(a = 1:5, b = rnorm(10))
 lapply(x, mean)
 ## $a
 ## [1] 3
 ##
 ## $b
 ## [1] 0.4671
                                                                    4/12
 
 lapply
 x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
 lapply(x, mean)
 ## $a
 ## [1] 2.5
 ##
 ## $b
 ## [1] 0.5261
 ##
 ## $c
 ## [1] 1.421
 ##
 ## $d
 ## [1] 4.927
                                                                        5/12
 
 lapply
 > x <- 1:4
 > lapply(x, runif)
 [[1]]
 [1] 0.2675082
 [[2]]
 [1] 0.2186453 0.5167968
 [[3]]
 [1] 0.2689506 0.1811683 0.5185761
 [[4]]
 [1] 0.5627829 0.1291569 0.2563676 0.7179353
                                             6/12
 
 lapply
 > x <- 1:4
 > lapply(x, runif, min = 0, max = 10)
 [[1]]
 [1] 3.302142
 [[2]]
 [1] 6.848960 7.195282
 [[3]]
 [1] 3.5031416 0.8465707 9.7421014
 [[4]]
 [1] 1.195114 3.594027 2.930794 2.766946
                                         7/12
 
 lapply
lapply and friends make heavy use of anonymous functions.
 > x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2))
 > x
 $a
      [,1] [,2]
 [1,]    1    3
 [2,]    2    4
 $b
      [,1] [,2]
 [1,]    1    4
 [2,]    2    5
 [3,]    3    6
                                                           8/12
 
 lapply
An anonymous function for extracting the first column of each matrix.
 > lapply(x, function(elt) elt[,1])
 $a
 [1] 1 2
 $b
 [1] 1 2 3
                                                                      9/12
 
 sapply
sapply will try to simplify the result of lapply if possible.
 · If the result is a list where every element is length 1, then a vector is returned
 · If the result is a list where every element is a vector of the same length (> 1), a matrix is returned.
 · If it can’t figure things out, a list is returned
                                                                                                      10/12
 
 sapply
 > x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
 > lapply(x, mean)
 $a
 [1] 2.5
 $b
 [1] 0.06082667
 $c
 [1] 1.467083
 $d
 [1] 5.074749
                                                                          11/12
 
 sapply
 > sapply(x, mean)
          a          b          c          d
 2.50000000 0.06082667 1.46708277 5.07474950
 > mean(x)
 [1] NA
 Warning message:
 In mean.default(x) : argument is not numeric or logical: returning NA
                                                                       12/12
"
"./02_RProgramming/lectures/mapply.pdf","Introduction to the R Language
Loop Functions - mapply
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 mapply
mapply is a multivariate apply of sorts which applies a function in parallel over a set of arguments.
 > str(mapply)
 function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE,
             USE.NAMES = TRUE)
 · FUN is a function to apply
 · ... contains arguments to apply over
 · MoreArgs is a list of other arguments to FUN.
 · SIMPLIFY indicates whether the result should be simplified
                                                                                                      2/6
 
 mapply
The following is tedious to type
list(rep(1, 4), rep(2, 3), rep(3, 2), rep(4, 1))
Instead we can do
 > mapply(rep, 1:4, 4:1)
 [[1]]
 [1] 1 1 1 1
 [[2]]
 [1] 2 2 2
 [[3]]
 [1] 3 3
 [[4]]
 [1] 4
                                                 3/6
 
 Vectorizing a Function
> noise <- function(n, mean, sd) {
+ rnorm(n, mean, sd)
+ }
> noise(5, 1, 2)
[1] 2.4831198 2.4790100 0.4855190 -1.2117759
[5] -0.2743532
> noise(1:5, 1:5, 2)
[1] -4.2128648 -0.3989266  4.2507057 1.1572738
[5] 3.7413584
                                               4/6
 
 Instant Vectorization
 > mapply(noise, 1:5, 1:5, 2)
 [[1]]
 [1] 1.037658
 [[2]]
 [1] 0.7113482 2.7555797
 [[3]]
 [1] 2.769527 1.643568 4.597882
 [[4]]
 [1] 4.476741 5.658653 3.962813 1.204284
 [[5]]
 [1] 4.797123 6.314616 4.969892 6.530432 6.723254
                                                  5/6
 
 Instant Vectorization
Which is the same as
 list(noise(1, 1, 2), noise(2, 2, 2),
      noise(3, 3, 2), noise(4, 4, 2),
      noise(5, 5, 2))
                                      6/6
"
"./02_RProgramming/lectures/OverviewHistoryR.pdf","Overview and History of R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is R?
What is R?
           2/16
 
 What is R?
R is a dialect of the S language.
                                  3/16
 
 What is S?
· S is a language that was developed by John Chambers and others at Bell Labs.
· S was initiated in 1976 as an internal statistical analysis environment—originally implemented as
  Fortran libraries.
· Early versions of the language did not contain functions for statistical modeling.
· In 1988 the system was rewritten in C and began to resemble the system that we have today (this
  was Version 3 of the language). The book Statistical Models in S by Chambers and Hastie (the
  white book) documents the statistical analysis functionality.
· Version 4 of the S language was released in 1998 and is the version we use today. The book
  Programming with Data by John Chambers (the green book) documents this version of the
  language.
                                                                                                4/16
 
 Historical Notes
· In 1993 Bell Labs gave StatSci (now Insightful Corp.) an exclusive license to develop and sell the
  S language.
· In 2004 Insightful purchased the S language from Lucent for $2 million and is the current owner.
· In 2006, Alcatel purchased Lucent Technologies and is now called Alcatel-Lucent.
· Insightful sells its implementation of the S language under the product name S-PLUS and has
  built a number of fancy features (GUIs, mostly) on top of it—hence the “PLUS”.
· In 2008 Insightful is acquired by TIBCO for $25 million
· The fundamentals of the S language itself has not changed dramatically since 1998.
· In 1998, S won the Association for Computing Machinery’s Software System Award.
                                                                                                5/16
 
 S Philosophy
In “Stages in the Evolution of S”, John Chambers writes:
“[W]e wanted users to be able to begin in an interactive environment, where they did not consciously
think of themselves as programming. Then as their needs became clearer and their sophistication
increased, they should be able to slide gradually into programming, when the language and system
aspects would become more important.”
http://www.stat.bell-labs.com/S/history.html
                                                                                                6/16
 
 Back to R
· 1991: Created in New Zealand by Ross Ihaka and Robert Gentleman. Their experience
  developing R is documented in a 1996 JCGS paper.
· 1993: First announcement of R to the public.
· 1995: Martin Mächler convinces Ross and Robert to use the GNU General Public License to
  make R free software.
· 1996: A public mailing list is created (R-help and R-devel)
· 1997: The R Core Group is formed (containing some people associated with S-PLUS). The core
  group controls the source code for R.
· 2000: R version 1.0.0 is released.
· 2013: R version 3.0.2 is released on December 2013.
                                                                                         7/16
 
 Features of R
· Syntax is very similar to S, making it easy for S-PLUS users to switch over.
· Semantics are superficially similar to S, but in reality are quite different (more on that later).
· Runs on almost any standard computing platform/OS (even on the PlayStation 3)
· Frequent releases (annual + bugfix releases); active development.
                                                                                                     8/16
 
 Features of R (cont'd)
· Quite lean, as far as software goes; functionality is divided into modular packages
· Graphics capabilities very sophisticated and better than most stat packages.
· Useful for interactive work, but contains a powerful programming language for developing new
  tools (user -> programmer)
· Very active and vibrant user community; R-help and R-devel mailing lists and Stack Overflow
                                                                                              9/16
 
 Features of R (cont'd)
It's free! (Both in the sense of beer and in the sense of speech.)
                                                                   10/16
 
 Free Software
With free software, you are granted
 · The freedom to run the program, for any purpose (freedom 0).
 · The freedom to study how the program works, and adapt it to your needs (freedom 1). Access to
    the source code is a precondition for this.
 · The freedom to redistribute copies so you can help your neighbor (freedom 2).
 · The freedom to improve the program, and release your improvements to the public, so that the
    whole community benefits (freedom 3). Access to the source code is a precondition for this.
http://www.fsf.org
                                                                                                11/16
 
 Drawbacks of R
· Essentially based on 40 year old technology.
· Little built in support for dynamic or 3-D graphics (but things have improved greatly since the “old
  days”).
· Functionality is based on consumer demand and user contributions. If no one feels like
  implementing your favorite method, then it’s your job!
     - (Or you need to pay someone to do it)
· Objects must generally be stored in physical memory; but there have been advancements to deal
  with this too
· Not ideal for all possible situations (but this is a drawback of all software packages).
                                                                                                 12/16
 
 Design of the R System
The R system is divided into 2 conceptual parts:
  1. The “base” R system that you download from CRAN
  2. Everything else.
R functionality is divided into a number of packages.
 · The “base” R system contains, among other things, the base package which is required to run R
   and contains the most fundamental functions.
 · The other packages contained in the “base” system include utils, stats, datasets, graphics,
   grDevices, grid, methods, tools, parallel, compiler, splines, tcltk, stats4.
 · There are also “Recommend” packages: boot, class, cluster,                codetools, foreign,
   KernSmooth, lattice, mgcv, nlme, rpart, survival, MASS, spatial, nnet, Matrix.
                                                                                            13/16
 
 Design of the R System
And there are many other packages available:
 · There are about 4000 packages on CRAN that have been developed by users and programmers
   around the world.
 · There are also many packages associated with the Bioconductor project (http://bioconductor.org).
 · People often make packages available on their personal websites; there is no reliable way to
   keep track of how many packages are available in this fashion.
                                                                                              14/16
 
 Some R Resources
Available from CRAN (http://cran.r-project.org)
 · An Introduction to R
 · Writing R Extensions
 · R Data Import/Export
 · R Installation and Administration (mostly for building R from sources)
 · R Internals (not for the faint of heart)
                                                                          15/16
 
 Some Useful Books on S/R
Standard texts
 · Chambers (2008). Software for Data Analysis, Springer. (your textbook)
 · Chambers (1998). Programming with Data, Springer.
 · Venables & Ripley (2002). Modern Applied Statistics with S, Springer.
 · Venables & Ripley (2000). S Programming, Springer.
 · Pinheiro & Bates (2000). Mixed-Effects Models in S and S-PLUS, Springer.
 · Murrell (2005). R Graphics, Chapman & Hall/CRC Press.
Other resources
 · Springer has a series of books called Use R!.
 · A longer list of books is at http://www.r-project.org/doc/bib/R-books.html
                                                                              16/16
"
"./02_RProgramming/lectures/profiler.pdf","Profiling R Code
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Why is My Code So Slow?
· Profiling is a systematic way to examine how much time is spend in different parts of a program
· Useful when trying to optimize your code
· Often code runs fine once, but what if you have to put it in a loop for 1,000 iterations? Is it still fast
  enough?
· Profiling is better than guessing
                                                                                                        2/17
 
 On Optimizing Your Code
 · Getting biggest impact on speeding up code depends on knowing where the code spends most
   of its time
 · This cannot be done without performance analysis or profiling
We should forget about small efficiencies, say about 97% of the time: premature
optimization is the root of all evil
--Donald Knuth
                                                                                        3/17
 
 General Principles of Optimization
· Design first, then optimize
· Remember: Premature optimization is the root of all evil
· Measure (collect data), don’t guess.
· If you're going to be scientist, you need to apply the same principles here!
                                                                               4/17
 
 Using system.time()
· Takes an arbitrary R expression as input (can be wrapped in curly braces) and returns the
  amount of time taken to evaluate the expression
· Computes the time (in seconds) needed to execute an expression
    - If there’s an error, gives time until the error occurred
· Returns an object of class proc_time
    - user time: time charged to the CPU(s) for this expression
    - elapsed time: ""wall clock"" time
                                                                                        5/17
 
 Using system.time()
· Usually, the user time and elapsed time are relatively close, for straight computing tasks
· Elapsed time may be greater than user time if the CPU spends a lot of time waiting around
· Elapsted time may be smaller than the user time if your machine has multiple cores/processors
  (and is capable of using them)
    - Multi-threaded BLAS libraries (vecLib/Accelerate, ATLAS, ACML, MKL)
    - Parallel processing via the parallel package
                                                                                             6/17
 
 Using system.time()
## Elapsed time > user time
system.time(readLines(""http://www.jhsph.edu""))
   user system elapsed
  0.004   0.002   0.431
## Elapsed time < user time
hilbert <- function(n) {
        i <- 1:n
        1 / outer(i - 1, i, ""+”)
}
x <- hilbert(1000)
system.time(svd(x))
   user system elapsed
  1.605   0.094   0.742
                                               7/17
 
 Timing Longer Expressions
system.time({
    n <- 1000
    r <- numeric(n)
    for (i in 1:n) {
        x <- rnorm(n)
        r[i] <- mean(x)
    }
})
##    user  system elapsed
##   0.097   0.002   0.099
                           8/17
 
 Beyond system.time()
· Using system.time() allows you to test certain functions or code blocks to see if they are taking
  excessive amounts of time
· Assumes you already know where the problem is and can call system.time() on it
· What if you don’t know where to start?
                                                                                                9/17
 
 The R Profiler
· The Rprof() function starts the profiler in R
    - R must be compiled with profiler support (but this is usually the case)
· The summaryRprof() function summarizes the output from Rprof() (otherwise it’s not
  readable)
· DO NOT use system.time() and Rprof() together or you will be sad
                                                                                10/17
 
 The R Profiler
· Rprof() keeps track of the function call stack at regularly sampled intervals and tabulates how
  much time is spend in each function
· Default sampling interval is 0.02 seconds
· NOTE: If your code runs very quickly, the profiler is not useful, but then you probably don't need it
  in that case
                                                                                                  11/17
 
 R Profiler Raw Output
## lm(y ~ x)
sample.interval=10000
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""lm.fit"" ""lm""
""lm.fit"" ""lm""
""lm.fit"" ""lm""
                                                                            12/17
 
 Using summaryRprof()
· The summaryRprof() function tabulates the R profiler output and calculates how much time is
  spend in which function
· There are two methods for normalizing the data
· ""by.total"" divides the time spend in each function by the total run time
· ""by.self"" does the same but first subtracts out time spent in functions above in the call stack
                                                                                                  13/17
 
 By Total
$by.total
                      total.time total.pct self.time self.pct
""lm""                        7.41    100.00      0.30     4.05
""lm.fit""                    3.50     47.23      2.99    40.35
""model.frame.default""       2.24     30.23      0.12     1.62
""eval""                      2.24     30.23      0.00     0.00
""model.frame""               2.24     30.23      0.00     0.00
""na.omit""                   1.54     20.78      0.24     3.24
""na.omit.data.frame""        1.30     17.54      0.49     6.61
""lapply""                    1.04     14.04      0.00     0.00
""[.data.frame""              1.03     13.90      0.79    10.66
""[""                         1.03     13.90      0.00     0.00
""as.list.data.frame""        0.82     11.07      0.82    11.07
""as.list""                   0.82     11.07      0.00     0.00
                                                              14/17
 
 By Self
$by.self
                        self.time self.pct total.time total.pct
""lm.fit""                     2.99    40.35       3.50     47.23
""as.list.data.frame""         0.82    11.07       0.82     11.07
""[.data.frame""               0.79    10.66       1.03     13.90
""structure""                  0.73     9.85       0.73      9.85
""na.omit.data.frame""         0.49     6.61       1.30     17.54
""list""                       0.46     6.21       0.46      6.21
""lm""                         0.30     4.05       7.41    100.00
""model.matrix.default""       0.27     3.64       0.79     10.66
""na.omit""                    0.24     3.24       1.54     20.78
""as.character""               0.18     2.43       0.18      2.43
""model.frame.default""        0.12     1.62       2.24     30.23
""anyDuplicated.default""      0.02     0.27       0.02      0.27
                                                                15/17
 
 summaryRprof() Output
$sample.interval
[1] 0.02
$sampling.time
[1] 7.41
                      16/17
 
 Summary
· Rprof() runs the profiler for performance of analysis of R code
· summaryRprof() summarizes the output of Rprof() and gives percent of time spent in each
  function (with two types of normalization)
· Good to break your code into functions so that the profiler can give useful information about
  where time is being spent
· C or Fortran code is not profiled
                                                                                           17/17
"
"./02_RProgramming/lectures/reading_data_I.pdf","Reading and Writing Data Part I
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Reading Data
There are a few principal functions reading data into R.
 · read.table, read.csv, for reading tabular data
 · readLines, for reading lines of a text file
 · source, for reading in R code files (inverse of dump)
 · dget, for reading in R code files (inverse of dput)
 · load, for reading in saved workspaces
 · unserialize, for reading single R objects in binary form
                                                            2/9
 
 Writing Data
There are analogous functions for writing data to files
 · write.table
 · writeLines
 · dump
 · dput
 · save
 · serialize
                                                        3/9
 
 Reading Data Files with read.table
The read.table function is one of the most commonly used functions for reading data. It has a few
important arguments:
 · file, the name of a file, or a connection
 · header, logical indicating if the file has a header line
 · sep, a string indicating how the columns are separated
 · colClasses, a character vector indicating the class of each column in the dataset
 · nrows, the number of rows in the dataset
 · comment.char, a character string indicating the comment character
 · skip, the number of lines to skip from the beginning
 · stringsAsFactors, should character variables be coded as factors?
                                                                                               4/9
 
 read.table
For small to moderately sized datasets, you can usually call read.table without specifying any other
arguments
 data <- read.table(""foo.txt"")
R will automatically
 · skip lines that begin with a #
 · figure out how many rows there are (and how much memory needs to be allocated)
 · figure what type of variable is in each column of the table Telling R all these things directly makes
   R run faster and more efficiently.
 · read.csv is identical to read.table except that the default separator is a comma.
                                                                                                      5/9
 
 Reading in Larger Datasets with read.table
With much larger datasets, doing the following things will make your life easier and will prevent R
from choking.
 · Read the help page for read.table, which contains many hints
 · Make a rough calculation of the memory required to store your dataset. If the dataset is larger
   than the amount of RAM on your computer, you can probably stop right here.
 · Set comment.char = """" if there are no commented lines in your file.
                                                                                                 6/9
 
 Reading in Larger Datasets with read.table
· Use the colClasses argument. Specifying this option instead of using the default can make
  ’read.table’ run MUCH faster, often twice as fast. In order to use this option, you have to know the
  class of each column in your data frame. If all of the columns are “numeric”, for example, then
  you can just set colClasses = ""numeric"". A quick an dirty way to figure out the classes of
  each column is the following:
initial <- read.table(""datatable.txt"", nrows = 100)
classes <- sapply(initial, class)
tabAll <- read.table(""datatable.txt"",
                      colClasses = classes)
· Set nrows. This doesn’t make R run faster but it helps with memory usage. A mild overestimate
  is okay. You can use the Unix tool wc to calculate the number of lines in a file.
                                                                                                    7/9
 
 Know Thy System
In general, when using R with larger datasets, it’s useful to know a few things about your system.
 · How much memory is available?
 · What other applications are in use?
 · Are there other users logged into the same system?
 · What operating system?
 · Is the OS 32 or 64 bit?
                                                                                                   8/9
 
 Calculating Memory Requirements
I have a data frame with 1,500,000 rows and 120 columns, all of which are numeric data. Roughly,
how much memory is required to store this data frame?
1,500,000 × 120 × 8 bytes/numeric
= 1440000000 bytes
= 1440000000 / 220 bytes/MB
= 1,373.29 MB
= 1.34 GB
                                                                                             9/9
"
"./02_RProgramming/lectures/reading_data_II.pdf","Reading and Writing Data Part II
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Textual Formats
· dumping and dputing are useful because the resulting textual format is edit-able, and in the case
  of corruption, potentially recoverable.
· Unlike writing out a table or csv file, dump and dput preserve the metadata (sacrificing some
  readability), so that another user doesn’t have to specify it all over again.
· Textual formats can work much better with version control programs like subversion or git which
  can only track changes meaningfully in text files
· Textual formats can be longer-lived; if there is corruption somewhere in the file, it can be easier to
  fix the problem
· Textual formats adhere to the “Unix philosophy”
· Downside: The format is not very space-efficient
                                                                                                     2/9
 
 dput-ting R Objects
Another way to pass data around is by deparsing the R object with dput and reading it back in using
dget.
 > y <- data.frame(a = 1, b = ""a"")
 > dput(y)
 structure(list(a = 1,
                b = structure(1L, .Label = ""a"",
                              class = ""factor"")),
           .Names = c(""a"", ""b""), row.names = c(NA, -1L),
           class = ""data.frame"")
 > dput(y, file = ""y.R"")
 > new.y <- dget(""y.R"")
 > new.y
    a b
 1 1 a
                                                                                                 3/9
 
 Dumping R Objects
Multiple objects can be deparsed using the dump function and read back in using source.
 > x <- ""foo""
 > y <- data.frame(a = 1, b = ""a"")
 > dump(c(""x"", ""y""), file = ""data.R"")
 > rm(x, y)
 > source(""data.R"")
 > y
   a b
 1 1 a
 > x
 [1] ""foo""
                                                                                        4/9
 
 Interfaces to the Outside World
Data are read in using connection interfaces. Connections can be made to files (most common) or to
other more exotic things.
 · file, opens a connection to a file
 · gzfile, opens a connection to a file compressed with gzip
 · bzfile, opens a connection to a file compressed with bzip2
 · url, opens a connection to a webpage
                                                                                               5/9
 
 File Connections
> str(file)
function (description = """", open = """", blocking = TRUE,
           encoding = getOption(""encoding""))
· description is the name of the file
· open is a code indicating
    - “r” read only
    - “w” writing (and initializing a new file)
    - “a” appending
    - “rb”, “wb”, “ab” reading, writing, or appending in binary mode (Windows)
                                                                               6/9
 
 Connections
In general, connections are powerful tools that let you navigate files or other external objects. In
practice, we often don’t need to deal with the connection interface directly.
  con <- file(""foo.txt"", ""r"")
  data <- read.csv(con)
  close(con)
is the same as
  data <- read.csv(""foo.txt"")
                                                                                                 7/9
 
 Reading Lines of a Text File
 > con <- gzfile(""words.gz"")
 > x <- readLines(con, 10)
 > x
  [1] ""1080""     ""10-point"" ""10th""     ""11-point""
  [5] ""12-point"" ""16-point"" ""18-point"" ""1st""
  [9] ""2""        ""20-point""
writeLines takes a character vector and writes each element one line at a time to a text file.
                                                                                               8/9
 
 Reading Lines of a Text File
readLines can be useful for reading in lines of webpages
 ## This might take time
 con <- url(""http://www.jhsph.edu"", ""r"")
 x <- readLines(con)
 > head(x)
 [1] ""<!DOCTYPE HTML PUBLIC \""-//W3C//DTD HTML 4.0 Transitional//EN\"">""
 [2] """"
 [3] ""<html>""
 [4] ""<head>""
 [5] ""\t<meta http-equiv=\""Content-Type\"" content=\""text/html;charset=utf-8
                                                                            9/9
"
"./02_RProgramming/lectures/Scoping.pdf","Introduction to the R Language
Scoping Rules
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 A Diversion on Binding Values to Symbol
How does R know which value to assign to which symbol? When I type
  > lm <- function(x) { x * x }
  > lm
  function(x) { x * x }
how does R know what value to assign to the symbol lm? Why doesn’t it give it the value of lm that
is in the stats package?
                                                                                               2/24
 
 A Diversion on Binding Values to Symbol
When R tries to bind a value to a symbol, it searches through a series of environments to find the
appropriate value. When you are working on the command line and need to retrieve the value of an
R object, the order is roughly
  1. Search the global environment for a symbol name matching the one requested.
  2. Search the namespaces of each of the packages on the search list
The search list can be found by using the search function.
 > search()
 [1] "".GlobalEnv""           ""package:stats""     ""package:graphics""
 [4] ""package:grDevices"" ""package:utils""        ""package:datasets""
 [7] ""package:methods""      ""Autoloads""         ""package:base""
                                                                                               3/24
 
 Binding Values to Symbol
· The global environment or the user’s workspace is always the first element of the search list and
  the base package is always the last.
· The order of the packages on the search list matters!
· User’s can configure which packages get loaded on startup so you cannot assume that there will
  be a set list of packages available.
· When a user loads a package with library the namespace of that package gets put in position
  2 of the search list (by default) and everything else gets shifted down the list.
· Note that R has separate namespaces for functions and non-functions so it’s possible to have an
  object named c and a function named c.
                                                                                                4/24
 
 Scoping Rules
The scoping rules for R are the main feature that make it different from the original S language.
 · The scoping rules determine how a value is associated with a free variable in a function
 · R uses lexical scoping or static scoping. A common alternative is dynamic scoping.
 · Related to the scoping rules is how R uses the search list to bind a value to a symbol
 · Lexical scoping turns out to be particularly useful for simplifying statistical computations
                                                                                                  5/24
 
 Lexical Scoping
Consider the following function.
 f <- function(x, y) {
          x^2 + y / z
 }
This function has 2 formal arguments x and y. In the body of the function there is another symbol z.
In this case z is called a free variable. The scoping rules of a language determine how values are
assigned to free variables. Free variables are not formal arguments and are not local variables
(assigned insided the function body).
                                                                                                6/24
 
 Lexical Scoping
Lexical scoping in R means that
the values of free variables are searched for in the environment in which the function was defined.
What is an environment?
 · An environment is a collection of (symbol, value) pairs, i.e. x is a symbol and 3.14 might be its
   value.
 · Every environment has a parent environment; it is possible for an environment to have multiple
   “children”
 · the only environment without a parent is the empty environment
 · A function + an environment = a closure or function closure.
                                                                                                  7/24
 
 Lexical Scoping
Searching for the value for a free variable:
 · If the value of a symbol is not found in the environment in which a function was defined, then the
   search is continued in the parent environment.
 · The search continues down the sequence of parent environments until we hit the top-level
   environment; this usually the global environment (workspace) or the namespace of a package.
 · After the top-level environment, the search continues down the search list until we hit the empty
   environment. If a value for a given symbol cannot be found once the empty environment is
   arrived at, then an error is thrown.
                                                                                                  8/24
 
 Lexical Scoping
Why does all this matter?
· Typically, a function is defined in the global environment, so that the values of free variables are
  just found in the user’s workspace
· This behavior is logical for most people and is usually the “right thing” to do
· However, in R you can have functions defined inside other functions
    - Languages like C don’t let you do this
· Now things get interesting — In this case the environment in which a function is defined is the
  body of another function!
                                                                                                   9/24
 
 Lexical Scoping
 make.power <- function(n) {
          pow <- function(x) {
                   x^n
          }
          pow
 }
This function returns another function as its value.
 > cube <- make.power(3)
 > square <- make.power(2)
 > cube(3)
 [1] 27
 > square(3)
 [1] 9
                                                     10/24
 
 Exploring a Function Closure
What’s in a function’s environment?
 > ls(environment(cube))
 [1] ""n""    ""pow""
 > get(""n"", environment(cube))
 [1] 3
 > ls(environment(square))
 [1] ""n""    ""pow""
 > get(""n"", environment(square))
 [1] 2
                                    11/24
 
 Lexical vs. Dynamic Scoping
 y <- 10
 f <- function(x) {
          y <- 2
          y^2 + g(x)
 }
 g <- function(x) {
          x*y
 }
What is the value of
 f(3)
                            12/24
 
 Lexical vs. Dynamic Scoping
· With lexical scoping the value of y in the function g is looked up in the environment in which the
  function was defined, in this case the global environment, so the value of y is 10.
· With dynamic scoping, the value of y is looked up in the environment from which the function was
  called (sometimes referred to as the calling environment).
    - In R the calling environment is known as the parent frame
· So the value of y would be 2.
                                                                                                13/24
 
 Lexical vs. Dynamic Scoping
When a function is defined in the global environment and is subsequently called from the global
environment, then the defining environment and the calling environment are the same. This can
sometimes give the appearance of dynamic scoping.
 > g <- function(x) {
 + a <- 3
 + x+a+y
 + }
 > g(2)
 Error in g(2) : object ""y"" not found
 > y <- 3
 > g(2)
 [1] 8
                                                                                           14/24
 
 Other Languages
Other languages that support lexical scoping
 · Scheme
 · Perl
 · Python
 · Common Lisp (all languages converge to Lisp)
                                                15/24
 
 Consequences of Lexical Scoping
· In R, all objects must be stored in memory
· All functions must carry a pointer to their respective defining environments, which could be
  anywhere
· In S-PLUS, free variables are always looked up in the global workspace, so everything can be
  stored on the disk because the “defining environment” of all functions is the same.
                                                                                          16/24
 
 Application: Optimization
Why is any of this information useful?
· Optimization routines in R like optim, nlm, and optimize require you to pass a function whose
  argument is a vector of parameters (e.g. a log-likelihood)
· However, an object function might depend on a host of other things besides its parameters (like
  data)
· When writing software which does optimization, it may be desirable to allow the user to hold
  certain parameters fixed
                                                                                            17/24
 
 Maximizing a Normal Likelihood
Write a “constructor” function
 make.NegLogLik <- function(data, fixed=c(FALSE,FALSE)) {
          params <- fixed
          function(p) {
                  params[!fixed] <- p
                  mu <- params[1]
                  sigma <- params[2]
                  a <- -0.5*length(data)*log(2*pi*sigma^2)
                  b <- -0.5*sum((data-mu)^2) / (sigma^2)
                  -(a + b)
          }
 }
Note: Optimization functions in R minimize functions, so you need to use the negative log-likelihood.
                                                                                                  18/24
 
 Maximizing a Normal Likelihood
> set.seed(1); normals <- rnorm(100, 1, 2)
> nLL <- make.NegLogLik(normals)
> nLL
function(p) {
                params[!fixed] <- p
                mu <- params[1]
                sigma <- params[2]
                a <- -0.5*length(data)*log(2*pi*sigma^2)
                b <- -0.5*sum((data-mu)^2) / (sigma^2)
                -(a + b)
        }
<environment: 0x165b1a4>
> ls(environment(nLL))
[1] ""data""   ""fixed"" ""params""
                                                         19/24
 
 Estimating Parameters
 > optim(c(mu = 0, sigma = 1), nLL)$par
        mu   sigma
 1.218239 1.787343
Fixing σ = 2
 > nLL <- make.NegLogLik(normals, c(FALSE, 2))
 > optimize(nLL, c(-1, 3))$minimum
 [1] 1.217775
Fixing μ = 1
 > nLL <- make.NegLogLik(normals, c(1, FALSE))
 > optimize(nLL, c(1e-6, 10))$minimum
 [1] 1.800596
                                               20/24
 
 Plotting the Likelihood
nLL <- make.NegLogLik(normals, c(1, FALSE))
x <- seq(1.7, 1.9, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = ""l"")
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = ""l"")
                                            21/24
 
 Plotting the Likelihood
                        22/24
 
 Plotting the Likelihood
                        23/24
 
 Lexical Scoping Summary
· Objective functions can be “built” which contain all of the necessary data for evaluating the
  function
· No need to carry around long argument lists — useful for interactive and exploratory work.
· Code can be simplified and cleand up
· Reference: Robert Gentleman and Ross Ihaka (2000). “Lexical Scope and Statistical Computing,”
  JCGS, 9, 491–508.
                                                                                             24/24
"
"./02_RProgramming/lectures/simulation.pdf","Simulation
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Generating Random Numbers
Functions for probability distributions in R
 · rnorm: generate random Normal variates with a given mean and standard deviation
 · dnorm: evaluate the Normal probability density (with a given mean/SD) at a point (or vector of
   points)
 · pnorm: evaluate the cumulative distribution function for a Normal distribution
 · rpois: generate random Poisson variates with a given rate
                                                                                             2/15
 
 Generating Random Numbers
Probability distribution functions usually have four functions associated with them. The functions are
prefixed with a
 · d for density
 · r for random number generation
 · p for cumulative distribution
 · q for quantile function
                                                                                                   3/15
 
 Generating Random Numbers
Working with the Normal distributions requires using these four functions
  dnorm(x,  mean = 0, sd  =  1, log = FALSE)
  pnorm(q,  mean = 0, sd  =  1, lower.tail = TRUE, log.p = FALSE)
  qnorm(p,  mean = 0, sd  =  1, lower.tail = TRUE, log.p = FALSE)
  rnorm(n,  mean = 0, sd  =  1)
If Φ is the cumulative distribution function for a standard Normal distribution, then pnorm(q) = Φ(q)
and qnorm(p) = Φ−1 (p).
                                                                                                  4/15
 
 Generating Random Numbers
> x <- rnorm(10)
> x
 [1] 1.38380206 0.48772671 0.53403109 0.66721944
 [5] 0.01585029 0.37945986 1.31096736 0.55330472
 [9] 1.22090852 0.45236742
> x <- rnorm(10, 20, 2)
> x
 [1] 23.38812 20.16846 21.87999 20.73813 19.59020
 [6] 18.73439 18.31721 22.51748 20.36966 21.04371
> summary(x)
   Min. 1st Qu. Median     Mean 3rd Qu.    Max.
  18.32   19.73   20.55   20.67   21.67   23.39
                                                  5/15
 
 Generating Random Numbers
Setting the random number seed with set.seed ensures reproducibility
 > set.seed(1)
 > rnorm(5)
 [1] -0.6264538   0.1836433 -0.8356286 1.5952808
 [5] 0.3295078
 > rnorm(5)
 [1] -0.8204684   0.4874291  0.7383247 0.5757814
 [5] -0.3053884
 > set.seed(1)
 > rnorm(5)
 [1] -0.6264538   0.1836433 -0.8356286 1.5952808
 [5] 0.3295078
Always set the random number seed when conducting a simulation!
                                                                     6/15
 
 Generating Random Numbers
Generating Poisson data
 > rpois(10, 1)
  [1] 3 1 0 1 0 0 1 0 1 1
 > rpois(10, 2)
  [1] 6 2 2 1 3 2 2 1 1 2
 > rpois(10, 20)
  [1] 20 11 21 20 20 21 17 15 24 20
 > ppois(2, 2)  ## Cumulative distribution
 [1] 0.6766764  ## Pr(x <= 2)
 > ppois(4, 2)
 [1] 0.947347   ## Pr(x <= 4)
 > ppois(6, 2)
 [1] 0.9954662  ## Pr(x <= 6)
                                           7/15
 
 Generating Random Numbers From a Linear
Model
Suppose we want to simulate from the following linear model
                                         y = β0 + β1 x + ε
where ε ∼ (0, 22 ). Assume x ∼ (0, 12 ), β0 = 0.5 and β1 = 2 .
 > set.seed(20)
 > x <- rnorm(100)
 > e <- rnorm(100, 0, 2)
 > y <- 0.5 + 2 * x + e
 > summary(y)
    Min. 1st Qu. Median
 -6.4080 -1.5400 0.6789   0.6893  2.9300    6.5050
 > plot(x, y)
                                                                 8/15
 
 Generating Random Numbers From a Linear
Model
                                        9/15
 
 Generating Random Numbers From a Linear
Model
What if x is binary?
 > set.seed(10)
 > x <- rbinom(100, 1, 0.5)
 > e <- rnorm(100, 0, 2)
 > y <- 0.5 + 2 * x + e
 > summary(y)
    Min. 1st Qu. Median
 -3.4940 -0.1409 1.5770 1.4320 2.8400 6.9410
 > plot(x, y)
                                             10/15
 
 Generating Random Numbers From a Linear
Model
                                        11/15
 
 Generating Random Numbers From a
Generalized Linear Model
Suppose we want to simulate from a Poisson model where
Y ~ Poisson(μ)
log μ = β0 + β1 x
and β0 = 0.5 and β1 = 0.3. We need to use the rpois function for this
 > set.seed(1)
 > x <- rnorm(100)
 > log.mu <- 0.5 + 0.3 * x
 > y <- rpois(100, exp(log.mu))
 > summary(y)
     Min. 1st Qu. Median    Mean 3rd Qu.     Max.
     0.00    1.00   1.00    1.55     2.00    6.00
 > plot(x, y)
                                                                      12/15
 
 Generating Random Numbers From a
Generalized Linear Model
                                 13/15
 
 Random Sampling
The sample function draws randomly from a specified set of (scalar) objects allowing you to sample
from arbitrary distributions.
  > set.seed(1)
  > sample(1:10, 4)
  [1] 3 4 5 7
  > sample(1:10, 4)
  [1] 3 9 8 5
  > sample(letters, 5)
  [1] ""q"" ""b"" ""e"" ""x"" ""p""
  > sample(1:10) ## permutation
   [1] 4 710 6 9 2 8 3 1 5
  > sample(1:10)
   [1] 2 3 4 1 9 5 10 8 6 7
  > sample(1:10, replace = TRUE) ## Sample w/replacement
   [1] 2 9 7 8 2 8 5 9 7 8
                                                                                              14/15
 
 Simulation
Summary
 · Drawing samples from specific probability distributions can be done with r* functions
 · Standard distributions are built in: Normal, Poisson, Binomial, Exponential, Gamma, etc.
 · The sample function can be used to draw random samples from arbitrary vectors
 · Setting the random number generator seed via set.seed is critical for reproducibility
                                                                                            15/15
"
"./02_RProgramming/lectures/split.pdf","split
split takes a vector or other objects and splits it into groups determined by a factor or list of
factors.
 > str(split)
 function (x, f, drop = FALSE, ...)
 · x is a vector (or list) or data frame
 · f is a factor (or coerced to one) or a list of factors
 · drop indicates whether empty factors levels should be dropped
                                                                                             6/14
 
 split
 > x <- c(rnorm(10), runif(10), rnorm(10, 1))
 > f <- gl(3, 10)
 > split(x, f)
 $‘1‘
  [1] -0.8493038 -0.5699717 -0.8385255 -0.8842019
  [5] 0.2849881 0.9383361 -1.0973089 2.6949703
  [9] 1.5976789 -0.1321970
 $‘2‘
  [1] 0.09479023 0.79107293 0.45857419 0.74849293
  [5] 0.34936491 0.35842084 0.78541705 0.57732081
  [9] 0.46817559 0.53183823
 $‘3‘
  [1] 0.6795651 0.9293171 1.0318103 0.4717443
  [5] 2.5887025 1.5975774 1.3246333 1.4372701
                                                  7/14
 
 split
A common idiom is split followed by an lapply.
 > lapply(split(x, f), mean)
 $‘1‘
 [1] 0.1144464
 $‘2‘
 [1] 0.5163468
 $‘3‘
 [1] 1.246368
                                               8/14
 
 Splitting a Data Frame
> library(datasets)
> head(airquality)
  Ozone Solar.R Wind Temp Month Day
1    41     190 7.4    67     5   1
2    36     118 8.0    72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA      NA 14.3   56     5   5
6    28      NA 14.9   66     5   6
                                    9/14
 
 Splitting a Data Frame
> s <- split(airquality, airquality$Month)
> lapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")]))
$‘5‘
   Ozone Solar.R      Wind
      NA       NA 11.62258
$‘6‘
    Ozone   Solar.R      Wind
       NA 190.16667 10.26667
$‘7‘
     Ozone    Solar.R       Wind
        NA 216.483871   8.941935
                                                                      10/14
 
 Splitting a Data Frame
> sapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")]))
               5         6          7        8        9
Ozone         NA        NA         NA       NA       NA
Solar.R       NA 190.16667 216.483871       NA 167.4333
Wind    11.62258 10.26667    8.941935 8.793548 10.1800
> sapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")],
                                 na.rm = TRUE))
                  5            6             7            8           9
Ozone      23.61538     29.44444     59.115385    59.961538   31.44828
Solar.R   181.29630    190.16667    216.483871   171.857143 167.43333
Wind       11.62258     10.26667      8.941935     8.793548   10.18000
                                                                        11/14
 
 Splitting on More than One Level
> x <- rnorm(10)
> f1 <- gl(2, 5)
> f2 <- gl(5, 2)
> f1
 [1] 1 1 1 1 1 2 2 2 2 2
Levels: 1 2
> f2
 [1] 1 1 2 2 3 3 4 4 5 5
Levels: 1 2 3 4 5
> interaction(f1, f2)
 [1] 1.1 1.1 1.2 1.2 1.3 2.3 2.4 2.4 2.5 2.5
10 Levels: 1.1 2.1 1.2 2.2 1.3 2.3 1.4 ... 2.5
                                               12/14
 
 Splitting on More than One Level
Interactions can create empty levels.
 > str(split(x, list(f1, f2)))
 List of 10
  $ 1.1: num [1:2] -0.378 0.445
  $ 2.1: num(0)
  $ 1.2: num [1:2] 1.4066 0.0166
  $ 2.2: num(0)
  $ 1.3: num -0.355
  $ 2.3: num 0.315
  $ 1.4: num(0)
  $ 2.4: num [1:2] -0.907 0.723
  $ 1.5: num(0)
  $ 2.5: num [1:2] 0.732 0.360
                                      13/14
 
 split
Empty levels can be dropped.
 > str(split(x, list(f1, f2), drop = TRUE))
 List of 6
  $ 1.1: num [1:2] -0.378 0.445
  $ 1.2: num [1:2] 1.4066 0.0166
  $ 1.3: num -0.355
  $ 2.3: num 0.315
  $ 2.4: num [1:2] -0.907 0.723
  $ 2.5: num [1:2] 0.732 0.360
                                            14/14
"
"./02_RProgramming/lectures/Str.pdf"," 
 "
"./02_RProgramming/lectures/Subsetting.pdf","Introduction to the R Language
Data Types and Basic Operations
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 Subsetting
There are a number of operators that can be used to extract subsets of R objects.
 · [ always returns an object of the same class as the original; can be used to select more than one
   element (there is one exception)
 · [[ is used to extract elements of a list or a data frame; it can only be used to extract a single
   element and the class of the returned object will not necessarily be a list or data frame
 · $ is used to extract elements of a list or data frame by name; semantics are similar to that of [[.
                                                                                                    2/14
 
 Subsetting
> x <- c(""a"", ""b"", ""c"", ""c"", ""d"", ""a"")
> x[1]
[1] ""a""
> x[2]
[1] ""b""
> x[1:4]
[1] ""a"" ""b"" ""c"" ""c""
> x[x > ""a""]
[1] ""b"" ""c"" ""c"" ""d""
> u <- x > ""a""
> u
[1] FALSE TRUE TRUE TRUE TRUE FALSE
> x[u]
[1] ""b"" ""c"" ""c"" ""d""
                                       3/14
 
 Subsetting a Matrix
Matrices can be subsetted in the usual way with (i,j) type indices.
 > x <- matrix(1:6, 2, 3)
 > x[1, 2]
 [1] 3
 > x[2, 1]
 [1] 2
Indices can also be missing.
 > x[1, ]
 [1] 1 3 5
 > x[, 2]
 [1] 3 4
                                                                    4/14
 
 Subsetting a Matrix
By default, when a single element of a matrix is retrieved, it is returned as a vector of length 1 rather
than a 1 × 1 matrix. This behavior can be turned off by setting drop = FALSE.
 > x <- matrix(1:6, 2, 3)
 > x[1, 2]
 [1] 3
 > x[1, 2, drop = FALSE]
      [,1]
 [1,] 3
                                                                                                      5/14
 
 Subsetting a Matrix
Similarly, subsetting a single column or a single row will give you a vector, not a matrix (by default).
 > x <- matrix(1:6, 2, 3)
 > x[1, ]
 [1] 1 3 5
 > x[1, , drop = FALSE]
      [,1] [,2] [,3]
 [1,]      1   3      5
                                                                                                      6/14
 
 Subsetting Lists
​6/14
 Subsetting Lists
 > x <- list(foo = 1:4, bar = 0.6)
 > x[1]
 $foo
 [1] 1 2 3 4
 > x[[1]]
 [1] 1 2 3 4
 > x$bar
 [1] 0.6
 > x[[""bar""]]
 [1] 0.6
 > x[""bar""]
 $bar
 [1] 0.6
                                   7/14
 
 Subsetting Lists
> x <- list(foo = 1:4, bar = 0.6, baz = ""hello"")
> x[c(1, 3)]
$foo
[1] 1 2 3 4
$baz
[1] ""hello""
                                                 8/14
 
 Subsetting Lists
The [[ operator can be used with computed indices; $ can only be used with literal names.
 > x <- list(foo = 1:4, bar = 0.6, baz = ""hello"")
 > name <- ""foo""
 > x[[name]] ##  computed index for ‘foo’
 [1] 1 2 3 4
 > x$name     ## element ‘name’ doesn’t exist!
 NULL
 > x$foo
 [1] 1 2 3 4 ##  element ‘foo’ does exist
                                                                                          9/14
 
 Subsetting Nested Elements of a List
The [[ can take an integer sequence.
 > x <- list(a = list(10, 12, 14), b = c(3.14, 2.81))
 > x[[c(1, 3)]]
 [1] 14
 > x[[1]][[3]]
 [1] 14
 > x[[c(2, 1)]]
 [1] 3.14
                                                      10/14
 
 Partial Matching
Partial matching of names is allowed with [[ and $.
 > x <- list(aardvark = 1:5)
 > x$a
 [1] 1 2 3 4 5
 > x[[""a""]]
 NULL
 > x[[""a"", exact = FALSE]]
 [1] 1 2 3 4 5
                                                    11/14
 
 Removing NA Values
A common task is to remove missing values (NAs).
 > x <- c(1, 2, NA, 4, NA, 5)
 > bad <- is.na(x)
 > x[!bad]
 [1] 1 2 4 5
                                                 12/14
 
 Removing NA Values
What if there are multiple things and you want to take the subset with no missing values?
 > x <- c(1, 2, NA, 4, NA, 5)
 > y <- c(""a"", ""b"", NA, ""d"", NA, ""f"")
 > good <- complete.cases(x, y)
 > good
 [1] TRUE TRUE FALSE TRUE FALSE TRUE
 > x[good]
 [1] 1 2 4 5
 > y[good]
 [1] ""a"" ""b"" ""d"" ""f""
                                                                                          13/14
 
 Removing NA Values
> airquality[1:6, ]
  Ozone Solar.R Wind Temp Month Day
1    41     190 7.4    67     5   1
2    36     118 8.0    72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA     NA 14.3    56     5   5
6    28     NA 14.9    66     5   6
> good <- complete.cases(airquality)
> airquality[good, ][1:6, ]
  Ozone Solar.R Wind Temp Month Day
1    41     190 7.4    67     5   1
2    36     118 8.0    72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
7    23     299 8.6    65     5   7
                                     14/14
"
"./02_RProgramming/lectures/tapply.pdf","Introduction to the R Language
Loop Functions - tapply
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 tapply
tapply is used to apply a function over subsets of a vector. I don’t know why it’s called tapply.
 > str(tapply)
 function (X, INDEX, FUN = NULL, ..., simplify = TRUE)
 · X is a vector
 · INDEX is a factor or a list of factors (or else they are coerced to factors)
 · FUN is a function to be applied
 · ... contains other arguments to be passed FUN
 · simplify, should we simplify the result?
                                                                                                  2/14
 
 tapply
Take group means.
 > x <- c(rnorm(10), runif(10), rnorm(10, 1))
 > f <- gl(3, 10)
 > f
  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3
 [24] 3 3 3 3 3 3 3
 Levels: 1 2 3
 > tapply(x, f, mean)
         1         2         3
 0.1144464 0.5163468 1.2463678
                                                    3/14
 
 tapply
Take group means without simplification.
 > tapply(x, f, mean, simplify = FALSE)
 $‘1‘
 [1] 0.1144464
 $‘2‘
 [1] 0.5163468
 $‘3‘
 [1] 1.246368
                                         4/14
 
 tapply
Find group ranges.
 > tapply(x, f, range)
 $‘1‘
 [1] -1.097309 2.694970
 $‘2‘
 [1] 0.09479023 0.79107293
 $‘3‘
 [1] 0.4717443 2.5887025
                           5/14
"
"./02_RProgramming/lectures/Vectorized.pdf","Introduction to the R Language
Vectorized Operations
Roger Peng, Associate Professor
Johns Hopkins Bloomberg School of Public Health
 
 Vectorized Operations
Many operations in R are vectorized making code more efficient, concise, and easier to read.
 > x <- 1:4; y <- 6:9
 > x + y
 [1] 7 9 11 13
 > x > 2
 [1] FALSE FALSE TRUE TRUE
 > x >= 2
 [1] FALSE TRUE TRUE TRUE
 > y == 8
 [1] FALSE FALSE TRUE FALSE
 > x * y
 [1] 6 14 24 36
 > x / y
 [1] 0.1666667 0.2857143 0.3750000 0.4444444
                                                                                             2/3
 
 Vectorized Matrix Operations
> x <- matrix(1:4, 2, 2); y <- matrix(rep(10, 4), 2, 2)
> x * y       ## element-wise multiplication
     [,1] [,2]
[1,]   10   30
[2,]   20   40
> x / y
     [,1] [,2]
[1,] 0.1 0.3
[2,] 0.2 0.4
> x %*% y     ## true matrix multiplication
     [,1] [,2]
[1,]   40   40
[2,]   60   60
                                                        3/3
"
"./02_RProgramming/mapply/mapply.pdf","7/30/13                                                                        Introduction to the R Language
                          Introduction to the R Language
                          Loop Functions - mapply
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                1/6
 
 7/30/13                                                                        Introduction to the R Language
              mapply
              mapplyis a multivariate apply of sorts which applies a function in parallel over a set of arguments.
                 > str(mapply)
                 function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE,
                                   USE.NAMES = TRUE)
                 · FUNis a function to apply
                 · ... contains arguments to apply over
                 · MoreArgsis a list of other arguments to FUN.
                 · SIMPLIFYindicates whether the result should be simplified
                                                                                                                   2/6
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                         2/6
 
 7/30/13                                                                        Introduction to the R Language
              mapply
              The following is tedious to type
              list(rep(1, 4), rep(2, 3), rep(3, 2), rep(4, 1))
              Instead we can do
                 > mapply(rep, 1:4, 4:1)
                 [[1]]
                 [1] 1 1 1 1
                 [[2]]
                 [1] 2 2 2
                 [[3]]
                 [1] 3 3
                 [[4]]
                 [1] 4
                                                                                                              3/6
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                    3/6
 
 7/30/13                                                                        Introduction to the R Language
              Vectorizing a Function
                 > noise <- function(n, mean, sd) {
                 + rnorm(n, mean, sd)
                 +}
                 > noise(5, 1, 2)
                 [1] 2.4831198 2.4790100 0.4855190 -1.2117759
                 [5] -0.2743532
                 > noise(1:5, 1:5, 2)
                 [1] -4.2128648 -0.3989266 4.2507057 1.1572738
                 [5] 3.7413584
                                                                                                              4/6
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                    4/6
 
 7/30/13                                                                        Introduction to the R Language
              Instant Vectorization
                 > mapply(noise, 1:5, 1:5, 2)
                 [[1]]
                 [1] 1.037658
                 [[2]]
                 [1] 0.7113482 2.7555797
                 [[3]]
                 [1] 2.769527 1.643568 4.597882
                 [[4]]
                 [1] 4.476741 5.658653 3.962813 1.204284
                 [[5]]
                 [1] 4.797123 6.314616 4.969892 6.530432 6.723254
                                                                                                              5/6
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                    5/6
 
 7/30/13                                                                        Introduction to the R Language
              Instant Vectorization
              Which is the same as
                 list(noise(1, 1, 2), noise(2, 2, 2),
                          noise(3, 3, 2), noise(4, 4, 2),
                          noise(5, 5, 2))
                                                                                                              6/6
file://localhost/Users/sean/Developer/GitHub/modules/roger/mapply/index.html#1                                    6/6
"
"./02_RProgramming/OverviewHistoryR/OverviewHistoryR.pdf","Overview and History of R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is R?
What is R?
           2/16
 
 What is R?
R is a dialect of the S language.
                                  3/16
 
 What is S?
· S is a language that was developed by John Chambers and others at Bell Labs.
· S was initiated in 1976 as an internal statistical analysis environment—originally implemented as
  Fortran libraries.
· Early versions of the language did not contain functions for statistical modeling.
· In 1988 the system was rewritten in C and began to resemble the system that we have today (this
  was Version 3 of the language). The book Statistical Models in S by Chambers and Hastie (the
  white book) documents the statistical analysis functionality.
· Version 4 of the S language was released in 1998 and is the version we use today. The book
  Programming with Data by John Chambers (the green book) documents this version of the
  language.
                                                                                                4/16
 
 Historical Notes
· In 1993 Bell Labs gave StatSci (now Insightful Corp.) an exclusive license to develop and sell the
  S language.
· In 2004 Insightful purchased the S language from Lucent for $2 million and is the current owner.
· In 2006, Alcatel purchased Lucent Technologies and is now called Alcatel-Lucent.
· Insightful sells its implementation of the S language under the product name S-PLUS and has
  built a number of fancy features (GUIs, mostly) on top of it—hence the “PLUS”.
· In 2008 Insightful is acquired by TIBCO for $25 million
· The fundamentals of the S language itself has not changed dramatically since 1998.
· In 1998, S won the Association for Computing Machinery’s Software System Award.
                                                                                                5/16
 
 S Philosophy
In “Stages in the Evolution of S”, John Chambers writes:
“[W]e wanted users to be able to begin in an interactive environment, where they did not consciously
think of themselves as programming. Then as their needs became clearer and their sophistication
increased, they should be able to slide gradually into programming, when the language and system
aspects would become more important.”
http://www.stat.bell-labs.com/S/history.html
                                                                                                6/16
 
 Back to R
· 1991: Created in New Zealand by Ross Ihaka and Robert Gentleman. Their experience
  developing R is documented in a 1996 JCGS paper.
· 1993: First announcement of R to the public.
· 1995: Martin Mächler convinces Ross and Robert to use the GNU General Public License to
  make R free software.
· 1996: A public mailing list is created (R-help and R-devel)
· 1997: The R Core Group is formed (containing some people associated with S-PLUS). The core
  group controls the source code for R.
· 2000: R version 1.0.0 is released.
· 2013: R version 3.0.2 is released on December 2013.
                                                                                         7/16
 
 Features of R
· Syntax is very similar to S, making it easy for S-PLUS users to switch over.
· Semantics are superficially similar to S, but in reality are quite different (more on that later).
· Runs on almost any standard computing platform/OS (even on the PlayStation 3)
· Frequent releases (annual + bugfix releases); active development.
                                                                                                     8/16
 
 Features of R (cont'd)
· Quite lean, as far as software goes; functionality is divided into modular packages
· Graphics capabilities very sophisticated and better than most stat packages.
· Useful for interactive work, but contains a powerful programming language for developing new
  tools (user -> programmer)
· Very active and vibrant user community; R-help and R-devel mailing lists and Stack Overflow
                                                                                              9/16
 
 Features of R (cont'd)
It's free! (Both in the sense of beer and in the sense of speech.)
                                                                   10/16
 
 Free Software
With free software, you are granted
 · The freedom to run the program, for any purpose (freedom 0).
 · The freedom to study how the program works, and adapt it to your needs (freedom 1). Access to
    the source code is a precondition for this.
 · The freedom to redistribute copies so you can help your neighbor (freedom 2).
 · The freedom to improve the program, and release your improvements to the public, so that the
    whole community benefits (freedom 3). Access to the source code is a precondition for this.
http://www.fsf.org
                                                                                                11/16
 
 Drawbacks of R
· Essentially based on 40 year old technology.
· Little built in support for dynamic or 3-D graphics (but things have improved greatly since the “old
  days”).
· Functionality is based on consumer demand and user contributions. If no one feels like
  implementing your favorite method, then it’s your job!
     - (Or you need to pay someone to do it)
· Objects must generally be stored in physical memory; but there have been advancements to deal
  with this too
· Not ideal for all possible situations (but this is a drawback of all software packages).
                                                                                                 12/16
 
 Design of the R System
The R system is divided into 2 conceptual parts:
  1. The “base” R system that you download from CRAN
  2. Everything else.
R functionality is divided into a number of packages.
 · The “base” R system contains, among other things, the base package which is required to run R
   and contains the most fundamental functions.
 · The other packages contained in the “base” system include utils, stats, datasets, graphics,
   grDevices, grid, methods, tools, parallel, compiler, splines, tcltk, stats4.
 · There are also “Recommend” packages: boot, class, cluster,                codetools, foreign,
   KernSmooth, lattice, mgcv, nlme, rpart, survival, MASS, spatial, nnet, Matrix.
                                                                                            13/16
 
 Design of the R System
And there are many other packages available:
 · There are about 4000 packages on CRAN that have been developed by users and programmers
   around the world.
 · There are also many packages associated with the Bioconductor project (http://bioconductor.org).
 · People often make packages available on their personal websites; there is no reliable way to
   keep track of how many packages are available in this fashion.
                                                                                              14/16
 
 Some R Resources
Available from CRAN (http://cran.r-project.org)
 · An Introduction to R
 · Writing R Extensions
 · R Data Import/Export
 · R Installation and Administration (mostly for building R from sources)
 · R Internals (not for the faint of heart)
                                                                          15/16
 
 Some Useful Books on S/R
Standard texts
 · Chambers (2008). Software for Data Analysis, Springer. (your textbook)
 · Chambers (1998). Programming with Data, Springer.
 · Venables & Ripley (2002). Modern Applied Statistics with S, Springer.
 · Venables & Ripley (2000). S Programming, Springer.
 · Pinheiro & Bates (2000). Mixed-Effects Models in S and S-PLUS, Springer.
 · Murrell (2005). R Graphics, Chapman & Hall/CRC Press.
Other resources
 · Springer has a series of books called Use R!.
 · A longer list of books is at http://www.r-project.org/doc/bib/R-books.html
                                                                              16/16
"
"./02_RProgramming/profiler/Profiling R Code.pdf","Profiling R Code
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Why is My Code So Slow?
· Profiling is a systematic way to examine how much time is spend in different parts of a program
· Useful when trying to optimize your code
· Often code runs fine once, but what if you have to put it in a loop for 1,000 iterations? Is it still fast
  enough?
· Profiling is better than guessing
                                                                                                        2/17
 
 On Optimizing Your Code
 · Getting biggest impact on speeding up code depends on knowing where the code spends most
   of its time
 · This cannot be done without performance analysis or profiling
We should forget about small eﬃciencies, say about 97% of the time: premature
optimization is the root of all evil
--Donald Knuth
                                                                                        3/17
 
 General Principles of Optimization
· Design first, then optimize
· Remember: Premature optimization is the root of all evil
· Measure (collect data), don’t guess.
· If you're going to be scientist, you need to apply the same principles here!
                                                                               4/17
 
 Using system.time()
· Takes an arbitrary R expression as input (can be wrapped in curly braces) and returns the
  amount of time taken to evaluate the expression
· Computes the time (in seconds) needed to execute an expression
    - If there’s an error, gives time until the error occurred
· Returns an object of class proc_time
    - user time: time charged to the CPU(s) for this expression
    - elapsed time: ""wall clock"" time
                                                                                        5/17
 
 Using system.time()
· Usually, the user time and elapsed time are relatively close, for straight computing tasks
· Elapsed time may be greater than user time if the CPU spends a lot of time waiting around
· Elapsted time may be smaller than the user time if your machine has multiple cores/processors
  (and is capable of using them)
    - Multi-threaded BLAS libraries (vecLib/Accelerate, ATLAS, ACML, MKL)
    - Parallel processing via the parallel package
                                                                                             6/17
 
 Using system.time()
## Elapsed time > user time
system.time(readLines(""http://www.jhsph.edu""))
   user system elapsed
  0.004   0.002   0.431
## Elapsed time < user time
hilbert <- function(n) {
        i <- 1:n
        1 / outer(i - 1, i, ""+”)
}
x <- hilbert(1000)
system.time(svd(x))
   user system elapsed
  1.605   0.094   0.742
                                               7/17
 
 Timing Longer Expressions
system.time({
    n <- 1000
    r <- numeric(n)
    for (i in 1:n) {
        x <- rnorm(n)
        r[i] <- mean(x)
    }
})
##    user  system elapsed
##   0.097   0.002   0.099
                           8/17
 
 Beyond system.time()
· Using system.time() allows you to test certain functions or code blocks to see if they are taking
  excessive amounts of time
· Assumes you already know where the problem is and can call system.time() on it
· What if you don’t know where to start?
                                                                                                9/17
 
 The R Profiler
· The Rprof() function starts the profiler in R
    - R must be compiled with profiler support (but this is usually the case)
· The summaryRprof() function summarizes the output from Rprof() (otherwise it’s not
  readable)
· DO NOT use system.time() and Rprof() together or you will be sad
                                                                                10/17
 
 The R Profiler
· Rprof() keeps track of the function call stack at regularly sampled intervals and tabulates how
  much time is spend in each function
· Default sampling interval is 0.02 seconds
· NOTE: If your code runs very quickly, the profiler is not useful, but then you probably don't need it
  in that case
                                                                                                  11/17
 
 R Profiler Raw Output
## lm(y ~ x)
sample.interval=10000
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""list"" ""eval"" ""eval"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""na.omit"" ""model.frame.default"" ""model.frame"" ""eval"" ""eval"" ""lm""
""lm.fit"" ""lm""
""lm.fit"" ""lm""
""lm.fit"" ""lm""
                                                                            12/17
 
 Using summaryRprof()
· The summaryRprof() function tabulates the R profiler output and calculates how much time is
  spend in which function
· There are two methods for normalizing the data
· ""by.total"" divides the time spend in each function by the total run time
· ""by.self"" does the same but first subtracts out time spent in functions above in the call stack
                                                                                                  13/17
 
 By Total
$by.total
                      total.time total.pct self.time self.pct
""lm""                        7.41    100.00      0.30     4.05
""lm.fit""                    3.50     47.23      2.99    40.35
""model.frame.default""       2.24     30.23      0.12     1.62
""eval""                      2.24     30.23      0.00     0.00
""model.frame""               2.24     30.23      0.00     0.00
""na.omit""                   1.54     20.78      0.24     3.24
""na.omit.data.frame""        1.30     17.54      0.49     6.61
""lapply""                    1.04     14.04      0.00     0.00
""[.data.frame""              1.03     13.90      0.79    10.66
""[""                         1.03     13.90      0.00     0.00
""as.list.data.frame""        0.82     11.07      0.82    11.07
""as.list""                   0.82     11.07      0.00     0.00
                                                              14/17
 
 By Self
$by.self
                        self.time self.pct total.time total.pct
""lm.fit""                     2.99    40.35       3.50     47.23
""as.list.data.frame""         0.82    11.07       0.82     11.07
""[.data.frame""               0.79    10.66       1.03     13.90
""structure""                  0.73     9.85       0.73      9.85
""na.omit.data.frame""         0.49     6.61       1.30     17.54
""list""                       0.46     6.21       0.46      6.21
""lm""                         0.30     4.05       7.41    100.00
""model.matrix.default""       0.27     3.64       0.79     10.66
""na.omit""                    0.24     3.24       1.54     20.78
""as.character""               0.18     2.43       0.18      2.43
""model.frame.default""        0.12     1.62       2.24     30.23
""anyDuplicated.default""      0.02     0.27       0.02      0.27
                                                                15/17
 
 summaryRprof() Output
$sample.interval
[1] 0.02
$sampling.time
[1] 7.41
                      16/17
 
 Summary
· Rprof() runs the profiler for performance of analysis of R code
· summaryRprof() summarizes the output of Rprof() and gives percent of time spent in each
  function (with two types of normalization)
· Good to break your code into functions so that the profiler can give useful information about
  where time is being spent
· C or Fortran code is not profiled
                                                                                           17/17
"
"./02_RProgramming/reading_data_I/ReadingWritingDataPart1.pdf","Reading and Writing Data Part I
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Reading Data
There are a few principal functions reading data into R.
 · read.table, read.csv, for reading tabular data
 · readLines, for reading lines of a text file
 · source, for reading in R code files (inverse of dump)
 · dget, for reading in R code files (inverse of dput)
 · load, for reading in saved workspaces
 · unserialize, for reading single R objects in binary form
                                                            2/9
 
 Writing Data
There are analogous functions for writing data to files
 · write.table
 · writeLines
 · dump
 · dput
 · save
 · serialize
                                                        3/9
 
 Reading Data Files with read.table
The read.table function is one of the most commonly used functions for reading data. It has a few
important arguments:
 · file, the name of a file, or a connection
 · header, logical indicating if the file has a header line
 · sep, a string indicating how the columns are separated
 · colClasses, a character vector indicating the class of each column in the dataset
 · nrows, the number of rows in the dataset
 · comment.char, a character string indicating the comment character
 · skip, the number of lines to skip from the beginning
 · stringsAsFactors, should character variables be coded as factors?
                                                                                               4/9
 
 read.table
For small to moderately sized datasets, you can usually call read.table without specifying any other
arguments
 data <- read.table(""foo.txt"")
R will automatically
 · skip lines that begin with a #
 · figure out how many rows there are (and how much memory needs to be allocated)
 · figure what type of variable is in each column of the table Telling R all these things directly makes
   R run faster and more efficiently.
 · read.csv is identical to read.table except that the default separator is a comma.
                                                                                                      5/9
 
 Reading in Larger Datasets with read.table
With much larger datasets, doing the following things will make your life easier and will prevent R
from choking.
 · Read the help page for read.table, which contains many hints
 · Make a rough calculation of the memory required to store your dataset. If the dataset is larger
   than the amount of RAM on your computer, you can probably stop right here.
 · Set comment.char = """" if there are no commented lines in your file.
                                                                                                 6/9
 
 Reading in Larger Datasets with read.table
· Use the colClasses argument. Specifying this option instead of using the default can make
  ’read.table’ run MUCH faster, often twice as fast. In order to use this option, you have to know the
  class of each column in your data frame. If all of the columns are “numeric”, for example, then
  you can just set colClasses = ""numeric"". A quick an dirty way to figure out the classes of
  each column is the following:
initial <- read.table(""datatable.txt"", nrows = 100)
classes <- sapply(initial, class)
tabAll <- read.table(""datatable.txt"",
                      colClasses = classes)
· Set nrows. This doesn’t make R run faster but it helps with memory usage. A mild overestimate
  is okay. You can use the Unix tool wc to calculate the number of lines in a file.
                                                                                                    7/9
 
 Know Thy System
In general, when using R with larger datasets, it’s useful to know a few things about your system.
 · How much memory is available?
 · What other applications are in use?
 · Are there other users logged into the same system?
 · What operating system?
 · Is the OS 32 or 64 bit?
                                                                                                   8/9
 
 Calculating Memory Requirements
I have a data frame with 1,500,000 rows and 120 columns, all of which are numeric data. Roughly,
how much memory is required to store this data frame?
1,500,000 × 120 × 8 bytes/numeric
= 1440000000 bytes
= 1440000000 / 220 bytes/MB
= 1,373.29 MB
= 1.34 GB
                                                                                             9/9
"
"./02_RProgramming/reading_data_II/ReadWriteDataPart2.pdf","Reading and Writing Data Part II
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Textual Formats
· dumping and dputing are useful because the resulting textual format is edit-able, and in the case
  of corruption, potentially recoverable.
· Unlike writing out a table or csv file, dump and dput preserve the metadata (sacrificing some
  readability), so that another user doesn’t have to specify it all over again.
· Textual formats can work much better with version control programs like subversion or git which
  can only track changes meaningfully in text files
· Textual formats can be longer-lived; if there is corruption somewhere in the file, it can be easier to
  fix the problem
· Textual formats adhere to the “Unix philosophy”
· Downside: The format is not very space-efficient
                                                                                                     2/9
 
 dput-ting R Objects
Another way to pass data around is by deparsing the R object with dput and reading it back in using
dget.
 > y <- data.frame(a = 1, b = ""a"")
 > dput(y)
 structure(list(a = 1,
                b = structure(1L, .Label = ""a"",
                              class = ""factor"")),
           .Names = c(""a"", ""b""), row.names = c(NA, -1L),
           class = ""data.frame"")
 > dput(y, file = ""y.R"")
 > new.y <- dget(""y.R"")
 > new.y
    a b
 1 1 a
                                                                                                 3/9
 
 Dumping R Objects
Multiple objects can be deparsed using the dump function and read back in using source.
 > x <- ""foo""
 > y <- data.frame(a = 1, b = ""a"")
 > dump(c(""x"", ""y""), file = ""data.R"")
 > rm(x, y)
 > source(""data.R"")
 > y
   a b
 1 1 a
 > x
 [1] ""foo""
                                                                                        4/9
 
 Interfaces to the Outside World
Data are read in using connection interfaces. Connections can be made to files (most common) or to
other more exotic things.
 · file, opens a connection to a file
 · gzfile, opens a connection to a file compressed with gzip
 · bzfile, opens a connection to a file compressed with bzip2
 · url, opens a connection to a webpage
                                                                                               5/9
 
 File Connections
> str(file)
function (description = """", open = """", blocking = TRUE,
           encoding = getOption(""encoding""))
· description is the name of the file
· open is a code indicating
    - “r” read only
    - “w” writing (and initializing a new file)
    - “a” appending
    - “rb”, “wb”, “ab” reading, writing, or appending in binary mode (Windows)
                                                                               6/9
 
 Connections
In general, connections are powerful tools that let you navigate files or other external objects. In
practice, we often don’t need to deal with the connection interface directly.
  con <- file(""foo.txt"", ""r"")
  data <- read.csv(con)
  close(con)
is the same as
  data <- read.csv(""foo.txt"")
                                                                                                 7/9
 
 Reading Lines of a Text File
 > con <- gzfile(""words.gz"")
 > x <- readLines(con, 10)
 > x
  [1] ""1080""     ""10-point"" ""10th""     ""11-point""
  [5] ""12-point"" ""16-point"" ""18-point"" ""1st""
  [9] ""2""        ""20-point""
writeLines takes a character vector and writes each element one line at a time to a text file.
                                                                                               8/9
 
 Reading Lines of a Text File
readLines can be useful for reading in lines of webpages
 ## This might take time
 con <- url(""http://www.jhsph.edu"", ""r"")
 x <- readLines(con)
 > head(x)
 [1] ""<!DOCTYPE HTML PUBLIC \""-//W3C//DTD HTML 4.0 Transitional//EN\"">""
 [2] """"
 [3] ""<html>""
 [4] ""<head>""
 [5] ""\t<meta http-equiv=\""Content-Type\"" content=\""text/html;charset=utf-8
                                                                            9/9
"
"./02_RProgramming/simulation/Simulation.pdf","Simulation
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Generating Random Numbers
Functions for probability distributions in R
 · rnorm: generate random Normal variates with a given mean and standard deviation
 · dnorm: evaluate the Normal probability density (with a given mean/SD) at a point (or vector of
   points)
 · pnorm: evaluate the cumulative distribution function for a Normal distribution
 · rpois: generate random Poisson variates with a given rate
                                                                                             2/15
 
 Generating Random Numbers
Probability distribution functions usually have four functions associated with them. The functions are
prefixed with a
 · d for density
 · r for random number generation
 · p for cumulative distribution
 · q for quantile function
                                                                                                   3/15
 
 Generating Random Numbers
Working with the Normal distributions requires using these four functions
  dnorm(x,  mean = 0, sd  =  1, log = FALSE)
  pnorm(q,  mean = 0, sd  =  1, lower.tail = TRUE, log.p = FALSE)
  qnorm(p,  mean = 0, sd  =  1, lower.tail = TRUE, log.p = FALSE)
  rnorm(n,  mean = 0, sd  =  1)
If Φ is the cumulative distribution function for a standard Normal distribution, then pnorm(q) = Φ(q)
and qnorm(p) = Φ−1 (p).
                                                                                                  4/15
 
 Generating Random Numbers
> x <- rnorm(10)
> x
 [1] 1.38380206 0.48772671 0.53403109 0.66721944
 [5] 0.01585029 0.37945986 1.31096736 0.55330472
 [9] 1.22090852 0.45236742
> x <- rnorm(10, 20, 2)
> x
 [1] 23.38812 20.16846 21.87999 20.73813 19.59020
 [6] 18.73439 18.31721 22.51748 20.36966 21.04371
> summary(x)
   Min. 1st Qu. Median     Mean 3rd Qu.    Max.
  18.32   19.73   20.55   20.67   21.67   23.39
                                                  5/15
 
 Generating Random Numbers
Setting the random number seed with set.seed ensures reproducibility
 > set.seed(1)
 > rnorm(5)
 [1] -0.6264538   0.1836433 -0.8356286 1.5952808
 [5] 0.3295078
 > rnorm(5)
 [1] -0.8204684   0.4874291  0.7383247 0.5757814
 [5] -0.3053884
 > set.seed(1)
 > rnorm(5)
 [1] -0.6264538   0.1836433 -0.8356286 1.5952808
 [5] 0.3295078
Always set the random number seed when conducting a simulation!
                                                                     6/15
 
 Generating Random Numbers
Generating Poisson data
 > rpois(10, 1)
  [1] 3 1 0 1 0 0 1 0 1 1
 > rpois(10, 2)
  [1] 6 2 2 1 3 2 2 1 1 2
 > rpois(10, 20)
  [1] 20 11 21 20 20 21 17 15 24 20
 > ppois(2, 2)  ## Cumulative distribution
 [1] 0.6766764  ## Pr(x <= 2)
 > ppois(4, 2)
 [1] 0.947347   ## Pr(x <= 4)
 > ppois(6, 2)
 [1] 0.9954662  ## Pr(x <= 6)
                                           7/15
 
 Generating Random Numbers From a Linear
Model
Suppose we want to simulate from the following linear model
                                         y = β0 + β1 x + ε
where ε ∼ (0, 22 ). Assume x ∼ (0, 12 ), β0 = 0.5 and β1 = 2 .
 > set.seed(20)
 > x <- rnorm(100)
 > e <- rnorm(100, 0, 2)
 > y <- 0.5 + 2 * x + e
 > summary(y)
    Min. 1st Qu. Median
 -6.4080 -1.5400 0.6789   0.6893  2.9300    6.5050
 > plot(x, y)
                                                                 8/15
 
 Generating Random Numbers From a Linear
Model
                                        9/15
 
 Generating Random Numbers From a Linear
Model
What if x is binary?
 > set.seed(10)
 > x <- rbinom(100, 1, 0.5)
 > e <- rnorm(100, 0, 2)
 > y <- 0.5 + 2 * x + e
 > summary(y)
    Min. 1st Qu. Median
 -3.4940 -0.1409 1.5770 1.4320 2.8400 6.9410
 > plot(x, y)
                                             10/15
 
 Generating Random Numbers From a Linear
Model
                                        11/15
 
 Generating Random Numbers From a
Generalized Linear Model
Suppose we want to simulate from a Poisson model where
Y ~ Poisson(μ)
log μ = β0 + β1 x
and β0 = 0.5 and β1 = 0.3. We need to use the rpois function for this
 > set.seed(1)
 > x <- rnorm(100)
 > log.mu <- 0.5 + 0.3 * x
 > y <- rpois(100, exp(log.mu))
 > summary(y)
     Min. 1st Qu. Median    Mean 3rd Qu.     Max.
     0.00    1.00   1.00    1.55     2.00    6.00
 > plot(x, y)
                                                                      12/15
 
 Generating Random Numbers From a
Generalized Linear Model
                                 13/15
 
 Random Sampling
The sample function draws randomly from a specified set of (scalar) objects allowing you to sample
from arbitrary distributions.
  > set.seed(1)
  > sample(1:10, 4)
  [1] 3 4 5 7
  > sample(1:10, 4)
  [1] 3 9 8 5
  > sample(letters, 5)
  [1] ""q"" ""b"" ""e"" ""x"" ""p""
  > sample(1:10) ## permutation
   [1] 4 710 6 9 2 8 3 1 5
  > sample(1:10)
   [1] 2 3 4 1 9 5 10 8 6 7
  > sample(1:10, replace = TRUE) ## Sample w/replacement
   [1] 2 9 7 8 2 8 5 9 7 8
                                                                                              14/15
 
 Simulation
Summary
 · Drawing samples from specific probability distributions can be done with r* functions
 · Standard distributions are built in: Normal, Poisson, Binomial, Exponential, Gamma, etc.
 · The sample function can be used to draw random samples from arbitrary vectors
 · Setting the random number generator seed via set.seed is critical for reproducibility
                                                                                            15/15
"
"./02_RProgramming/Str/str.pdf","The str Function
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
"
"./02_RProgramming/Subsetting/Introduction to the R Language.pdf","7/30/13                                                                            Introduction to the R Language
                          Introduction to the R Language
                          Data Types and Basic Operations
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                1/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting
              There are a number of operators that can be used to extract subsets of R objects.
                 · [always returns an object of the same class as the original; can be used to select more than one
                    element (there is one exception)
                 · [[ is used to extract elements of a list or a data frame; it can only be used to extract a single
                    element and the class of the returned object will not necessarily be a list or data frame
                 · $is used to extract elements of a list or data frame by name; semantics are similar to hat of [[.
                                                                                                                     2/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                        2/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting
                 > x <- c(""a"", ""b"", ""c"", ""c"", ""d"", ""a"")
                 > x[1]
                 [1] ""a""
                 > x[2]
                 [1] ""b""
                 > x[1:4]
                 [1] ""a"" ""b"" ""c"" ""c""
                 > x[x > ""a""]
                 [1] ""b"" ""c"" ""c"" ""d""
                 > u <- x > ""a""
                 >u
                 [1] FALSE TRUE TRUE TRUE TRUE FALSE
                 > x[u]
                 [1] ""b"" ""c"" ""c"" ""d""
                                                                                                                  3/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                     3/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting a Matrix
              Matrices can be subsetted in the usual way with (i,j) type indices.
                 > x <- matrix(1:6, 2, 3)
                 > x[1, 2]
                 [1] 3
                 > x[2, 1]
                 [1] 2
              Indices can also be missing.
                 > x[1, ]
                 [1] 1 3 5
                 > x[, 2]
                 [1] 3 4
                                                                                                                  4/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                     4/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting a Matrix
              By default, when a single element of a matrix is retrieved, it is returned as a vector of length 1 rather
              than a 1 × 1 matrix. This behavior can be turned off by setting drop = FALSE.
                 > x <- matrix(1:6, 2, 3)
                 > x[1, 2]
                 [1] 3
                 > x[1, 2, drop = FALSE]
                          [,1]
                 [1,] 3
                                                                                                                    5/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                       5/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting a Matrix
              Similarly, subsetting a single column or a single row will give you a vector, not a matrix (by default).
                 > x <- matrix(1:6, 2, 3)
                 > x[1, ]
                 [1] 1 3 5
                 > x[1, , drop = FALSE]
                          [,1] [,2] [,3]
                 [1,]          1         3       5
                                                                                                                     6/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                        6/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting Lists
                 6/14
                 ​
                 Subsetting Lists
                 > x <- list(foo = 1:4, bar = 0.6)
                 > x[1]
                 $foo
                 [1] 1 2 3 4
                 > x[[1]]
                 [1] 1 2 3 4
                 > x$bar
                 [1] 0.6
                 > x[[""bar""]]
                 [1] 0.6
                 > x[""bar""]
                 $bar
                 [1] 0.6
                                                                                                                  7/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                     7/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting Lists
                 > x <- list(foo = 1:4, bar = 0.6, baz = ""hello"")
                 > x[c(1, 3)]
                 $foo
                 [1] 1 2 3 4
                 $baz
                 [1] ""hello""
                                                                                                                  8/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                     8/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting Lists
              The [[operator can be used with computed indices; $can only be used with literal names.
                 > x <- list(foo = 1:4, bar = 0.6, baz = ""hello"")
                 > name <- ""foo""
                 > x[[name]] ## computed index for ‘foo’
                 [1] 1 2 3 4
                 > x$name                ## element ‘name’ doesn’t exist!
                 NULL
                 > x$foo
                 [1] 1 2 3 4 ## element ‘foo’ does exist
                                                                                                                  9/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                     9/14
 
 7/30/13                                                                            Introduction to the R Language
              Subsetting Nested Elements of a List
              The [[can take an integer sequence.
                 > x <- list(a = list(10, 12, 14), b = c(3.14, 2.81))
                 > x[[c(1, 3)]]
                 [1] 14
                 > x[[1]][[3]]
                 [1] 14
                 > x[[c(2, 1)]]
                 [1] 3.14
                                                                                                                  10/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                      10/14
 
 7/30/13                                                                            Introduction to the R Language
              Partial Matching
              Partial matching of names is allowed with [[and $.
                 > x <- list(aardvark = 1:5)
                 > x$a
                 [1] 1 2 3 4 5
                 > x[[""a""]]
                 NULL
                 > x[[""a"", exact = FALSE]]
                 [1] 1 2 3 4 5
                                                                                                                  11/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                      11/14
 
 7/30/13                                                                            Introduction to the R Language
              Removing NA Values
              A common task is to remove missing values (NAs).
                 > x <- c(1, 2, NA, 4, NA, 5)
                 > bad <- is.na(x)
                 > x[!bad]
                 [1] 1 2 4 5
                                                                                                                  12/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                      12/14
 
 7/30/13                                                                            Introduction to the R Language
              Removing NA Values
              What if there are multiple things and you want to take the subset with no missing values?
                 > x <- c(1, 2, NA, 4, NA, 5)
                 > y <- c(""a"", ""b"", NA, ""d"", NA, ""f"")
                 > good <- complete.cases(x, y)
                 > good
                 [1] TRUE TRUE FALSE TRUE FALSE TRUE
                 > x[good]
                 [1] 1 2 4 5
                 > y[good]
                 [1] ""a"" ""b"" ""d"" ""f""
                                                                                                                  13/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                      13/14
 
 7/30/13                                                                            Introduction to the R Language
              Removing NA Values
                 > airquality[1:6, ]
                    Ozone Solar.R Wind Temp Month Day
                 1        41           190 7.4 67                        5 1
                 2        36           118 8.0 72                        5 2
                 3        12           149 12.6 74                       5 3
                 4        18           313 11.5 62                       5 4
                 5        NA           NA 14.3 56                        5 5
                 6        28           NA 14.9 66                        5 6
                 > good <- complete.cases(airquality)
                 > airquality[good, ][1:6, ]
                    Ozone Solar.R Wind Temp Month Day
                 1        41           190 7.4 67                        5 1
                 2        36           118 8.0 72                        5 2
                 3        12           149 12.6 74                       5 3
                 4        18           313 11.5 62                       5 4
                 7        23           299 8.6 65                        5 7
                                                                                                                  14/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2b/index.html#1                                      14/14
"
"./02_RProgramming/tapply/tapply.pdf","7/30/13                                                                        Introduction to the R Language
                          Introduction to the R Language
                          Loop Functions - tapply
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                1/14
 
 7/30/13                                                                        Introduction to the R Language
              tapply
              tapplyis used to apply a function over subsets of a vector. I don’t know why it’s called tapply.
                 > str(tapply)
                 function (X, INDEX, FUN = NULL, ..., simplify = TRUE)
                 · Xis a vector
                 · INDEXis a factor or a list of factors (or else they are coerced to factors)
                 · FUNis a function to be applied
                 · ... contains other arguments to be passed FUN
                 · simplify, should we simplify the result?
                                                                                                               2/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                      2/14
 
 7/30/13                                                                        Introduction to the R Language
              tapply
              Take group means.
                 > x <- c(rnorm(10), runif(10), rnorm(10, 1))
                 > f <- gl(3, 10)
                 >f
                  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3
                 [24] 3 3 3 3 3 3 3
                 Levels: 1 2 3
                 > tapply(x, f, mean)
                               1                 2                    3
                 0.1144464 0.5163468 1.2463678
                                                                                                              3/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     3/14
 
 7/30/13                                                                        Introduction to the R Language
              tapply
              Take group means without simplification.
                 > tapply(x, f, mean, simplify = FALSE)
                 $‘1‘
                 [1] 0.1144464
                 $‘2‘
                 [1] 0.5163468
                 $‘3‘
                 [1] 1.246368
                                                                                                              4/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     4/14
 
 7/30/13                                                                        Introduction to the R Language
              tapply
              Find group ranges.
                 > tapply(x, f, range)
                 $‘1‘
                 [1] -1.097309 2.694970
                 $‘2‘
                 [1] 0.09479023 0.79107293
                 $‘3‘
                 [1] 0.4717443 2.5887025
                                                                                                              5/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     5/14
 
 7/30/13                                                                        Introduction to the R Language
              split
              splittakes a vector or other objects and splits it into groups determined by a factor or list of factors.
                 > str(split)
                 function (x, f, drop = FALSE, ...)
                 · xis a vector (or list) or data frame
                 · fis a factor (or coerced to one) or a list of factors
                 · dropindicates whether empty factors levels should be dropped
                                                                                                                    6/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                           6/14
 
 7/30/13                                                                        Introduction to the R Language
              split
                 > x <- c(rnorm(10), runif(10), rnorm(10, 1))
                 > f <- gl(3, 10)
                 > split(x, f)
                 $‘1‘
                  [1] -0.8493038 -0.5699717 -0.8385255 -0.8842019
                  [5] 0.2849881 0.9383361 -1.0973089 2.6949703
                  [9] 1.5976789 -0.1321970
                 $‘2‘
                  [1] 0.09479023 0.79107293 0.45857419 0.74849293
                  [5] 0.34936491 0.35842084 0.78541705 0.57732081
                  [9] 0.46817559 0.53183823
                 $‘3‘
                  [1] 0.6795651 0.9293171 1.0318103 0.4717443
                  [5] 2.5887025 1.5975774 1.3246333 1.4372701
                                                                                                              7/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     7/14
 
 7/30/13                                                                        Introduction to the R Language
              split
              A common idiom is splitfollowed by an lapply.
                 > lapply(split(x, f), mean)
                 $‘1‘
                 [1] 0.1144464
                 $‘2‘
                 [1] 0.5163468
                 $‘3‘
                 [1] 1.246368
                                                                                                              8/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     8/14
 
 7/30/13                                                                        Introduction to the R Language
              Splitting a Data Frame
                 > library(datasets)
                 > head(airquality)
                    Ozone Solar.R Wind Temp Month Day
                 1        41           190 7.4 67                         5 1
                 2        36           118 8.0 72                         5 2
                 3        12           149 12.6 74                        5 3
                 4        18           313 11.5 62                        5 4
                 5        NA             NA 14.3 56                       5 5
                 6        28             NA 14.9 66                       5 6
                                                                                                              9/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                     9/14
 
 7/30/13                                                                        Introduction to the R Language
              Splitting a Data Frame
                 > s <- split(airquality, airquality$Month)
                 > lapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")]))
                 $‘5‘
                       Ozone Solar.R                     Wind
                            NA              NA 11.62258
                 $‘6‘
                         Ozone Solar.R                          Wind
                              NA 190.16667 10.26667
                 $‘7‘
                          Ozone            Solar.R                    Wind
                               NA 216.483871 8.941935
                                                                                                              10/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                      10/14
 
 7/30/13                                                                                  Introduction to the R Language
              Splitting a Data Frame
                 > sapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")]))
                                            5                   6                 7          8                 9
                 Ozone                     NA                 NA                 NA       NA                 NA
                 Solar.R                   NA 190.16667 216.483871                        NA 167.4333
                 Wind          11.62258 10.26667 8.941935 8.793548 10.1800
                 > sapply(s, function(x) colMeans(x[, c(""Ozone"", ""Solar.R"", ""Wind"")],
                                                                               na.rm = TRUE))
                                                 5                          6                7                        8   9
                 Ozone               23.61538                 29.44444             59.115385           59.961538 31.44828
                 Solar.R 181.29630                          190.16667             216.483871 171.857143 167.43333
                 Wind                11.62258                 10.26667              8.941935             8.793548 10.18000
                                                                                                                            11/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                                    11/14
 
 7/30/13                                                                        Introduction to the R Language
              Splitting on More than One Level
                 > x <- rnorm(10)
                 > f1 <- gl(2, 5)
                 > f2 <- gl(5, 2)
                 > f1
                  [1] 1 1 1 1 1 2 2 2 2 2
                 Levels: 1 2
                 > f2
                  [1] 1 1 2 2 3 3 4 4 5 5
                 Levels: 1 2 3 4 5
                 > interaction(f1, f2)
                  [1] 1.1 1.1 1.2 1.2 1.3 2.3 2.4 2.4 2.5 2.5
                 10 Levels: 1.1 2.1 1.2 2.2 1.3 2.3 1.4 ... 2.5
                                                                                                              12/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                      12/14
 
 7/30/13                                                                        Introduction to the R Language
              Splitting on More than One Level
              Interactions can create empty levels.
                 > str(split(x, list(f1, f2)))
                 List of 10
                  $ 1.1: num [1:2] -0.378 0.445
                  $ 2.1: num(0)
                  $ 1.2: num [1:2] 1.4066 0.0166
                  $ 2.2: num(0)
                  $ 1.3: num -0.355
                  $ 2.3: num 0.315
                  $ 1.4: num(0)
                  $ 2.4: num [1:2] -0.907 0.723
                  $ 1.5: num(0)
                  $ 2.5: num [1:2] 0.732 0.360
                                                                                                              13/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                      13/14
 
 7/30/13                                                                        Introduction to the R Language
              split
              Empty levels can be dropped.
                 > str(split(x, list(f1, f2), drop = TRUE))
                 List of 6
                  $ 1.1: num [1:2] -0.378 0.445
                  $ 1.2: num [1:2] 1.4066 0.0166
                  $ 1.3: num -0.355
                  $ 2.3: num 0.315
                  $ 2.4: num [1:2] -0.907 0.723
                  $ 2.5: num [1:2] 0.732 0.360
                                                                                                              14/14
file://localhost/Users/sean/Developer/GitHub/modules/roger/tapply/index.html#1                                      14/14
"
"./02_RProgramming/Vectorized/Introduction to the R Language.pdf","7/30/13                                                                            Introduction to the R Language
                          Introduction to the R Language
                          Vectorized Operations
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2c/index.html#1                                1/3
 
 7/30/13                                                                            Introduction to the R Language
              Vectorized Operations
              Many operations in R are vectorized making code more efficient, concise, and easier to read.
                 > x <- 1:4; y <- 6:9
                 >x+y
                 [1] 7 9 11 13
                 >x>2
                 [1] FALSE FALSE TRUE TRUE
                 > x >= 2
                 [1] FALSE TRUE TRUE TRUE
                 > y == 8
                 [1] FALSE FALSE TRUE FALSE
                 >x*y
                 [1] 6 14 24 36
                 >x/y
                 [1] 0.1666667 0.2857143 0.3750000 0.4444444
                                                                                                                  2/3
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2c/index.html#1                                    2/3
 
 7/30/13                                                                            Introduction to the R Language
              Vectorized Matrix Operations
                 > x <- matrix(1:4, 2, 2); y <- matrix(rep(10, 4), 2, 2)
                 >x*y                      ## element-wise multiplication
                          [,1] [,2]
                 [1,] 10 30
                 [2,] 20 40
                 >x/y
                          [,1] [,2]
                 [1,] 0.1 0.3
                 [2,] 0.2 0.4
                 > x %*% y                 ## true matrix multiplication
                          [,1] [,2]
                 [1,] 40 40
                 [2,] 60 60
                                                                                                                  3/3
file://localhost/Users/sean/Developer/GitHub/modules/roger/Lecture 2c/index.html#1                                    3/3
"
"./03_GettingData/lectures/01_01_obtainingDataMotivation.pdf"," 
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_02_rawAndProcessedData.pdf"," 
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_03_componentsOfTidyData.pdf"," 
  
  
  
  
  
 "
"./03_GettingData/lectures/01_04_downLoadingFiles.pdf"," 
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_05_readingLocalFiles.pdf"," 
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_06_readingExcelFiles.pdf"," 
  
  
  
  
  
 "
"./03_GettingData/lectures/01_07_readingXML.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_08_readingJSON.pdf"," 
  
  
  
  
  
  
 "
"./03_GettingData/lectures/01_09_dataTable.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/02_01_readingMySQL.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/02_02_readingHDF5.pdf"," 
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/02_03_readingFromTheWeb.pdf"," 
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/02_04_readingFromAPIs.pdf"," 
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/02_05_readingFromOtherSources.pdf"," 
  
  
  
  
  
  
 "
"./03_GettingData/lectures/03_01_subsettingAndSorting.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/03_02_summarizingData.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/03_03_creatingNewVariables.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/03_04_reshapingData.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/03_05_mergingData.pdf"," 
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/04_01_editingTextVariables.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/04_02_regularExpressions.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/04_03_regularExpressionsII.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/04_04_workingWithDates.pdf"," 
  
  
  
  
  
  
  
  
 "
"./03_GettingData/lectures/04_05_dataResources.pdf"," 
  
  
  
  
  
  
  
 "
"./04_ExploratoryAnalysis/assets/img/example10.pdf","             10
                        ●
             9                              ●
                            ●
                                ● ● ●
Expression
                  ● ●                                               ● ●
                                                ● ● ●
                                                        ●
                                                                                                                                      ●
             8
                                                                ●
                                        ●
                                                                              ●
                                                            ●                                                 ●
                                                                                                  ● ●                     ●
                                                                          ●                                       ●
                                                                                  ●                     ● ●                   ●
                                                                                                                                              ●
                                                                                      ●       ●                       ●                   ●
             7                                                                                                                    ●
                                                                                          ●
             6
                                                                      Array
"
"./04_ExploratoryAnalysis/assets/img/figure1final.pdf","                                      Expression
                                    Expression                           c                                         Expression
                                                                                                                 Expression          a
                 4     5        6      7           8    9       10         11                    6       8          10     12   14
                               !               !
                            !                      !
                           !                       !                 Batch 1
                           !                   !
Array                                                                      Sample
                                       !                    !
        Sample                                                                          Sample
                                      !                 !
                                           !                !
                                                                     Batch 2
                                               !                !
                                                                         d                                                           b
                                                                                                                   Expression
                                                                                                                 Expression
                                                                                                     4       6       8     10   12       14
                           Normal
                     Normal
                 Normal
                 Normal
                                                                               Sample   Sample
                                               Normal
                                Normal
            Normal
            Normal
"
"./04_ExploratoryAnalysis/clusteringExample/Clustering example.pdf","8/28/13                                                                                                  Clustering example
                            Clustering example
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                   1/18
 
 8/28/13                                                                                                  Clustering example
              Samsung Galaxy S3
              http://www.samsung.com/global/galaxys3/
                                                                                                                            2/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        2/18
 
 8/28/13                                                                                                  Clustering example
              Samsung Data
              http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
                                                                                                                            3/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        3/18
 
 8/28/13                                                                                                  Clustering example
              Slightly processed data
                 download.file(""https://dl.dropboxusercontent.com/u/7710864/courseraPublic/samsungData.rda""
                                             ,destfile=""./data/samsungData.rda"",method=""curl"")
                 load(""./data/samsungData.rda"")
                 names(samsungData)[1:12]
                  [1] ""tBodyAcc-mean()-X"" ""tBodyAcc-mean()-Y"" ""tBodyAcc-mean()-Z"" ""tBodyAcc-std()-X""
                  [5] ""tBodyAcc-std()-Y"" ""tBodyAcc-std()-Z"" ""tBodyAcc-mad()-X"" ""tBodyAcc-mad()-Y""
                  [9] ""tBodyAcc-mad()-Z"" ""tBodyAcc-max()-X"" ""tBodyAcc-max()-Y"" ""tBodyAcc-max()-Z""
                 table(samsungData$activity)
                    laying sitting standing                                 walk walkdown                 walkup
                          1407             1286             1374            1226               986            1073
                                                                                                                            4/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        4/18
 
 8/28/13                                                                                                  Clustering example
              Plotting average acceleration for first subject
                 par(mfrow=c(1,2))
                 numericActivity <- as.numeric(as.factor(samsungData$activity))[samsungData$subject==1]
                 plot(samsungData[samsungData$subject==1,1],pch=19,col=numericActivity,ylab=names(samsungData)[1])
                 plot(samsungData[samsungData$subject==1,2],pch=19,col=numericActivity,ylab=names(samsungData)[2])
                 legend(150,-0.1,legend=unique(samsungData$activity),col=unique(numericActivity),pch=19)
                                                                                                                            5/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        5/18
 
 8/28/13                                                                                                  Clustering example
              Clustering based just on average acceleration
                 source(""http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R"")
                 distanceMatrix <- dist(samsungData[samsungData$subject==1,1:3])
                 hclustering <- hclust(distanceMatrix)
                 myplclust(hclustering,lab.col=numericActivity)
                                                                                                                            6/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        6/18
 
 8/28/13                                                                                                  Clustering example
              Plotting max acceleration for the first subject
                 par(mfrow=c(1,2))
                 plot(samsungData[samsungData$subject==1,10],pch=19,col=numericActivity,ylab=names(samsungData)[10])
                 plot(samsungData[samsungData$subject==1,11],pch=19,col=numericActivity,ylab=names(samsungData)[11])
                                                                                                                            7/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        7/18
 
 8/28/13                                                                                                  Clustering example
              Clustering based on maximum acceleration
                 source(""http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R"")
                 distanceMatrix <- dist(samsungData[samsungData$subject==1,10:12])
                 hclustering <- hclust(distanceMatrix)
                 myplclust(hclustering,lab.col=numericActivity)
                                                                                                                            8/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        8/18
 
 8/28/13                                                                                                  Clustering example
              Singular value decomposition
                 svd1 = svd(scale(samsungData[samsungData$subject==1,-c(562,563)]))
                 par(mfrow=c(1,2))
                 plot(svd1$u[,1],col=numericActivity,pch=19)
                 plot(svd1$u[,2],col=numericActivity,pch=19)
                                                                                                                            9/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                        9/18
 
 8/28/13                                                                                                  Clustering example
              Find maximum contributor
                 plot(svd1$v[,2],pch=19)
                                                                                                                            10/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         10/18
 
 8/28/13                                                                                                  Clustering example
              New clustering with maximum contributer
                 maxContrib <- which.max(svd1$v[,2])
                 distanceMatrix <- dist(samsungData[samsungData$subject==1,c(10:12,maxContrib)])
                 hclustering <- hclust(distanceMatrix)
                 myplclust(hclustering,lab.col=numericActivity)
                                                                                                                            11/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         11/18
 
 8/28/13                                                                                                  Clustering example
              New clustering with maximum contributer
                 names(samsungData)[maxContrib]
                 [1] ""fBodyAcc-meanFreq()-Z""
                                                                                                                            12/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         12/18
 
 8/28/13                                                                                                  Clustering example
              K-means clustering (nstart=1, first try)
                 kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6)
                 table(kClust$cluster,samsungData$activity[samsungData$subject==1])
                          laying sitting standing walk walkdown walkup
                    1              0             34               50        0                0            0
                    2            27               0                0        0                0            0
                    3              0              0                0        0                0          53
                    4              9              2                0        0                0            0
                    5            14              11                3        0                0            0
                    6              0              0                0 95                   49              0
                                                                                                                            13/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         13/18
 
 8/28/13                                                                                                  Clustering example
              K-means clustering (nstart=1, second try)
                 kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=1)
                 table(kClust$cluster,samsungData$activity[samsungData$subject==1])
                          laying sitting standing walk walkdown walkup
                    1              0              0                0 35                      0            0
                    2              5              0                0        0                0          53
                    3            19              13                5        0                0            0
                    4            26              34               48        0                0            0
                    5              0              0                0        0             48              0
                    6              0              0                0 60                      1            0
                                                                                                                            14/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         14/18
 
 8/28/13                                                                                                  Clustering example
              K-means clustering (nstart=100, first try)
                 kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100)
                 table(kClust$cluster,samsungData$activity[samsungData$subject==1])
                          laying sitting standing walk walkdown walkup
                    1            29               0                0        0                0            0
                    2              0              0                0 95                      0            0
                    3              0              0                0        0             49              0
                    4              3              0                0        0                0          53
                    5            18              10                2        0                0            0
                    6              0             37               51        0                0            0
                                                                                                                            15/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         15/18
 
 8/28/13                                                                                                  Clustering example
              K-means clustering (nstart=100, second try)
                 kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100)
                 table(kClust$cluster,samsungData$activity[samsungData$subject==1])
                          laying sitting standing walk walkdown walkup
                    1              0             37               51        0                0            0
                    2              0              0                0        0             49              0
                    3              0              0                0 95                      0            0
                    4            29               0                0        0                0            0
                    5              3              0                0        0                0          53
                    6            18              10                2        0                0            0
                                                                                                                            16/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         16/18
 
 8/28/13                                                                                                  Clustering example
              Cluster 1 Variable Centers (Laying)
                 plot(kClust$center[1,1:10],pch=19,ylab=""Cluster Center"",xlab="""")
                                                                                                                            17/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         17/18
 
 8/28/13                                                                                                  Clustering example
              Cluster 2 Variable Centers (Walking)
                 plot(kClust$center[6,1:10],pch=19,ylab=""Cluster Center"",xlab="""")
                                                                                                                            18/18
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/001clusteringExample/index.html#1                         18/18
"
"./04_ExploratoryAnalysis/Colors/Colors.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./04_ExploratoryAnalysis/Colors/ppt/RColors.pdf","    Plo$ng	  and	  Color	  in	  R	  
                Exploratory	  Data	  Analysis	  
                                 	  
Roger	  D.	  Peng,	  Associate	  Professor	  of	  Biosta4s4cs	  
Johns	  Hopkins	  Bloomberg	  School	  of	  Public	  Health	  
 
                     Plo$ng	  and	  Color	  
• The	  default	  color	  schemes	  for	  most	  plots	  in	  R	  
  are	  horrendous	  
   – I	  don’t	  have	  good	  taste	  and	  even	  I	  know	  that	  
• Recently	  there	  have	  been	  developments	  to	  
  improve	  the	  handling/speciﬁcaEon	  of	  colors	  in	  
  plots/graphs/etc.	  
• There	  are	  funcEons	  in	  R	  and	  in	  external	  
  packages	  that	  are	  very	  handy	  
 
                                      Colors	  1,	  2,	  and	  3	  
                                         ●
                                2                     ●
                                                              ●
                                                                                                                  col	  =	  1	  
                                                              ●                                     ●
                                1                                   ●
                                                                                ●
                    rnorm(10)
                                                 ●            ●
                                                                        ●                 ●
                                     ●
                           ●                                                                      ●
                                0                         ●
                                                              ●
                                                                                                                  col	  =	  2	  
                                                      ●
                                             ●                      ●           ●             ●
                                                              ●
                                                                                              ● ●
                                                                            ●
                                −1
                                                                                                              ●
                                                                                                          ●
                                                                                              ●
                                                              ●
col	  =	  3	  
                                 ●           −1.0    −0.5          0.0              0.5             1.0
                                                                  rnorm(10)
 
                    Default	  Image	  Plots	  in	  R	  
                  heat.colors()	                                   topo.colors()	  
1.0                                               1.0
0.8                                               0.8
0.6                                               0.6
0.4                                               0.4
0.2                                               0.2
0.0                                               0.0
      0.0   0.2       0.4       0.6   0.8   1.0         0.0   0.2     0.4       0.6     0.8   1.0
 
                    Color	  UEliEes	  in	  R	  
• The	  grDevices	  package	  has	  two	  funcEons	  
   – colorRamp!
   – colorRampPalette!
• These	  funcEons	  take	  paleOes	  of	  colors	  and	  
  help	  to	  interpolate	  between	  the	  colors	  
• The	  funcEon	  colors()	  lists	  the	  names	  of	  
  colors	  you	  can	  use	  in	  any	  plo$ng	  funcEon	  
 
          Color	  PaleOe	  UEliEes	  in	  R	  
• colorRamp:	  Take	  a	  paleOe	  of	  colors	  and	  
  return	  a	  funcEon	  that	  takes	  values	  between	  0	  
  and	  1,	  indicaEng	  the	  extremes	  of	  the	  color	  
  paleOe	  (e.g.	  see	  the	  ‘gray’	  funcEon)	  
• colorRampPalette:	  Take	  a	  paleOe	  of	  
  colors	  and	  return	  a	  funcEon	  that	  takes	  integer	  
  arguments	  and	  returns	  a	  vector	  of	  colors	  
  interpolaEng	  the	  paleOe	  (like	  heat.colors
  or	  topo.colors)	  
 
             colorRamp	  
> pal <- colorRamp(c(""red"", ""blue""))!
!                                 Red	  
> pal(0)!
     [,1] [,2] [,3]!              Blue	  
[1,] 255     0     0!
!
                        Green	  
> pal(1)!
     [,1] [,2] [,3]!
[1,]    0    0 255!
!
> pal(0.5)!
      [,1] [,2] [,3]!
[1,] 127.5     0 127.5!
 
         colorRamp	  
!
> pal(seq(0, 1, len = 10))!
            [,1] [,2]      [,3]!
  [1,] 255.00000    0   0.00000!
  [2,] 226.66667    0 28.33333!
  [3,] 198.33333    0 56.66667!
  [4,] 170.00000    0 85.00000!
  [5,] 141.66667    0 113.33333!
  [6,] 113.33333    0 141.66667!
  [7,] 85.00000     0 170.00000!
  [8,] 56.66667     0 198.33333!
  [9,] 28.33333     0 226.66667!
[10,]    0.00000    0 255.00000!
!
 
               colorRampPaleOe	  
> pal <- colorRampPalette(c(""red"", ""yellow""))!
!
> pal(2)!
[1] ""#FF0000"" ""#FFFF00""!
!
> pal(10)!
  [1] ""#FF0000"" ""#FF1C00"" ""#FF3800"" ""#FF5500"" ""#FF7100""!
  [6] ""#FF8D00"" ""#FFAA00"" ""#FFC600"" ""#FFE200"" ""#FFFF00”!
!
!
 
            RColorBrewer	  Package	  
• One	  package	  on	  CRAN	  that	  contains	  
  interesEng/useful	  color	  paleOes	  
• There	  are	  3	  types	  of	  paleOes	  
   – SequenEal	  
   – Diverging	  
   – QualitaEve	  
• PaleOe	  informaEon	  can	  be	  used	  in	  
  conjuncEon	  with	  the	  colorRamp() and	  
  colorRampPalette()!
 
  YlOrRd
  YlOrBr
 YlGnBu
    YlGn
    Reds
   RdPu
 Purples
   PuRd
PuBuGn
   PuBu
    OrRd
Oranges
   Greys
 Greens
   GnBu
   BuPu
   BuGn
   Blues
    Set3
    Set2
    Set1
 Pastel2
 Pastel1
  Paired
   Dark2
  Accent
Spectral
RdYlGn
 RdYlBu
   RdGy
   RdBu
    PuOr
   PRGn
    PiYG
   BrBG
 
 RColorBrewer	  and	  colorRampPaleOe	  
  > library(RColorBrewer)!
  !
  > cols <- brewer.pal(3, ""BuGn"")!
  !
  > cols!
  [1] ""#E5F5F9"" ""#99D8C9"" ""#2CA25F""!
  !
  > pal <- colorRampPalette(cols)!
  !
  > image(volcano, col = pal(20))!
  !
 
 RColorBrewer	  and	  colorRampPaleOe	  
        1.0
        0.8
        0.6
        0.4
        0.2
        0.0
              0.0   0.2   0.4   0.6   0.8   1.0
 
          The	  smoothScaOer	  funcEon	  
                       4
x <- rnorm(10000)!
y <- rnorm(10000)!
smoothScatter(x, y)!   2
!
                   y   0
                       −2
                       −4
                            −2   0       2   4
                                     x
 
        Some	  Other	  Plo$ng	  Notes	  
• The	  rgb funcEon	  can	  be	  used	  to	  produce	  any	  
  color	  via	  red,	  green,	  blue	  proporEons	  
• Color	  transparency	  can	  be	  added	  via	  the	  
  alpha parameter	  to	  rgb!
• The	  colorspace	  package	  can	  be	  used	  for	  a	  
  diﬀerent	  control	  over	  colors	  
 
       ScaOerplot	  with	  no	  transparency	  
                                   3                               ●
                                                                    ●
                                                                        ●● ●● ●● ●                     ●●
                                                      ●            ● ●                                   ●
                                           ●                                  ●●           ●
                                   2                          ● ●          ●
                                                                               ●●
                                                                                     ●       ●        ● ●        ●
                                                  ●                ● ●● ● ●   ●     ● ●              ● ●
                                                      ● ● ●●●   ●● ●●●   ● ●● ●●     ●
                                                       ● ●●●●●     ●
                                                               ●● ● ●●      ●●●
                                                                                ● ●● ●
                                                                               ●●          ●       ● ●             ●
                                                    ●● ● ●
                                                         ●● ●●● ●         ●●     ●● ● ●  ●● ●●● ●  ●●● ●      ●        ●
                                                         ●
                                                       ●●● ●             ● ●
                                                                           ● ●
                                                                             ●●●    ● ● ● ●●
                                                                                           ●   ●  ●
                                                                                                  ●  ●   ●
                                                                                                         ●   ●
                                   1                            ●  ●
                                                               ●● ●● ●          ●●               ●
                                             ●        ●
                                                     ●●●
                                                             ●
                                                            ●●
                                                             ●● ● ●●
                                                                ●    ● ●●●●
                                                                          ●
                                                                          ●
                                                                           ●● ●● ●
                                                                           ●
                                                                           ●      ●●●
                                                                                        ●●●●
                                                                                           ●●● ●●●●● ●●●● ●
                                           ●            ●●●● ●●● ●●
                                                                     ●●
                                                                      ●●●●
                                                                       ●●
                                                                          ●
                                                                          ●●●
                                                                            ●●●●●
                                                                                ●●● ●●●●● ●●●●
                                                                                            ●●●●●
                                                                                                  ●●●●●●●      ● ●
                                             ●
                                                     ●●   ● ●●  ●  ● ●
                                                                       ●  ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●● ●●
                                                                                 ●
                                                                                   ●
                                                                                   ● ●●
                                                                                      ●●●●
                                                                                          ●
                                                                                          ●
                                                                                          ●  ●●
                                                                                              ●●●
                                                                                                ●   ●●    ●● ●●● ●
                                               ●  ●  ● ● ● ● ●  ●  ●
                                                                   ●  ● ●
                                                                        ●
                                                                     ● ●●● ●
                                                                           ●
                                                                           ● ●● ●
                                                                                ● ●  ●●
                                                                                      ●
                                                                                      ●    ●
                                                                                           ●
                                                                                           ●
                                                                                           ●●●
                                                                                            ●●● ●● ●●● ●  ●●
                                                                                                       ● ● ●
                                                             ●●    ● ●     ●●   ●●●● ●
                                                                                     ●  ●●●        ●●●
                                                       ●●●●●●
                                                             ●●●
                                                             ●●
                                                             ●
                                                                 ●
                                                                ●●●●●
                                                                  ●●●
                                                                     ●
                                                                     ● ●●
                                                                      ●●  ●●
                                                                          ●
                                                                              ●●
                                                                             ●●●●
                                                                                ●●●●
                                                                                      ●
                                                                                      ●
                                                                                      ●●●
                                                                                        ●●●
                                                                                       ●●  ●
                                                                                           ●●
                                                                                            ●●●●
                                                                                             ●
                                                                                             ●    ● ●●     ●●● ●
                                   0           ● ● ●● ●●       ● ●● ●●
                                                                     ●
                                                                     ●
                                                                     ●●●●● ●
                                                                           ●● ● ● ●  ●
                                                                                     ●● ●
                                                                                        ●●● ●  ●       ●
                                                                                                       ●● ●      ●
                           y                  ● ●●●●●●●●●●    ●     ●●●●●●
                                                                         ●●    ●
                                                                              ●●  ●
                                                                                  ●●●●
                                                                                     ●●
                                                                                      ●    ● ● ●     ●       ●
                                                ●● ●●●● ●●●
                                                          ●● ●
                                                              ●●
                                                               ●●●
                                                                   ●
                                                                  ●● ●
                                                                   ●●
                                                                      ●
                                                                      ● ●
                                                                        ●
                                                                         ●●●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                  ●●●
                                                                                  ●  ●
                                                                                     ●
                                                                                      ●
                                                                                      ●●● ●
                                                                                        ●●●●
                                                                                          ● ●●
                                                                                             ●
                                                                                              ●●
                                                                                               ●
                                                                                               ●●
                                                                                                ●
                                                                                                 ● ●●
                                                                                                 ● ●● ● ●● ● ● ●
                                                     ●●
                                                      ●● ●          ●●       ●●●●●● ●●●
                                                                                      ●●        ●
                                                                                          ● ●●●●●     ●
                                                ● ●●  ●
                                                     ●●●●●●●●● ● ●●  ●●●●
                                                                        ●●
                                                                        ● ●●
                                                                          ● ●●●      ●  ●●●
                                                                                         ●        ●●● ●
                                                                                                      ● ●●
                                                  ●       ●
                                                                     ●
                                                                     ●
                                                                     ● ●
                                                                       ●   ●  ●●●●●●●●●
                                                                                      ●  ●●●●
                                                                                         ●●  ●●●● ●●   ●  ● ●● ●●
                                                 ● ● ● ●●    ●
                                                                ●●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                    ●●   ●
                                                                      ●● ● ●
                                                                           ●
                                                                             ●
                                                                             ●
                                                                              ● ●●●
                                                                              ●●● ●   ●●
                                                                                       ●●● ● ●
                                                                                              ●
                                                                                               ●     ●  ● ●● ●
                                   −1         ● ● ●
                                                    ●● ● ● ●
                                                            ● ● ● ●●● ●
                                                                    ● ●
                                                                   ●●
                                                                         ●●
                                                                           ●●●●
                                                                               ●● ● ● ●  ●     ●
                                                                                          ●● ●● ●
                                                                                              ● ● ●●
                                                                                                    ●      ●
                                                                                                     ● ● ●● ● ●
                                                                                                     ●
                                                                                                                ● ●●
                                                                                                                     ●
                                                                         ● ●      ●●●                      ●
                                        ●           ●      ● ● ●●●● ●    ● ● ●●●   ● ●
                                                                                     ●●●● ● ●
                                                                                       ● ●● ●
                                                  ●      ●●            ● ●●      ●  ●●      ● ●●         ●●
                                                               ●●●●        ●●●
                                                                             ●
                                                                             ●● ●  ●●●●       ●●              ●
                                   −2     ●●
                                              ●              ●          ●
                                                                 ● ● ●● ●
                                                                         ●
                                                                         ●● ●          ● ●
                                                                                           ● ●
                                                                                                      ●
                                                                                                           ●
                                                                      ●               ●
                                                        ●● ●
                                                                                           ●
                                                                                      ●
                                   −3                             ●
                                                                                              ●
                                        −3         −2          −1            0           1            2           3
                                                                               x
plot(x,	  y,	  pch	  =	  19)	  
 
              ScaOerplot	  with	  transparency	  
                                    3
                                    2
                                    1
                          y         0
                                    −1
                                    −2
                                    −3
                                              −3          −2           −1         0   1   2   3
                                                                                  x
plot(x,	  y,	  col	  =	  rgb(0,	  0,	  0,	  0.2),	  pch	  =	  19)	  
 
                                  Summary	  
• Careful	  use	  of	  colors	  in	  plots/maps/etc.	  can	  make	  
  it	  easier	  for	  the	  reader	  to	  get	  what	  you’re	  trying	  
  to	  say	  (why	  make	  it	  harder?)	  
• The	  RColorBrewer	  package	  is	  an	  R	  package	  that	  
  provides	  color	  paleOes	  for	  sequenEal,	  
  categorical,	  and	  diverging	  data	  
• The	  colorRamp	  and	  colorRampPalette	  
  funcEons	  can	  be	  used	  in	  conjuncEon	  with	  color	  
  paleOes	  to	  connect	  data	  to	  colors	  
• Transparency	  can	  someEmes	  be	  used	  to	  clarify	  
  plots	  with	  many	  points	  
"
"./04_ExploratoryAnalysis/dimensionReduction/Principal components analysis and singular value decomposition.pdf","8/28/13                                                                             Principal components analysis and singular value decomposition
                            Principal components analysis and
                            singular value decomposition
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                         1/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Matrix data
                 set.seed(12345); par(mar=rep(0.2,4))
                 dataMatrix <- matrix(rnorm(400),nrow=40)
                 image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
                                                                                                                                                   2/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              2/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Cluster the data
                 par(mar=rep(0.2,4))
                 heatmap(dataMatrix)
                                                                                                                                                   3/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              3/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              What if we add a pattern?
                 set.seed(678910)
                 for(i in 1:40){
                    # flip a coin
                    coinFlip <- rbinom(1,size=1,prob=0.5)
                    # if coin is heads add a common pattern to that row
                    if(coinFlip){
                          dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,3),each=5)
                    }
                 }
                                                                                                                                                   4/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              4/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              What if we add a pattern? - the data
                 par(mar=rep(0.2,4))
                 image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
                                                                                                                                                   5/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              5/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              What if we add a pattern? - the clustered data
                 par(mar=rep(0.2,4))
                 heatmap(dataMatrix)
                                                                                                                                                   6/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              6/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Patterns in rows and columns
                 hh <- hclust(dist(dataMatrix)); dataMatrixOrdered <- dataMatrix[hh$order,]
                 par(mfrow=c(1,3))
                 image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
                 plot(rowMeans(dataMatrixOrdered),40:1,,xlab=""Row"",ylab=""Row Mean"",pch=19)
                 plot(colMeans(dataMatrixOrdered),xlab=""Column"",ylab=""Column Mean"",pch=19)
                                                                                                                                                   7/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              7/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Related problems
              You have multivariate variables X , Q , X so X                1             n          1  = (X11 ,        Q, X      1m )
                 · Find a new set of multivariate variables that are uncorrelated and explain as much variance as
                    possible.
                 · If you put all the variables together in one matrix, find the best matrix created with fewer variables
                    (lower rank) that explains the original data.
              The first goal is statistical and the second goal is data compression.
                                                                                                                                                   8/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                              8/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Related solutions - PCA/SVD
              SVD
              If X is a matrix with each variable in a column and each observation in a row then the SVD is a ""matrix
              decomposition""
                                                                                                                     T
                                                                                                  X = U DV
              where the columns of U are orthogonal (left singular vectors), the columns of                                                        V are orthogonal (right
              singluar vectors) and D is a diagonal matrix (singular values).
              PCA
              The principal components are equal to the right singular values if you first scale (subtract the mean,
              divide by the standard deviation) the variables.
                                                                                                                                                                       9/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                                                  9/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Components of the SVD - u and v
                 svd1 <- svd(scale(dataMatrixOrdered))
                 par(mfrow=c(1,3))
                 image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
                 plot(svd1$u[,1],40:1,,xlab=""Row"",ylab=""First left singular vector"",pch=19)
                 plot(svd1$v[,1],xlab=""Column"",ylab=""First right singular vector"",pch=19)
                                                                                                                                                   10/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               10/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Components of the SVD - d and variance
              explained
                 svd1 <- svd(scale(dataMatrixOrdered))
                 par(mfrow=c(1,2))
                 plot(svd1$d,xlab=""Column"",ylab=""Singluar value"",pch=19)
                 plot(svd1$d^2/sum(svd1$d^2),xlab=""Column"",ylab=""Percent of variance explained"",pch=19)
                                                                                                                                                   11/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               11/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Relationship to principal components
                 svd1 <- svd(scale(dataMatrixOrdered))
                 pca1 <- prcomp(dataMatrixOrdered,scale=TRUE)
                 plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab=""Principal Component 1"",ylab=""Right Singular Vector 1"")
                 abline(c(0,1))
                                                                                                                                                   12/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               12/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Components of the SVD - variance explained
                 constantMatrix <- dataMatrixOrdered*0
                 for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1),each=5)}
                 svd1 <- svd(constantMatrix)
                 par(mfrow=c(1,3))
                 image(t(constantMatrix)[,nrow(constantMatrix):1])
                 plot(svd1$d,xlab=""Column"",ylab=""Singluar value"",pch=19)
                 plot(svd1$d^2/sum(svd1$d^2),xlab=""Column"",ylab=""Percent of variance explained"",pch=19)
                                                                                                                                                   13/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               13/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              What if we add a second pattern?
                 set.seed(678910)
                 for(i in 1:40){
                    # flip a coin
                    coinFlip1 <- rbinom(1,size=1,prob=0.5)
                    coinFlip2 <- rbinom(1,size=1,prob=0.5)
                    # if coin is heads add a common pattern to that row
                    if(coinFlip1){
                          dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5),each=5)
                    }
                    if(coinFlip2){
                          dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5),5)
                    }
                 }
                 hh <- hclust(dist(dataMatrix)); dataMatrixOrdered <- dataMatrix[hh$order,]
                                                                                                                                                   14/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               14/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Singular value decomposition - true patterns
                 svd2 <- svd(scale(dataMatrixOrdered))
                 par(mfrow=c(1,3))
                 image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
                 plot(rep(c(0,1),each=5),pch=19,xlab=""Column"",ylab=""Pattern 1"")
                 plot(rep(c(0,1),5),pch=19,xlab=""Column"",ylab=""Pattern 2"")
                                                                                                                                                   15/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               15/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              v and patterns of variance in rows
                 svd2 <- svd(scale(dataMatrixOrdered))
                 par(mfrow=c(1,3))
                 image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
                 plot(svd2$v[,1],pch=19,xlab=""Column"",ylab=""First right singluar vector"")
                 plot(svd2$v[,2],pch=19,xlab=""Column"",ylab=""Second right singluar vector"")
                                                                                                                                                   16/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               16/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              d and variance explained
                 svd1 <- svd(scale(dataMatrixOrdered))
                 par(mfrow=c(1,2))
                 plot(svd1$d,xlab=""Column"",ylab=""Singluar value"",pch=19)
                 plot(svd1$d^2/sum(svd1$d^2),xlab=""Column"",ylab=""Percent of variance explained"",pch=19)
                                                                                                                                                   17/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               17/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              fast.svd function {corpcor}
              Important parameters: m,tol
                 bigMatrix <- matrix(rnorm(1e4*40),nrow=1e4)
                 system.time(svd(scale(bigMatrix)))
                       user system elapsed
                    0.109 0.012 0.123
                 system.time(fast.svd(scale(bigMatrix),tol=0))
                 Timing stopped at: 0 0 0
                                                                                                                                                   18/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               18/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Missing values
                 dataMatrix2 <- dataMatrixOrdered
                 dataMatrix2[sample(1:100,size=40,replace=F)] <- NA
                 svd1 <- svd(scale(dataMatrix2))
                 Error: infinite or missing values in 'x'
                                                                                                                                                   19/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               19/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Imputing {impute}
                 library(impute)
                 dataMatrix2 <- dataMatrixOrdered
                 dataMatrix2[sample(1:100,size=40,replace=F)] <- NA
                 dataMatrix2 <- impute.knn(dataMatrix2)$data
                 svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
                 par(mfrow=c(1,2)); plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19)
                                                                                                                                                   20/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               20/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Face example
                 download.file(""https://spark-public.s3.amazonaws.com/dataanalysis/face.rda"",destfile=""./data/face.rda"",metho
                 load(""./data/face.rda"")
                 image(t(faceData)[,nrow(faceData):1])
                                                                                                                                                   21/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               21/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Face example - variance explained
                 svd1 <- svd(scale(faceData))
                 plot(svd1$d^2/sum(svd1$d^2),pch=19,xlab=""Singluar vector"",ylab=""Variance explained"")
                                                                                                                                                   22/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               22/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Face example - create approximations
                 svd1 <- svd(scale(faceData))
                 # %*% is matrix multiplication
                 # Here svd1$d[1] is a constant
                 approx1 <- svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1]
                 # In these examples we need to make the diagonal matrix out of d
                 approx5 <- svd1$u[,1:5] %*% diag(svd1$d[1:5])%*% t(svd1$v[,1:5])
                 approx10 <- svd1$u[,1:10] %*% diag(svd1$d[1:10])%*% t(svd1$v[,1:10])
                                                                                                                                                   23/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               23/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Face example - plot approximations
                 par(mfrow=c(1,4))
                 image(t(faceData)[,nrow(faceData):1])
                 image(t(approx10)[,nrow(approx10):1])
                 image(t(approx5)[,nrow(approx5):1])
                 image(t(approx1)[,nrow(approx1):1])
                                                                                                                                                   24/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               24/25
 
 8/28/13                                                                             Principal components analysis and singular value decomposition
              Notes and further resources
                 · Scale matters
                 · PC's/SV's may mix real patterns
                 · Can be computationally intensive
                 · Advanced data analysis from an elementary point of view
                 · Elements of statistical learning
                 · Alternatives
                          - Factor analysis
                          - Independent components analysis
                          - Latent semantic analysis
                                                                                                                                                   25/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/006dimensionReduction/index.html#1                                               25/25
"
"./04_ExploratoryAnalysis/exploratoryGraphs/data/PUMSDataDict06.pdf","                        DATA DICTIONARY - 2006 HOUSING
RT          1
     Record Type
            H .Housing Record or Group Quarters Unit
SERIALNO    7
     Housing unit/GQ person serial number
            0000001..9999999 .Unique identifier assigned within
                              .state
DIVISION    1
     Division code
            0 .Puerto Rico
            1 .New England (Northeast region)
            2 .Middle Atlantic (Northeast region)
            3 .East North Central (Midwest region)
            4 .West North Central (Midwest region)
            5 .South Atlantic (South region)
            6 .East South Central (South region)
            7 .West South Central (South Region)
            8 .Mountain (West region)
            9 .Pacific (West region)
PUMA        5
     Public use microdata area code (PUMA)
     Designates area of 100,000 or more population. Use with ST for unique
     code.
            00100..08200
            77777 .combination of 01801, 01802, and 01905 in Louisiana
REGION      1
     Region code
            1 .Northeast
            2 .Midwest
            3 .South
            4 .West
            9 .Puerto Rico
ST          2
     State Code
            01 .Alabama/AL
            02 .Alaska/AK
            04 .Arizona/AZ
            05 .Arkansas/AR
            06 .California/CA
            08 .Colorado/CO
            09 .Connecticut/CT
            10 .Delaware/DE
            11 .District of Columbia/DC
            12 .Florida/FL
            13 .Georgia/GA
            15 .Hawaii/HI
            16 .Idaho/ID
            17 .Illinois/IL
            18 .Indiana/IN
            19 .Iowa/IA
            20 .Kansas/KS
            21 .Kentucky/KY
            22 .Louisiana/LA
            23 .Maine/ME
 
             24  .Maryland/MD
            25  .Massachusetts/MA
            26  .Michigan/MI
            27  .Minnesota/MN
            28  .Mississippi/MS
            29  .Missouri/MO
            30  .Montana/MT
            31  .Nebraska/NE
            32  .Nevada/NV
            33  .New Hampshire/NH
            34  .New Jersey/NJ
            35  .New Mexico/NM
            36  .New York/NY
            37  .North Carolina/NC
            38  .North Dakota/ND
            39  .Ohio/OH
            40  .Oklahoma/OK
            41  .Oregon/OR
            42  .Pennsylvania/PA
            44  .Rhode Island/RI
            45  .South Carolina/SC
            46  .South Dakota/SD
            47  .Tennessee/TN
            48  .Texas/TX
            49  .Utah/UT
            50  .Vermont/VT
            51  .Virginia/VA
            53  .Washington/WA
            54  .West Virginia/WV
            55  .Wisconsin/WI
            56  .Wyoming/WY
            72  .Puerto Rico/PR
ADJUST      7
     Adjustment factor for dollar amounts (6 implied decimal places)
            1015675 .2006 factor (1.015675)
WGTP        4
     Housing Weight
            0001..9999 .Integer weight of housing unit
NP          2
     Number of person records following this housing record
                 00 .Vacant unit
                 01 .One person record (one person in household or
                    .any person in group quarters)
            02..20 .Number of person records (number of persons in
                    .household)
TYPE        1
     Type of unit
            1 .Housing unit
            2 .Institutional group quarters
            3 .Noninstitutional group quarters
ACR         1
     Lot size
            b  .N/A (GQ/not a one-family house or mobile home)
            1  .House on less than one acre
            2  .House on one to less than ten acres
            3  .House on ten or more acres
 
 AGS         1
     Sales of Agriculture Products
            b .N/A (less than 1 acre/GQ/vacant/
              .2 or more units in structure)
            1 .None
            2 .$     1 - $ 999
            3 .$ 1000 - $ 2499
            4 .$ 2500 - $ 4999
            5 .$ 5000 - $ 9999
            6 .$10000+
BDS         1
     Bedrooms
            b .N/A (GQ)
            0 .No bedrooms
            1 .1 Bedroom
            2 .2 Bedrooms
            3 .3 Bedrooms
            4 .4 Bedrooms
            5 .5 or more bedrooms
BLD         2
     Units in structure
            bb .N/A (GQ)
            01 .Mobile home or trailer
            02 .One-family house detached
            03 .One-family house attached
            04 .2 Apartments
            05 .3-4 Apartments
            06 .5-9 Apartments
            07 .10-19 Apartments
            08 .20-49 Apartments
            09 .50 or more apartments
            10 .Boat, RV, van, etc.
BUS         1
     Business or medical office on property
            b .N/A (GQ/not a one-family house or mobile home)
            1 .Yes
            2 .No
CONP        4
     Condo fee (monthly amount)
                   bbbb .N/A (not owned or being bought/not
                        .condo/GQ/vacant/no condo fee)
            0001..9999 .$1 - $9999 (Rounded and top-coded)
ELEP        3
     Electricity (monthly cost)
                  bbb .N/A (GQ/vacant)
                  001 .Included in rent or in condo fee
                  002 .No charge or electricity not used
            003..999 .$3 to $999 (Rounded and top-coded)
FS          5
     Food stamp amount
               bbbbb .N/A (vacant)
                    0 .None
            1..99999 .$1 to $99999
FULP        4
 
      House heating fuel (yearly cost)
                   bbbb .N/A (GQ/vacant)
                   0001 .Included in rent or in condo fee
                   0002 .No charge or these fuels not used
            0003..9999 .$3 to $9999 (Rounded and top-coded)
GASP        3
     Gas (monthly cost)
                  bbb .N/A (GQ/vacant)
                  001 .Included in rent or in condo fee
                  002 .Included in electricity payment
                  003 .No charge or gas not used
            004..999 .$4 to $999 (Rounded and top-coded)
HFL         1
     House heating fuel
            b .N/A (GQ/vacant)
            1 .Utility gas
            2 .Bottled, tank, or LP gas
            3 .Electricity
            4 .Fuel oil, kerosene, etc.
            5 .Coal or coke
            6 .Wood
            7 .Solar energy
            8 .Other fuel
            9 .No fuel used
INSP        4
     Fire/hazard/flood insurance (yearly amount)
                   bbbb .N/A (not owned or being bought/not a one
                        .family house, mobile home, or
                        .condo/GQ/vacant)
                   0000 .None
            0001..9999 .$1 to $9999 (Rounded and top-coded)
KIT         1
     Complete kitchen facilities
            b .N/A (GQ)
            1 .Yes, has all three facilities
            2 .No
MHP         5
     Mobile home costs (yearly amount)
                    bbbbb .N/A (GQ/vacant/not owned or being bought/
                          .not mobile home/no costs)
            00000..99999 .$0 to $99999 (Rounded and top-coded)
MRGI        1
     Payment include fire/hazard/flood insurance
            b .N/A (GQ/vacant/not owned or being bought/
              .Not a one family house, MHT or condo/not
              .mortgaged/no regular mortgage payment)
            1 .Yes, insurance included in payment
            2 .No, insurance paid separately or no insurance
MRGP        5
     Mortgage payment (monthly amount)
                    bbbbb .N/A (not owned or being bought/not a one
                          .family house, mobile home, or
                          .condo/GQ/vacant)
            00001..99999 .$1 to $99999 (Rounded and top-coded)
 
 MRGT        1
     Payment include real estate taxes
            b .N/A (GQ/vacant/not owned or being bought/not a
              .one family house or condo/not mortgaged/
              .No regular mortgage payment)
            1 .Yes, taxes included in payment
            2 .No, taxes paid separately or taxes not required
MRGX        1
     Mortgage status
            b .N/A (not owned or being bought/not a one family
              .house, mobile home, or condo/GQ/vacant)
            1 .Mortgage deed of trust, or similar debt
            2 .Contract to purchase
            3 .None
PLM         1
     Complete plumbing facilities
            b .N/A (GQ)
            1 .Yes, has all three facilities
            2 .No
RMS         1
     Rooms
            b .N/A (GQ)
            1 .1 Room
            2 .2 Rooms
            3 .3 Rooms
            4 .4 Rooms
            5 .5 Rooms
            6 .6 Rooms
            7 .7 Rooms
            8 .8 Rooms
            9 .9 or more rooms
RNTM        1
     Meals included in rent
            b .N/A (GQ/not a rental unit/rental-NCR)
            1 .Yes
            2 .No
RNTP        5
     Monthly rent
                    bbbbb .N/A (GQ/not a rental unit)
            00001..99999 .$1 to $99999 (Rounded and top-coded)
SMP         5
     Second mortgage payment (monthly amount)
                    bbbbb .N/A (GQ/vacant/condo/not owned or being
                          .bought/not a one family house/not
                          .mortgaged/ no second mortgage)
            00001..99999 .$1 to $99999 (Rounded and top-coded)
TEL         1
     Telephone in Unit
            b .N/A (GQ/vacant)
            1 .Yes
            2 .No
TEN         1
 
      Tenure
            b .N/A (GQ/vacant)
            1 .Owned with mortgage or loan
            2 .Owned free and clear
            3 .Rented for cash rent
            4 .No cash rent
VACS        1
     Vacancy status
            b .N/A (occupied/GQ)
            1 .For rent
            2 .Rented, not occupied
            3 .For sale only
            4 .Sold, not occupied
            5 .For seasonal/recreational/occasional use
            6 .For migratory workers
            7 .Other vacant
VAL         2
     Property value
            bb .N/A (GQ/rental unit/vacant, not for sale only)
            01 .Less than $ 10000
            02 .$ 10000 - $ 14999
            03 .$ 15000 - $ 19999
            04 .$ 20000 - $ 24999
            05 .$ 25000 - $ 29999
            06 .$ 30000 - $ 34999
            07 .$ 35000 - $ 39999
            08 .$ 40000 - $ 49999
            09 .$ 50000 - $ 59999
            10 .$ 60000 - $ 69999
            11 .$ 70000 - $ 79999
            12 .$ 80000 - $ 89999
            13 .$ 90000 - $ 99999
            14 .$100000 - $124999
            15 .$125000 - $149999
            16 .$150000 - $174999
            17 .$175000 - $199999
            18 .$200000 - $249999
            19 .$250000 - $299999
            20 .$300000 - $399999
            21 .$400000 - $499999
            22 .$500000 - $749999
            23 .$750000 - $999999
            24 .$1000000+
VEH         1
     Vehicles (1 ton or less) available
            b .N/A (GQ/vacant)
            0 .No vehicles
            1 .1 vehicle
            2 .2 vehicles
            3 .3 vehicles
            4 .4 vehicles
            5 .5 vehicles
            6 .6 or more vehicles
WATP        4
     Water (yearly cost)
                  bbbb .N/A (GQ/vacant)
                  0001 .Included in rent or in condo fee
 
                    0002 .No charge
            0003..9999 .$3 to $9999 (Rounded and top-coded)
YBL         1
    When structure first built
            b .N/A (GQ)
            1 .2005 or later
            2 .2000 to 2004
            3 .1990 to 1999
            4 .1980 to 1989
            5 .1970 to 1979
            6 .1960 to 1969
            7 .1950 to 1959
            8 .1940 to 1949
            9 .1939 or earlier
FES         1
    Family type and employment status
            b .N/A (GQ/vacant/not a family)
            1 .Married-couple family: Husband and wife in LF
            2 .Married-couple family: Husband in labor force, wife
              .not in LF
            3 .Married-couple family: Husband not in LF,
              .wife in LF
            4 .Married-couple family: Neither husband nor wife in
              .LF
            5 .Other family: Male householder, no wife present, in
              .LF
            6 .Other family: Male householder, no wife present,
              .not in LF
            7 .Other family: Female householder, no husband
              .present, in LF
            8 .Other family: Female householder, no husband
              .present, not in LF
FINCP       8
    Family income
                     bbbbbbbb .N/A(GQ/vacant)
                     00000000 .No family income
            -59999..99999999 .Total family income in dollars
FPARC       1
      Family presence and age of related children
            b .N/A (GQ/vacant/not a family)
            1 .With related children under 5 years only
            2 .With related children 5 to 17 years only
            3 .With related children under 5 years and 5 to 17
              .years
            4 .No related children
GRNTP       4
    Gross rent
                   bbbb .N/A (GQ/vacant, not rented for cash rent)
            0001..9999 .$1 - $9999
GRPIP       3
    Gross rent as a percentage of household income past 12 months
                  bbb .N/A (GQ/vacant/not rented for cash
                      .rent/owner occupied/no household income)
            001..100 .1% to 100%
                  101 .101% or more
 
 HHL         1
     Household language
            b .N/A (GQ/vacant)
            1 .English only
            2 .Spanish
            3 .Other Indo-European language
            4 .Asian or Pacific Island language
            5 .Other language
HHT         1
     Household/family type
            b .N/A (GQ/vacant)
            1 .Married-couple family household
              .Other family household:
            2 .Male householder, no wife present
            3 .Female householder, no husband present
              .Nonfamily household:
              .Male householder:
            4 .Living alone
            5 .Not living alone
              .Female householder:
            6 .Living alone
            7 .Not living alone
HINCP       8
     Household income
                    bbbbbbbb .N/A(GQ/vacant)
                    00000000 .No household income
            -59999..99999999 .Total household income in dollars
HUGCL       1
      Flag to indicate grandchild living in housing unit
            b .N/A (GQ/vacant)
            0 .HU does not contain grandchildren
            1 .HU does contain grandchildren
HUPAC       1
      Family presence and age of children
            b .N/A (GQ/vacant)
            1 .With children under 6 years only
            2 .With children 6 to 17 years only
            3 .With children under 6 years and 6 to 17 years
            4 .No children
HUPAOC      1
      HH presence and age of own children
            b .N/A (GQ/vacant)
            1 .Presence of own children under 6 years only
            2 .Presence of own children 6 to 17 years only
            3 .Presence of own children under 6 years and 6 to 17 years
            4 .No own children present
HUPARC      1
      HH presence and age of related children
            b .N/A (GQ/vacant)
            1 .Presence of related children under 6 years only
            2 .Presence of related children 6 to 17 years only
            3 .Presence of related children under 6 years and 6 to 17 years
            4 .No related children present
LNGI        1
 
     Linguistic isolation
            b .N/A (GQ/vacant)
            1 .Not linguistically isolated
            2 .Linguistically isolated
MV          1
    When moved into this house or apartment
            b .N/A (GQ/vacant)
            1 .12 months or less
            2 .13 to 23 months
            3 .2 to 4 years
            4 .5 to 9 years
            5 .10 to 19 years
            6 .20 to 29 years
            7 .30 years or more
NOC         2
    Number of own   children in household (unweighted)
                bb  .N/A(GQ/vacant)
                00  .No own children
            01..19  .Number of own children in household
NPF         2
    Number of persons in family (unweighted)
                bb .N/A (GQ/vacant/non-family household)
            02..20 .Number of persons in family
NPP         1
      Grandparent headed household with no parent present
            b .N/A (GQ/vacant)
            0 .Not a grandparent headed household with no parent present
            1 .Grandparent headed household with no parent present
NR          1
      Presence of nonrelative in household
            b .N/A (GQ/vacant)
            0 .None
            1 .1 or more nonrelatives
NRC         2
    Number of related children in household (unweighted)
                bb .N/A (GQ/vacant)
                00 .No related children
            01..19 .Number of related children in household
OCPIP
            3
    Selected monthly owner costs as a percentage of household
    income during the past 12 months
                 bbb .N/A (not owned or being bought/not a one
                      .family house, mobile home, or
                      .condo/GQ/vacant/no HH income)
            001..100 .1% to 100%
                 101 .101% or more
PARTNER     1
      Unmarried partner household
            b .N/A (GQ/vacant)
            0 .No unmarried partner in household
            1 .Male householder, male partner
            2 .Male householder, female partner
 
             3 .Female householder, male partner
            4 .Female householder, female partner
PSF         1
     Presence of subfamilies in Household
            b .N/A (GQ/vacant)
            0 .No subfamilies
            1 .1 or more subfamilies
R18         1
     Presence of persons under 18 years in household (unweighted)
            b .N/A (GQ/vacant)
            0 .No person under 18 in household
            1 .1 or more persons under 18 in household
R60         1
     Presence of persons 60 years and over in household (unweighted)
            b .N/A (GQ/vacant)
            0 .No person 60 and over
            1 .1 person 60 and over
            2 .2 or more persons 60 and over
R65         1
     Presence of persons 65 years and over in household (unweighted)
            b .N/A (GQ/vacant)
            0 .No person 65 and over
            1 .1 person 65 and over
            2 .2 or more persons 65 and over
RESMODE     1
      Response mode
            b .N/A (GQ)
            1 .Mail
            2 .CATI/CAPI
SMOCP       5
     Selected monthly owner costs
                    bbbbb .N/A (not owned or being bought/not a one
                          .family house, mobile home, or
                          .condo/GQ/vacant/no costs )
                    00000 .No costs
            00001..99999 .$1 - $99999
SMX         1
     Second mortgage or home equity loan status
            b .N/A (GQ/vacant/not owned or being bought/
              .not a one family house, mobile home, trailer or
              .condo/not mortgaged/no second mortgage)
            1 .Yes, a second mortgage
            2 .Yes, a home equity loan
            3 .No
            4 .Both a second mortgage and a home equity loan
SRNT        1
     Specified rent unit
            0 .Not specified rent unit
            1 .Specified rent unit
SVAL        1
     Specified value unit
            0 .Not specified owner unit
 
             1 .Specified value unit
TAXP        2
     Property taxes (yearly amount)
            bb .N/A (GQ/vacant/not owned or being bought/not a
               .one-family house, mobile home or trailer or
               .condo)
            01 .None
            02 .$    1 - $ 49
            03 .$ 50 - $ 99
            04 .$ 100 - $ 149
            05 .$ 150 - $ 199
            06 .$ 200 - $ 249
            07 .$ 250 - $ 299
            08 .$ 300 - $ 349
            09 .$ 350 - $ 399
            10 .$ 400 - $ 449
            11 .$ 450 - $ 499
            12 .$ 500 - $ 549
            13 .$ 550 - $ 599
            14 .$ 600 - $ 649
            15 .$ 650 - $ 699
            16 .$ 700 - $ 749
            17 .$ 750 - $ 799
            18 .$ 800 - $ 849
            19 .$ 850 - $ 899
            20 .$ 900 - $ 949
            21 .$ 950 - $ 999
            22 .$1000 - $1099
            23 .$1100 - $1199
            24 .$1200 - $1299
            25 .$1300 - $1399
            26 .$1400 - $1499
            27 .$1500 - $1599
            28 .$1600 - $1699
            29 .$1700 - $1799
            30 .$1800 - $1899
            31 .$1900 - $1999
            32 .$2000 - $2099
            33 .$2100 - $2199
            34 .$2200 - $2299
            35 .$2300 - $2399
            36 .$2400 - $2499
            37 .$2500 - $2599
            38 .$2600 - $2699
            39 .$2700 - $2799
            40 .$2800 - $2899
            41 .$2900 - $2999
            42 .$3000 - $3099
            43 .$3100 - $3199
            44 .$3200 - $3299
            45 .$3300 - $3399
            46 .$3400 - $3499
            47 .$3500 - $3599
            48 .$3600 - $3699
            49 .$3700 - $3799
            50 .$3800 - $3899
            51 .$3900 - $3999
            52 .$4000 - $4099
            53 .$4100 - $4199
            54 .$4200 - $4299
 
            55  .$4300 - $4399
           56  .$4400 - $4499
           57  .$4500 - $4599
           58  .$4600 - $4699
           59  .$4700 - $4799
           60  .$4800 - $4899
           61  .$4900 - $4999
           62  .$5000 - $5499
           63  .$5500 - $5999
           64  .$6000 - $6999
           65  .$7000 - $7999
           66  .$8000 - $8999
           67  .$9000 - $9999
           68  .$10000+
WIF        1
    Workers in family during the past 12 months
           b .N/A (GQ/vacant/non-family household)
           0 .No workers
           1 .1 worker
           2 .2 workers
           3 .3 or more workers in family
WKEXREL    2
    Work experience of householder and spouse
             b .N/A (GQ/vacant/not a family)
             1 .Householder and spouse worked FT
             2 .Householder worked FT; spouse worked < FT
             3 .Householder worked FT; spouse did not work
             4 .Householder worked < FT; spouse worked FT
             5 .Householder worked < FT; spouse worked < FT
             6 .Householder worked < FT; spouse did not work
             7 .Householder did not work; spouse worked FT
             8 .Householder did not work; spouse worked < FT
             9 .Householder did not work; spouse did not work
           10 .Male householder worked FT; no spouse present
           11 .Male householder worked < FT; no spouse present
           12 .Male householder did not work; no spouse present
           13 .Female householder worked FT; no spouse present
           14 .Female householder worked < FT; no spouse present
           15 .Female householder did not work; no spouse present
WORKSTAT   2
     Work status of householder or spouse in family households
           bb .N/A (GQ/not a family household)
             1 .Husband and wife both in labor force, both employed or in
               .Armed Forces
             2 .Husband and wife both in labor force, husband employed or in
               .Armed Forces, wife unemployed
             3 .Husband in labor force and wife not in labor force, husband
               .employed or in Armed Forces
             4 .Husband and wife both in labor force, husband unemployed, wife
               .employed or in Armed Forces
             5 .Husband and wife both in labor force, husband unemployed, wife
               .unemployed
             6 .Husband in labor force, husband unemployed, wife not in labor
               .force
             7 .Husband not in labor force, wife in labor force, wife
               .employed or in Armed Forces
             8 .Husband not in labor force, wife in labor force, wife
               .unemployed
 
               9 .Neither husband nor wife in labor force
            10 .Male householder with no wife present, householder in
                .labor force, employed or in Armed Forces
            11 .Male householder with no wife present, householder in
                .labor force and unemployed
            12 .Male householder with no wife present, householder not in
                .labor force
            13 .Female householder with no husband present, householder in
                .labor force, employed or in Armed Forces
            14 .Female householder with no husband present, householder in
                .labor force and unemployed
            15 .Female householder with no husband present, householder not in
                .labor force
FACRP       1
     Lot size allocation
            0 .No
            1 .Yes
FAGSP       1
     Sales of Agricultural Products allocation
            0 .No
            1 .Yes
FBDSP       1
     Number of bedrooms allocation
            0 .No
            1 .Yes
FBLDP       1
     Units in structure allocation
            0 .No
            1 .Yes
FBUSP       1
     Business or medical office on property allocation
            0 .No
            1 .Yes
FCONP       1
     Condominium fee allocation
            0 .No
            1 .Yes
FELEP       1
     Electricity (monthly cost) allocation
            0 .No
            1 .Yes
FFSP        1
     Food stamp amount (yearly amount) allocation
            0 .No
            1 .Yes
FFULP       1
     House heating fuel (yearly cost) allocation
            0 .No
            1 .Yes
FGASP       1
     Gas (monthly cost) allocation
 
             0 .No
            1 .Yes
FHFLP       1
     House heating fuel allocation
            0 .No
            1 .Yes
FINSP       1
     Fire, hazard, flood insurance allocation
            0 .No
            1 .Yes
FKITP       1
     Complete kitchen facilities allocation
            0 .No
            1 .Yes
FMHP        1
     Mobile home costs allocation
            0 .No
            1 .Yes
FMRGIP      1
     Payment include fire, hazard, flood insurance
     allocation
            0 .No
            1 .Yes
FMRGP       1
     Regular mortgage payment allocation
            0 .No
            1 .Yes
FMRGTP      1
     Payment include real estate taxes allocation
            0 .No
            1 .Yes
FMRGXP      1
     Mortgage status allocation
            0 .No
            1 .Yes
FMVYP       1
     When moved into this house or apartment allocation
            0 .No
            1 .Yes
FPLMP       1
     Complete plumbing facilities allocation
            0 .No
            1 .Yes
FRMSP       1
     Rooms allocation
            0 .No
            1 .Yes
FRNTMP      1
     Meals included in rent allocation
 
             0 .No
            1 .Yes
FRNTP       1
     Monthly rent allocation
            0 .No
            1 .Yes
FSMP        1
     Second mortgage payment allocation
            0 .No
            1 .Yes
FSMXHP      1
     Home equity loan status allocation
            0 .No
            1 .Yes
FSMXSP      1
     Second mortgage status allocation
            0 .No
            1 .Yes
FTAXP       1
     Taxes on property allocation
            0 .No
            1 .Yes
FTELP       1
     Telephones in house allocation
            0 .No
            1 .Yes
FTENP       1
     Tenure allocation
            0 .No
            1 .Yes
FVACSP      1
     Vacancy status allocation
            0 .No
            1 .Yes
FVALP       1
     Value allocation
            0 .No
            1 .Yes
FVEHP       1
     Vehicles available by household allocation
            0 .No
            1 .Yes
FWATP       1
     Water (yearly cost) allocation
            0 .No
            1 .Yes
FYBLP       1
     When structure first built allocation
            0 .No
 
            1 .Yes
WGTP1      4
    Housing Weight replicate 1
           0001..9999 .Integer weight of housing unit
WGTP2      4
    Housing Weight replicate 2
           0001..9999 .Integer weight of housing unit
WGTP3      4
    Housing Weight replicate 3
           0001..9999 .Integer weight of housing unit
WGTP4      4
    Housing Weight replicate 4
           0001..9999 .Integer weight of housing unit
WGTP5      4
    Housing Weight replicate 5
           0001..9999 .Integer weight of housing unit
WGTP6      4
    Housing Weight replicate 6
           0001..9999 .Integer weight of housing unit
WGTP7      4
    Housing Weight replicate 7
           0001..9999 .Integer weight of housing unit
WGTP8      4
    Housing Weight replicate 8
           0001..9999 .Integer weight of housing unit
WGTP9      4
    Housing Weight replicate 9
           0001..9999 .Integer weight of housing unit
WGTP10     4
    Housing Weight replicate 10
           0001..9999 .Integer weight of housing unit
WGTP11     4
    Housing Weight replicate 11
           0001..9999 .Integer weight of housing unit
WGTP12     4
    Housing Weight replicate 12
           0001..9999 .Integer weight of housing unit
WGTP13     4
    Housing Weight replicate 13
           0001..9999 .Integer weight of housing unit
WGTP14     4
    Housing Weight replicate 14
           0001..9999 .Integer weight of housing unit
WGTP15     4
    Housing Weight replicate 15
           0001..9999 .Integer weight of housing unit
 
 WGTP16     4
    Housing Weight replicate 16
           0001..9999 .Integer weight of housing unit
WGTP17     4
    Housing Weight replicate 17
           0001..9999 .Integer weight of housing unit
WGTP18     4
    Housing Weight replicate 18
           0001..9999 .Integer weight of housing unit
WGTP19     4
    Housing Weight replicate 19
           0001..9999 .Integer weight of housing unit
WGTP20     4
    Housing Weight replicate 20
           0001..9999 .Integer weight of housing unit
WGTP21     4
    Housing Weight replicate 21
           0001..9999 .Integer weight of housing unit
WGTP22     4
    Housing Weight replicate 22
           0001..9999 .Integer weight of housing unit
WGTP23     4
    Housing Weight replicate 23
           0001..9999 .Integer weight of housing unit
WGTP24     4
    Housing Weight replicate 24
           0001..9999 .Integer weight of housing unit
WGTP25     4
    Housing Weight replicate 25
           0001..9999 .Integer weight of housing unit
WGTP26     4
    Housing Weight replicate 26
           0001..9999 .Integer weight of housing unit
WGTP27     4
    Housing Weight replicate 27
           0001..9999 .Integer weight of housing unit
WGTP28     4
    Housing Weight replicate 28
           0001..9999 .Integer weight of housing unit
WGTP29     4
    Housing Weight replicate 29
           0001..9999 .Integer weight of housing unit
WGTP30     4
    Housing Weight replicate 30
           0001..9999 .Integer weight of housing unit
WGTP31     4
 
     Housing Weight replicate 31
           0001..9999 .Integer weight of housing unit
WGTP32     4
    Housing Weight replicate 32
           0001..9999 .Integer weight of housing unit
WGTP33     4
    Housing Weight replicate 33
           0001..9999 .Integer weight of housing unit
WGTP34     4
    Housing Weight replicate 34
           0001..9999 .Integer weight of housing unit
WGTP35     4
    Housing Weight replicate 35
           0001..9999 .Integer weight of housing unit
WGTP36     4
    Housing Weight replicate 36
           0001..9999 .Integer weight of housing unit
WGTP37     4
    Housing Weight replicate 37
           0001..9999 .Integer weight of housing unit
WGTP38     4
    Housing Weight replicate 38
           0001..9999 .Integer weight of housing unit
WGTP39     4
    Housing Weight replicate 39
           0001..9999 .Integer weight of housing unit
WGTP40     4
    Housing Weight replicate 40
           0001..9999 .Integer weight of housing unit
WGTP41     4
    Housing Weight replicate 41
           0001..9999 .Integer weight of housing unit
WGTP42     4
    Housing Weight replicate 42
           0001..9999 .Integer weight of housing unit
WGTP43     4
    Housing Weight replicate 43
           0001..9999 .Integer weight of housing unit
WGTP44     4
    Housing Weight replicate 44
           0001..9999 .Integer weight of housing unit
WGTP45     4
    Housing Weight replicate 45
           0001..9999 .Integer weight of housing unit
WGTP46     4
    Housing Weight replicate 46
 
            0001..9999 .Integer weight of housing unit
WGTP47     4
    Housing Weight replicate 47
           0001..9999 .Integer weight of housing unit
WGTP48     4
    Housing Weight replicate 48
           0001..9999 .Integer weight of housing unit
WGTP49     4
    Housing Weight replicate 49
           0001..9999 .Integer weight of housing unit
WGTP50     4
    Housing Weight replicate 50
           0001..9999 .Integer weight of housing unit
WGTP51     4
    Housing Weight replicate 51
           0001..9999 .Integer weight of housing unit
WGTP52     4
    Housing Weight replicate 52
           0001..9999 .Integer weight of housing unit
WGTP53     4
    Housing Weight replicate 53
           0001..9999 .Integer weight of housing unit
WGTP54     4
    Housing Weight replicate 54
           0001..9999 .Integer weight of housing unit
WGTP55     4
    Housing Weight replicate 55
           0001..9999 .Integer weight of housing unit
WGTP56     4
    Housing Weight replicate 56
           0001..9999 .Integer weight of housing unit
WGTP57     4
    Housing Weight replicate 57
           0001..9999 .Integer weight of housing unit
WGTP58     4
    Housing Weight replicate 58
           0001..9999 .Integer weight of housing unit
WGTP59     4
    Housing Weight replicate 59
           0001..9999 .Integer weight of housing unit
WGTP60     4
    Housing Weight replicate 60
           0001..9999 .Integer weight of housing unit
WGTP61     4
    Housing Weight replicate 61
           0001..9999 .Integer weight of housing unit
 
 WGTP62     4
    Housing Weight replicate 62
           0001..9999 .Integer weight of housing unit
WGTP63     4
    Housing Weight replicate 63
           0001..9999 .Integer weight of housing unit
WGTP64     4
    Housing Weight replicate 64
           0001..9999 .Integer weight of housing unit
WGTP65     4
    Housing Weight replicate 65
           0001..9999 .Integer weight of housing unit
WGTP66     4
    Housing Weight replicate 66
           0001..9999 .Integer weight of housing unit
WGTP67     4
    Housing Weight replicate 67
           0001..9999 .Integer weight of housing unit
WGTP68     4
    Housing Weight replicate 68
           0001..9999 .Integer weight of housing unit
WGTP69     4
    Housing Weight replicate 69
           0001..9999 .Integer weight of housing unit
WGTP70     4
    Housing Weight replicate 70
           0001..9999 .Integer weight of housing unit
WGTP71     4
    Housing Weight replicate 71
           0001..9999 .Integer weight of housing unit
WGTP72     4
    Housing Weight replicate 72
           0001..9999 .Integer weight of housing unit
WGTP73     4
    Housing Weight replicate 73
           0001..9999 .Integer weight of housing unit
WGTP74     4
    Housing Weight replicate 74
           0001..9999 .Integer weight of housing unit
WGTP75     4
    Housing Weight replicate 75
           0001..9999 .Integer weight of housing unit
WGTP76     4
    Housing Weight replicate 76
           0001..9999 .Integer weight of housing unit
WGTP77     4
 
      Housing Weight replicate 77
            0001..9999 .Integer weight of housing unit
WGTP78      4
     Housing Weight replicate 78
            0001..9999 .Integer weight of housing unit
WGTP79      4
     Housing Weight replicate 79
            0001..9999 .Integer weight of housing unit
WGTP80      4
     Housing Weight replicate 80
            0001..9999 .Integer weight of housing unit
                        DATA DICTIONARY - 2006 POPULATION
RT          1
     Record Type
            P .Person Record
SERIALNO    7
     Housing unit/GQ person serial number
            0000001..9999999 .Unique identifier assigned within
                              .state
SPORDER     2
     Person number
            01..20 .Person number
PUMA        5
     Public use microdata area code (PUMA)
     Designates area of 100,000 or more population. Use with ST for unique
     code.
            00100..08200
            77777 .combination of 01801, 01802, and 01905 in Louisiana
ST          2
     State Code
            01 .Alabama/AL
            02 .Alaska/AK
            04 .Arizona/AZ
            05 .Arkansas/AR
            06 .California/CA
            08 .Colorado/CO
            09 .Connecticut/CT
            10 .Delaware/DE
            11 .District of Columbia/DC
            12 .Florida/FL
            13 .Georgia/GA
            15 .Hawaii/HI
            16 .Idaho/ID
            17 .Illinois/IL
            18 .Indiana/IN
            19 .Iowa/IA
            20 .Kansas/KS
            21 .Kentucky/KY
            22 .Louisiana/LA
            23 .Maine/ME
            24 .Maryland/MD
            25 .Massachusetts/MA
 
             26  .Michigan/MI
            27  .Minnesota/MN
            28  .Mississippi/MS
            29  .Missouri/MO
            30  .Montana/MT
            31  .Nebraska/NE
            32  .Nevada/NV
            33  .New Hampshire/NH
            34  .New Jersey/NJ
            35  .New Mexico/NM
            36  .New York/NY
            37  .North Carolina/NC
            38  .North Dakota/ND
            39  .Ohio/OH
            40  .Oklahoma/OK
            41  .Oregon/OR
            42  .Pennsylvania/PA
            44  .Rhode Island/RI
            45  .South Carolina/SC
            46  .South Dakota/SD
            47  .Tennessee/TN
            48  .Texas/TX
            49  .Utah/UT
            50  .Vermont/VT
            51  .Virginia/VA
            53  .Washington/WA
            54  .West Virginia/WV
            55  .Wisconsin/WI
            56  .Wyoming/WY
            72  .Puerto Rico/PR
ADJUST      7
     Adjustment factor for dollar amounts (6 implied decimal places)
            1015675 .2006 factor (1.015675)
PWGTP       4
     Person's weight
            0001..9999 .Integer weight of person
AGEP        2
     Age
                 00 .Under 1 year
            01..99 .1 to 99 years (Top-coded***)
CIT         1
     Citizenship status
            1 .Born in the U.S.
               .Born in the U.S., Guam, the U.S. Virgin Islands, or the Northern
               .Marianas if current residence is Puerto Rico
            2 .Born in Puerto Rico, Guam, the U.S. Virgin Islands,
               .or the Northern Marianas
               .Born in Puerto Rico if current residence is Puerto Rico
            3 .Born abroad of American parents
            4 .U.S. citizen by naturalization
            5 .Not a citizen of the U.S.
COW         1
     Class of worker
            b .N/A (less than 16 years old/unemployed who
               .never worked/NILF who last worked more than 5 years
               .ago)
 
             1 .Employee of a private for profit company or
              .business or of an individual, for wages,
              .salary, or commissions
            2 .Employee of a private not-for-profit,
              .tax-exempt, or charitable organization
            3 .Local government employee (city, county, etc.)
            4 .State government employee
            5 .Federal government employee
            6 .Self-employed in own not incorporated
              .business, professional practice, or farm
            7 .Self-employed in own incorporated
              .business, professional practice or farm
            8 .Working without pay in family business or farm
            9 .Unemployed
DDRS        1
     Difficulty dressing
            b .N/A (Less than 5 years old)
            1 .Yes
            2 .No
DEYE        1
     Vision or hearing difficulty
            b .N/A (Less than 5 years old)
            1 .Yes
            2 .No
DOUT        1
     Difficulty going out
            b .N/A (Less than 16 years old)
            1 .Yes
            2 .No
DPHY        1
     Physical difficulty
            b .N/A (Less than 5 years old)
            1 .Yes
            2 .No
DREM        1
     Difficulty remembering
            b .N/A (Less than 5 years old)
            1 .Yes
            2 .No
DWRK        1
     Difficulty working
            b .N/A (Less than 16 years old)
            1 .Yes
            2 .No
ENG         1
     Ability to speak English
            b .N/A (less than 5 years old/speaks only English)
            1 .Very well
            2 .Well
            3 .Not well
            4 .Not at all
FER         1
     Child born within the past 12 months
 
             b .N/A (less than 15 years/greater than 50 years/
              .male)
            1 .Yes
            2 .No
GCL         1
     Grandchildren living in this house
            b .N/A (less than 30 years/institutional GQ)
            1 .Yes
            2 .No
GCM         1
     Months responsible for grandchildren
            b .N/A (less than 30 years/grandparent not responsible for
              .grandchild/institutional GQ)
            1 .Less than 6 months
            2 .6 to 11 months
            3 .1 to 2 years
            4 .3 to 4 years
            5 .5 or more years
GCR         1
     Responsible for grandchildren
            b .N/A (less than 30 years/grandchild not living in
house/institutional GQ)
            1 .Yes
            2 .No
INTP        6
     Interest, dividends, and net rental income past 12 months (signed)
                     bbbbbb .N/A (less than 15 years old)
                     000000 .None
                     -09999 .Loss of $9999 or more
            -00001..-09998 .Loss $1 to $9998
                     000001 .$1 or breakeven
            000002..999999 .$2 to $999999 (Rounded and top-coded)
JWMNP       3
     Travel time to work
                  bbb .N/A (not a worker or worker who worked at
                      .home)
            001..200 .1 to 200 minutes to get to work (Top-coded)
JWRIP       2
     Vehicle occupancy
            bb .N/A (not a worker or worker whose means of
               .transportation to work was not car, truck,
               .or van)
            01 .Drove alone
            02 .In 2-person carpool
            03 .In 3-person carpool
            04 .In 4-person carpool
            05 .In 5-person carpool
            06 .In 6-person carpool
            07 .In 7-person carpool
            08 .In 8-person carpool
            09 .In 9-person carpool
            10 .In 10-person or more carpool
JWTR        2
 
      Means of transportation to work
             bb .N/A (not a worker--not in the labor force,
                .including persons under 16 years; unemployed;
                .employed, with a job but not at work; Armed
                .Forces, with a job but not at work)
             01 .Car, truck, or van
             02 .Bus or trolley bus
             03 .Streetcar or trolley car (carro publico in Puerto Rico)
             04 .Subway or elevated
             05 .Railroad
             06 .Ferryboat
             07 .Taxicab
             08 .Motorcycle
             09 .Bicycle
             10 .Walked
             11 .Worked at home
             12 .Other method
LANX         1
     Language  other than English at home
             b .N/A (less than 5 years old)
             1 .Yes, speaks another language
             2 .No, speaks only English
MAR          1
     Marital status
             1 .Married
             2 .Widowed
             3 .Divorced
             4 .Separated
             5 .Never married or under 15 years old
MIG          1
     Mobility  status (lived here 1 year ago)
             b .N/A(less than 1 year old)
             1 .Yes, same house (nonmovers)
             2 .No, outside US if current residence is US;
               .No, outside Puerto Rico and US if current residence is
               .Puerto Rico
             3 .No, different house in US if current residence is US;
               .No, different house in Puerto Rico is current residence is
               .Puerto Rico
MIL          1
     Military  service
             b .N/A (less than 17 years old)
             1 .Yes, now on active duty
             2 .Yes, on active duty during the last 12 months, but not now
             3 .Yes, on active duty in the past, but not during the last 12
               .months
             4 .No, training for Reserves/National Guard only
             5 .No, never served in the military
MILY       1
     Years of active duty military service
             b .N/A (less than 17 years/no active duty
               .military service)
             1 .Less than 2 years of service
             2 .2 years or more of service
MLPA         1
 
      Served September 2001 or later
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPB        1
     Served August 1990 - August 2001 (including Persian Gulf War)
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPC        1
     Served September 1980 - July 1990
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPD        1
     Served May 1975 - August 1980
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPE        1
     Served Vietnam era (August 1964 - April 1975)
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPF        1
     Served March 1961 - July 1964
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPG        1
     Served February 1955 - February 1961
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPH        1
     Served Korean War (July 1950 - January 1955)
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPI        1
     Served January 1947 - June 1950
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPJ        1
     Served World War II (December 1941 - December 1946)
            b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
MLPK        1
     Served November 1941 or earlier
 
             b .N/A (Less than 17 years old/no active duty)
            0 .Did not serve this period
            1 .Served this period
NWAB        1
     Temporary absence from work
            b .N/A (less than 16 years old/at work/on layoff)
            1 .Yes
            2 .No
            3 .Did not report
NWAV        1
     Available for work
            b .N/A (less than 16 years/at work/not looking)
            1 .Yes
            2 .No, temporarily ill
            3 .No, other reasons
            4 .No, unspecified
            5 .Did not report
NWLA        1
     On layoff from work
            b .N/A (less than 16 years old/at work)
            1 .Yes
            2 .No
            3 .Did not report
NWLK        1
     Looking for work
            b .N/A (less than 16 years old/at work/temporarily
              .absent/informed of recall)
            1 .Yes
            2 .No
            3 .Did not report
NWRE        1
     Informed of recall
            b .N/A (less than 16 years old/at work/not on layoff)
            1 .Yes
            2 .No
            3 .Did not report
OIP         6
     All other income past 12 months
                    bbbbbb .N/A (less than 15 years old)
                    000000 .None
            000001..999999 .$1 to $999999 (Rounded and top-coded)
PAP         5
     Public assistance income past 12 months
                   bbbbb .N/A (less than 15 years old)
                   00000 .None
            00001..99999 .$1 to $99999 (Rounded)
REL         2
     Relationship
            00 .Reference person
            01 .Husband/wife
            02 .Son/daughter
            03 .Brother/sister
            04 .Father/mother
 
             05 .Grandchild
            06 .Inlaw
            07 .Other relative
            08 .Roomer/boarder
            09 .Housemate/roommate
            10 .Unmarried partner
            11 .Foster child
            12 .Other nonrelative
            13 .Institutionalized group quarters population
            14 .Noninstitutionalized group quarters population
RETP        6
     Retirement income past 12 months
                    bbbbbb .N/A (less than 15 years old)
                    000000 .None
            000001..999999 .$1 to $999999 (Rounded and top-coded)
SCH         1
     School enrollment
            b .N/A (less than 3 years old)
            1 .No, has not attended in the last 3 months
            2 .Yes, public school or public college
            3 .Yes, private school or private college
SCHG        1
     Grade level attending
            b .N/A (not attending school)
            1 .Nursery school/preschool
            2 .Kindergarten
            3 .Grade 1 to grade 4
            4 .Grade 5 to grade 8
            5 .Grade 9 to grade 12
            6 .College undergraduate
            7 .Graduate or professional school
SCHL        2
     Educational attainment
            bb .N/A (less than 3 years old)
            01 .No school completed
            02 .Nursery school to grade 4
            03 .Grade 5 or grade 6
            04 .Grade 7 or grade 8
            05 .Grade 9
            06 .Grade 10
            07 .Grade 11
            08 .Grade 12 no diploma
            09 .High school graduate
            10 .Some college, but less than 1 year
            11 .One or more years of college, no degree
            12 .Associate's degree
            13 .Bachelor's degree
            14 .Master's degree
            15 .Professional school degree
            16 .Doctorate degree
SEMP        6
     Self-employment income past 12 months (signed)
                    bbbbbb .N/A (less than 15 years old)
                    000000 .None
                    -09999 .Loss of $9999 or more
            -00001..-09998 .Loss $1 to $9998
 
                      000001 .$1 or breakeven
            000002..999999 .$2 to $999999 (Rounded and top-coded)
SEX         1
     Sex
            1 .Male
            2 .Female
SSIP        5
     Supplementary Security Income past 12 months
                    bbbbb .N/A (less than 15 years old)
                    00000 .None
            00001..99999 .$1 to $99999 (Rounded)
SSP         5
     Social security income past 12 months
                    bbbbb .N/A (less than 15 years old)
                    00000 .None
            00001..99999 .$1 to $99999 (Rounded)
WAGP        6
     Wages or salary income past 12 months
                     bbbbbb .N/A (less than 15 years old)
                     000000 .None
            000001..999999 .$1 to 999999 (Rounded and top-coded)
WKHP        2
     Usual hours worked per week past 12 months
                bb .N/A (less than 16 years old/did not work
                    .during the past 12 months)
            01..98 .1 to 98 usual hours
                99 .99 or more usual hours
WKL         1
     When last worked
            b .N/A (less than 16 years old)
            1 .Within the past 12 months
            2 .1-5 years ago
            3 .Over 5 years ago or never worked
WKW         2
     Weeks worked past 12 months
                bb .N/A (less than 16 years old/did not work
                    .during the past 12 months)
            01..52 .1 to 52 weeks worked past 12 months
YOEP        4
     Year of entry
            bbbb .Born in the US
            1919 .1919 or earlier
            1920 .1920
            1921 .1921
            1922 .1922
            1923 .1923
            1924 .1924
            1925 .1925
            1926 .1926
            1927 .1927
            1928 .1928
            1929 .1929
            1930 .1930
 
 1931 .1931 or 1932
1933 .1933 or 1934
1935 .1935
1936 .1936
1937 .1937
1938 .1938
1939 .1939
1940 .1940
1941 .1941
1942 .1942
1943 .1943
1944 .1944
1945 .1945
1946 .1946
1947 .1947
1948 .1948
1949 .1949
1950 .1950
1951 .1951
1952 .1952
1953 .1953
1954 .1954
1955 .1955
1956 .1956
1957 .1957
1958 .1958
1959 .1959
1960 .1960
1961 .1961
1962 .1962
1963 .1963
1964 .1964
1965 .1965
1966 .1966
1967 .1967
1968 .1968
1969 .1969
1970 .1970
1971 .1971
1972 .1972
1973 .1973
1974 .1974
1975 .1975
1976 .1976
1977 .1977
1978 .1978
1979 .1979
1980 .1980
1981 .1981
1982 .1982
1983 .1983
1984 .1984
1985 .1985
1986 .1986
1987 .1987
1988 .1988
1989 .1989
1990 .1990
1991 .1991
1992 .1992
1993 .1993
 
             1994 .1994
            1995 .1995
            1996 .1996
            1997 .1997
            1998 .1998
            1999 .1999
            2000 .2000
            2001 .2001
            2002 .2002
            2003 .2003
            2004 .2004
            2005 .2005
            2006 .2006
UWRK        1
     Worked last week
            b .N/A (less than 16 years/not reported)
            1 .Worked
            2 .Did not work
ANC         1
     Ancestry recode
            1 .Single
            2 .Multiple
            3 .Unclassified
            4 .Not reported
ANC1P       3
     Recoded Detailed Ancestry - first entry
            001 .Alsatian
            003 .Austrian
            005 .Basque
            008 .Belgian
            009 .Flemish
            011 .British
            012 .British Isles
            020 .Danish
            021 .Dutch
            022 .English
            024 .Finnish
            026 .French
            032 .German
            040 .Prussian
            046 .Greek
            049 .Icelander
            050 .Irish
            051 .Italian
            068 .Sicilian
            077 .Luxemburger
            078 .Maltese
            082 .Norwegian
            084 .Portuguese
            087 .Scotch Irish
            088 .Scottish
            089 .Swedish
            091 .Swiss
            097 .Welsh
            098 .Scandinavian
            099 .Celtic
            100 .Albanian
            102 .Belorussian
 
 103 .Bulgarian
109 .Croatian
111 .Czech
112 .Bohemian
114 .Czechoslovakian
115 .Estonian
122 .German Russian
124 .Rom
125 .Hungarian
128 .Latvian
129 .Lithuanian
130 .Macedonian
142 .Polish
144 .Romanian
148 .Russian
152 .Serbian
153 .Slovak
154 .Slovene
170 .Georgia CIS
171 .Ukrainian
176 .Yugoslavian
177 .Herzegovinian
178 .Slavic
179 .Slavonian
183 .Northern European
187 .Western European
190 .Eastern European
195 .European
200 .Spaniard
210 .Mexican
211 .Mexican American
212 .Mexicano
213 .Chicano
215 .Mexican American Indian
218 .Mexican State
221 .Costa Rican
222 .Guatemalan
223 .Honduran
224 .Nicaraguan
225 .Panamanian
226 .Salvadoran
227 .Central American
231 .Argentinean
232 .Bolivian
233 .Chilean
234 .Colombian
235 .Ecuadorian
236 .Paraguayan
237 .Peruvian
238 .Uruguayan
239 .Venezuelan
249 .South American
250 .Latin American
251 .Latin
252 .Latino
261 .Puerto Rican
271 .Cuban
275 .Dominican
290 .Hispanic
291 .Spanish
295 .Spanish American
 
 300 .Bahamian
301 .Barbadian
302 .Belizean
308 .Jamaican
310 .Dutch West Indian
314 .Trinidadian Tobagonian
322 .British West Indian
325 .Antigua and Barbuda
329 .Grenadian
330 .Vincent-Grenadine Islander
331 .St Lucia Islander
335 .West Indian
336 .Haitian
359 .Other West Indian
360 .Brazilian
370 .Guyanese
400 .Algerian
402 .Egyptian
406 .Moroccan
416 .Iranian
417 .Iraqi
419 .Israeli
421 .Jordanian
425 .Lebanese
429 .Syrian
431 .Armenian
434 .Turkish
435 .Yemeni
442 .Kurdish
465 .Palestinian
483 .Assyrian
484 .Chaldean
490 .Mideast
495 .Arab
496 .Arabic
499 .Other Arab
508 .Cameroon
510 .Cape Verdean
522 .Ethiopian
523 .Eritrean
529 .Ghanian
534 .Kenyan
541 .Liberian
553 .Nigerian
564 .Senegalese
566 .Sierra Leonean
568 .Somalian
570 .South African
576 .Sudanese
587 .Other Subsaharan African
598 .Western African
599 .African
600 .Afghan
603 .Bangladeshi
609 .Nepali
615 .Asian Indian
618 .Bengali
620 .East Indian
650 .Punjab
680 .Pakistani
690 .Sri Lankan
 
 700 .Burmese
703 .Cambodian
706 .Chinese
707 .Cantonese
712 .Mongolian
720 .Filipino
730 .Indonesian
740 .Japanese
748 .Okinawan
750 .Korean
765 .Laotian
768 .Hmong
770 .Malaysian
776 .Thai
782 .Taiwanese
785 .Vietnamese
793 .Eurasian
794 .Amerasian
795 .Asian
799 .Other Asian
800 .Austrailian
803 .New Zealander
808 .Polynesian
811 .Hawaiian
814 .Samoan
815 .Tongan
820 .Micronesian
821 .Guamanian
822 .Chamorro Islander
841 .Fijian
850 .Pacific Islander
899 .Other Pacific
900 .Afro American
901 .Afro
902 .African American
903 .Black
904 .Negro
907 .Creole
913 .Central American Indian
914 .South American Indian
917 .Native American
918 .Indian
919 .Cherokee
920 .American Indian
921 .Aleut
922 .Eskimo
924 .White
925 .Anglo
927 .Appalachian
929 .Pennsylvania German
931 .Canadian
935 .French Canadian
936 .Acadian
937 .Cajun
939 .American or United States
983 .Texas
994 .North American
995 .Mixture
996 .Uncodable entries
997 .Other groups
998 .Other responses
 
            999 .Not reported
ANC2P      3
    Recoded Detailed Ancestry - second entry
           001 .Alsatian
           003 .Austrian
           005 .Basque
           008 .Belgian
           009 .Flemish
           011 .British
           012 .British Isles
           020 .Danish
           021 .Dutch
           022 .English
           024 .Finnish
           026 .French
           032 .German
           040 .Prussian
           046 .Greek
           049 .Icelander
           050 .Irish
           051 .Italian
           068 .Sicilian
           077 .Luxemburger
           078 .Maltese
           082 .Norwegian
           084 .Portuguese
           087 .Scotch Irish
           088 .Scottish
           089 .Swedish
           091 .Swiss
           097 .Welsh
           098 .Scandinavian
           099 .Celtic
           100 .Albanian
           102 .Belorussian
           103 .Bulgarian
           109 .Croatian
           111 .Czech
           112 .Bohemian
           114 .Czechoslovakian
           115 .Estonian
           122 .German Russian
           124 .Rom
           125 .Hungarian
           128 .Latvian
           129 .Lithuanian
           130 .Macedonian
           142 .Polish
           144 .Romanian
           148 .Russian
           152 .Serbian
           153 .Slovak
           154 .Slovene
           170 .Georgia CIS
           171 .Ukrainian
           176 .Yugoslavian
           177 .Herzegovinian
           178 .Slavic
           179 .Slavonian
           183 .Northern European
 
 187 .Western European
190 .Eastern European
195 .European
200 .Spaniard
210 .Mexican
211 .Mexican American
212 .Mexicano
213 .Chicano
215 .Mexican American Indian
218 .Mexican State
221 .Costa Rican
222 .Guatemalan
223 .Honduran
224 .Nicaraguan
225 .Panamanian
226 .Salvadoran
227 .Central American
231 .Argentinean
232 .Bolivian
233 .Chilean
234 .Colombian
235 .Ecuadorian
236 .Paraguayan
237 .Peruvian
238 .Uruguayan
239 .Venezuelan
249 .South American
250 .Latin American
251 .Latin
252 .Latino
261 .Puerto Rican
271 .Cuban
275 .Dominican
290 .Hispanic
291 .Spanish
295 .Spanish American
300 .Bahamian
301 .Barbadian
302 .Belizean
308 .Jamaican
310 .Dutch West Indian
314 .Trinidadian Tobagonian
322 .British West Indian
325 .Antigua and Barbuda
329 .Grenadian
330 .Vincent-Grenadine Islander
331 .St Lucia Islander
335 .West Indian
336 .Haitian
359 .Other West Indian
360 .Brazilian
370 .Guyanese
400 .Algerian
402 .Egyptian
406 .Moroccan
416 .Iranian
417 .Iraqi
419 .Israeli
421 .Jordanian
425 .Lebanese
429 .Syrian
 
 431 .Armenian
434 .Turkish
435 .Yemeni
442 .Kurdish
465 .Palestinian
483 .Assyrian
484 .Chaldean
490 .Mideast
495 .Arab
496 .Arabic
499 .Other Arab
508 .Cameroon
510 .Cape Verdean
522 .Ethiopian
523 .Eritrean
529 .Ghanian
534 .Kenyan
541 .Liberian
553 .Nigerian
564 .Senegalese
566 .Sierra Leonean
568 .Somalian
570 .South African
576 .Sudanese
587 .Other Subsaharan African
598 .Western African
599 .African
600 .Afghan
603 .Bangladeshi
609 .Nepali
615 .Asian Indian
618 .Bengali
620 .East Indian
650 .Punjab
680 .Pakistani
690 .Sri Lankan
700 .Burmese
703 .Cambodian
706 .Chinese
707 .Cantonese
712 .Mongolian
720 .Filipino
730 .Indonesian
740 .Japanese
748 .Okinawan
750 .Korean
765 .Laotian
768 .Hmong
770 .Malaysian
776 .Thai
782 .Taiwanese
785 .Vietnamese
793 .Eurasian
794 .Amerasian
795 .Asian
799 .Other Asian
800 .Austrailian
803 .New Zealander
808 .Polynesian
811 .Hawaiian
814 .Samoan
 
            815 .Tongan
           820 .Micronesian
           821 .Guamanian
           822 .Chamorro Islander
           841 .Fijian
           850 .Pacific Islander
           899 .Other Pacific
           900 .Afro American
           901 .Afro
           902 .African American
           903 .Black
           904 .Negro
           907 .Creole
           913 .Central American Indian
           914 .South American Indian
           917 .Native American
           918 .Indian
           919 .Cherokee
           920 .American Indian
           921 .Aleut
           922 .Eskimo
           924 .White
           925 .Anglo
           927 .Appalachian
           929 .Pennsylvania German
           931 .Canadian
           935 .French Canadian
           936 .Acadian
           937 .Cajun
           939 .American or United States
           983 .Texas
           994 .North American
           995 .Mixture
           996 .Uncodable entries
           997 .Other groups
           998 .Other responses
           999 .Not reported
DECADE     1
    Decade of entry
           b .N/A (Born in the US)
           1 .Before 1950
           2 .1950 - 1959
           3 .1960 - 1969
           4 .1970 - 1979
           5 .1980 - 1989
           6 .1990 - 1999
           7 .2000 - 2009
DRIVESP    1
    Number of vehicles calculated from JWRI
           b .N/A (Nonworker or worker who does not drive to work)
           1 .1.000 vehicles (Drove alone)
           2 .0.500 vehicles (In a 2-person carpool)
           3 .0.333 vehicles (In a 3-person carpool)
           4 .0.250 vehicles (In a 4-person carpool)
           5 .0.200 vehicles (In a 5- or 6-person carpool)
           6 .0.143 vehicles (In a 7-or-more person carpool)
DS         1
    Disability recode
 
             b .N/A (Less than 5 years old)
            1 .With a disability
            2 .Without a disability
ESP         1
     Employment status of parents
            b .N/A (not own child of householder, and not
              .child in subfamily)
              .Living with two parents:
            1 .Both parents in labor force
            2 .Father only in labor force
            3 .Mother only in labor force
            4 .Neither parent in labor force
              .Living with one parent:
              .Living with father:
            5 .Father in the labor force
            6 .Father not in labor force
              .Living with mother:
            7 .Mother in the labor force
            8 .Mother not in labor force
ESR         1
     Employment status recode
            b .N/A (less than 16 years old)
            1 .Civilian employed, at work
            2 .Civilian employed, with a job but not at work
            3 .Unemployed
            4 .Armed forces, at work
            5 .Armed forces, with a job but not at work
            6 .Not in labor force
HISP        2
     Recoded detailed Hispanic origin
            01 .Not Spanish/Hispanic/Latino
            02 .Mexican
            03 .Puerto Rican
            04 .Cuban
            05 .Dominican
            06 .Costa Rican
            07 .Guatemalan
            08 .Honduran
            09 .Nicaraguan
            10 .Panamanian
            11 .Salvadoran
            12 .Other Central American
            13 .Argentinean
            14 .Bolivian
            15 .Chilean
            16 .Colombian
            17 .Ecuadorian
            18 .Paraguayan
            19 .Peruvian
            20 .Uruguayan
            21 .Venezuelan
            22 .Other South American
            23 .Spaniard
            24 .All Other Spanish/Hispanic/Latino
INDP        4
     Industry recode
            bbbb .N/A (less than 16 years old/unemployed who
 
      .never worked/NILF who last worked more than
     .5 years ago)
0170 .AGR-CROP PRODUCTION
0180 .AGR-ANIMAL PRODUCTION
0190 .AGR-FORESTRY EXCEPT LOGGING
0270 .AGR-LOGGING
0280 .AGR-FISHING, HUNTING, AND TRAPPING
0290 .AGR-SUPPORT ACTIVITIES FOR AGRICULTURE AND FORESTRY
0370 .EXT-OIL AND GAS EXTRACTION
0380 .EXT-COAL MINING
0390 .EXT-METAL ORE MINING
0470 .EXT-NONMETALLIC MINERAL MINING AND QUARRYING
0480 .EXT-NOT SPECIFIED TYPE OF MINING
0490 .EXT-SUPPORT ACTIVITIES FOR MINING
0570 .UTL-ELECTRIC POWER GENERATION, TRANSMISSION AND
     .DISTRIBUTION
0580 .UTL-NATURAL GAS DISTRIBUTION
0590 .UTL-ELECTRIC AND GAS, AND OTHER COMBINATIONS
0670 .UTL-WATER, STEAM, AIR CONDITIONING, AND IRRIGATION SYSTEMS
0680 .UTL-SEWAGE TREATMENT FACILITIES
0690 .UTL-NOT SPECIFIED UTILITIES
0770 .CON-CONSTRUCTION, INCL CLEANING DURING AND IMM AFTER
1070 .MFG-ANIMAL FOOD, GRAIN AND OILSEED MILLING
1080 .MFG-SUGAR AND CONFECTIONERY PRODUCTS
1090 .MFG-FRUIT AND VEGETABLE PRESERVING AND SPECIALTY FOODS
1170 .MFG-DAIRY PRODUCTS
1180 .MFG-ANIMAL SLAUGHTERING AND PROCESSING
1190 .MFG-RETAIL BAKERIES
1270 .MFG-BAKERIES, EXCEPT RETAIL
1280 .MFG-SEAFOOD AND OTHER MISCELLANEOUS FOODS, N.E.C.
1290 .MFG-NOT SPECIFIED FOOD INDUSTRIES
1370 .MFG-BEVERAGE
1390 .MFG-TOBACCO
1470 .MFG-FIBER, YARN, AND THREAD MILLS
1480 .MFG-FABRIC MILLS, EXCEPT KNITTING
1490 .MFG-TEXTILE AND FABRIC FINISHING AND COATING MILLS
1570 .MFG-CARPET AND RUG MILLS
1590 .MFG-TEXTILE PRODUCT MILLS, EXCEPT CARPET AND RUG
1670 .MFG-KNITTING MILLS
1680 .MFG-CUT AND SEW APPAREL
1690 .MFG-APPAREL ACCESSORIES AND OTHER APPAREL
1770 .MFG-FOOTWEAR
1790 .MFG-LEATHER TANNING AND PRODUCTS, EXCEPT FOOTWEAR
1870 .MFG-PULP, PAPER, AND PAPERBOARD MILLS
1880 .MFG-PAPERBOARD CONTAINERS AND BOXES
1890 .MFG-MISCELLANEOUS PAPER AND PULP PRODUCTS
1990 .MFG-PRINTING AND RELATED SUPPORT ACTIVITIES
2070 .MFG-PETROLEUM REFINING
2090 .MFG-MISCELLANEOUS PETROLEUM AND COAL PRODUCTS
2170 .MFG-RESIN, SYNTHETIC RUBBER AND FIBERS, AND FILAMENTS
2180 .MFG-AGRICULTURAL CHEMICALS
2190 .MFG-PHARMACEUTICALS AND MEDICINES
2270 .MFG-PAINT, COATING, AND ADHESIVES
2280 .MFG-SOAP, CLEANING COMPOUND, AND COSMETICS
2290 .MFG-INDUSTRIAL AND MISCELLANEOUS CHEMICALS
2370 .MFG-PLASTICS PRODUCTS
2380 .MFG-TIRES
2390 .MFG-RUBBER PRODUCTS, EXCEPT TIRES
2470 .MFG-POTTERY, CERAMICS, AND RELATED PRODUCTS
2480 .MFG-STRUCTURAL CLAY PRODUCTS
2490 .MFG-GLASS AND GLASS PRODUCTS
 
 2570 .MFG-CEMENT, CONCRETE, LIME, AND GYPSUM PRODUCTS
2590 .MFG-MISCELLANEOUS NONMETALLIC MINERAL PRODUCTS
2670 .MFG-IRON AND STEEL MILLS AND STEEL PRODUCTS
2680 .MFG-ALUMINUM PRODUCTION AND PROCESSING
2690 .MFG-NONFERROUS METAL, EXCEPT ALUMINUM, PRODUCTION AND
     .PROCESSING
2770 .MFG-FOUNDRIES
2780 .MFG-METAL FORGINGS AND STAMPINGS
2790 .MFG-CUTLERY AND HAND TOOLS
2870 .MFG-STRUCTURAL METALS, AND TANK AND SHIPPING CONTAINERS
2880 .MFG-MACHINE SHOPS; TURNED PRODUCTS; SCREWS, NUTS AND BOLTS
2890 .MFG-COATING, ENGRAVING, HEAT TREATING AND ALLIED ACTIVITIES
2970 .MFG-ORDNANCE
2980 .MFG-MISCELLANEOUS FABRICATED METAL PRODUCTS
2990 .MFG-NOT SPECIFIED METAL INDUSTRIES
3070 .MFG-AGRICULTURAL IMPLEMENTS
3080 .MFG-CONSTRUCTION, MINING AND OIL FIELD MACHINERY
3090 .MFG-COMMERCIAL AND SERVICE INDUSTRY MACHINERY
3170 .MFG-METALWORKING MACHINERY
3180 .MFG-ENGINES, TURBINES, AND POWER TRANSMISSION EQUIPMENT
3190 .MFG-MACHINERY, N.E.C.
3290 .MFG-NOT SPECIFIED MACHINERY
3360 .MFG-COMPUTER AND PERIPHERAL EQUIPMENT
3370 .MFG-COMMUNICATIONS, AUDIO, AND VIDEO EQUIPMENT
3380 .MFG-NAVIGATIONAL, MEASURING, ELECTROMEDICAL, AND CONTROL
     .INSTRUMENTS
3390 .MFG-ELECTRONIC COMPONENTS AND PRODUCTS, N.E.C.
3470 .MFG-HOUSEHOLD APPLIANCES
3490 .MFG-ELECTRICAL LIGHTING, EQUIPMENT, AND SUPPLIES, N.E.C.
3570 .MFG-MOTOR VEHICLES AND MOTOR VEHICLE EQUIPMENT
3580 .MFG-AIRCRAFT AND PARTS
3590 .MFG-AEROSPACE PRODUCTS AND PARTS
3670 .MFG-RAILROAD ROLLING STOCK
3680 .MFG-SHIP AND BOAT BUILDING
3690 .MFG-OTHER TRANSPORTATION EQUIPMENT
3770 .MFG-SAWMILLS AND WOOD PRESERVATION
3780 .MFG-VENEER, PLYWOOD, AND ENGINEERED WOOD PRODUCTS
3790 .MFG-PREFABRICATED WOOD BUILDINGS AND MOBILE HOMES
3870 .MFG-MISCELLANEOUS WOOD PRODUCTS
3890 .MFG-FURNITURE AND RELATED PRODUCTS
3960 .MFG-MEDICAL EQUIPMENT AND SUPPLIES
3970 .MFG-TOYS, AMUSEMENT, AND SPORTING GOODS
3980 .MFG-MISCELLANEOUS MANUFACTURING, N.E.C.
3990 .MFG-NOT SPECIFIED MANUFACTURING INDUSTRIES
4070 .WHL-MOTOR VEHICLES PARTS AND SUPPLIES MERCHANT WHOLESALERS
4080 .WHL-FURNITURE AND HOME FURNISHING MERCHANT WHOLESALERS
4090 .WHL-LUMBER AND OTHER CONSTRUCTION MATERIALS MERCHANT
     .WHOLESALERS
4170 .WHL-PROFESSIONAL AND COMMERCIAL EQUIPMENT AND SUPPLIES
     .MERCHANT WHOLESALERS
4180 .WHL-METALS AND MINERALS, EXCEPT PETROLEUM, MERCHANT
     .WHOLESALERS
4190 .WHL-ELECTRICAL GOODS MERCHANT WHOLESALERS
4260 .WHL-HARDWARE, PLUMBING AND HEATING EQUIPMENT, AND SUPPLIES
     .MERCHANT WHOLESALERS
4270 .WHL-MACHINERY, EQUIPMENT, AND SUPPLIES MERCHANT WHOLESALERS
4280 .WHL-RECYCLABLE MATERIAL MERCHANT WHOLESALERS
4290 .WHL-MISCELLANEOUS DURABLE GOODS MERCHANT WHOLESALERS
4370 .WHL-PAPER AND PAPER PRODUCTS MERCHANT WHOLESALERS
4380 .WHL-DRUGS, SUNDRIES, AND CHEMICAL AND ALLIED PRODUCTS
     .MERCHANT WHOLESALERS
 
 4390 .WHL-APPAREL, FABRICS, AND NOTIONS MERCHANT WHOLESALERS
4470 .WHL-GROCERIES AND RELATED PRODUCTS MERCHANT WHOLESALERS
4480 .WHL-FARM PRODUCT RAW MATERIALS MERCHANT WHOLESALERS
4490 .WHL-PETROLEUM AND PETROLEUM PRODUCTS MERCHANT WHOLESALERS
4560 .WHL-ALCOHOLIC BEVERAGES MERCHANT WHOLESALERS
4570 .WHL-FARM SUPPLIES MERCHANT WHOLESALERS
4580 .WHL-MISCELLANEOUS NONDURABLE GOODS MERCHANT WHOLESALERS
4585 .WHL-ELECTRONIC MARKETS AGENTS AND BROKERS
4590 .WHL-NOT SPECIFIED WHOLESALE TRADE
4670 .RET-AUTOMOBILE DEALERS
4680 .RET-OTHER MOTOR VEHICLE DEALERS
4690 .RET-AUTO PARTS, ACCESSORIES, AND TIRE STORES
4770 .RET-FURNITURE AND HOME FURNISHINGS STORES
4780 .RET-HOUSEHOLD APPLIANCE STORES
4790 .RET-RADIO, TV, AND COMPUTER STORES
4870 .RET-BUILDING MATERIAL AND SUPPLIES DEALERS
4880 .RET-HARDWARE STORES
4890 .RET-LAWN AND GARDEN EQUIPMENT AND SUPPLIES STORES
4970 .RET-GROCERY STORES
4980 .RET-SPECIALTY FOOD STORES
4990 .RET-BEER, WINE, AND LIQUOR STORES
5070 .RET-PHARMACIES AND DRUG STORES
5080 .RET-HEALTH AND PERSONAL CARE, EXCEPT DRUG, STORES
5090 .RET-GASOLINE STATIONS
5170 .RET-CLOTHING AND ACCESSORIES, EXCEPT SHOE, STORES
5180 .RET-SHOE STORES
5190 .RET-JEWELRY, LUGGAGE,AND LEATHER GOODS STORES
5270 .RET-SPORTING GOODS, CAMERA, AND HOBBY AND TOY STORES
5280 .RET-SEWING, NEEDLEWORK AND PIECE GOODS STORES
5290 .RET-MUSIC STORES
5370 .RET-BOOK STORES AND NEWS DEALERS
5380 .RET-DEPARTMENT AND DISCOUNT STORES
5390 .RET-MISCELLANEOUS GENERAL MERCHANDISE STORES
5470 .RET-FLORISTS
5480 .RET-OFFICE SUPPLIES AND STATIONARY STORES
5490 .RET-USED MERCHANDISE STORES
5570 .RET-GIFT, NOVELTY, AND SOUVENIR SHOPS
5580 .RET-MISCELLANEOUS RETAIL STORES
5590 .RET-ELECTRONIC SHOPPING
5591 .RET-ELECTRONIC AUCTIONS
5592 .RET-MAIL-ORDER HOUSES
5670 .RET-VENDING MACHINE OPERATORS
5680 .RET-FUEL DEALERS
5690 .RET-OTHER DIRECT SELLING ESTABLISHMENTS
5790 .RET-NOT SPECIFIED RETAIL TRADE
6070 .TRN-AIR TRANSPORTATION
6080 .TRN-RAIL TRANSPORTATION
6090 .TRN-WATER TRANSPORTATION
6170 .TRN-TRUCK TRANSPORTATION
6180 .TRN-BUS SERVICE AND URBAN TRANSIT
6190 .TRN-TAXI AND LIMOUSINE SERVICE
6270 .TRN-PIPELINE TRANSPORTATION
6280 .TRN-SCENIC AND SIGHTSEEING TRANSPORTATION
6290 .TRN-SERVICES INCIDENTAL TO TRANSPORTATION
6370 .TRN-POSTAL SERVICE
6380 .TRN-COURIERS AND MESSENGERS
6390 .TRN-WAREHOUSING AND STORAGE
6470 .INF-NEWSPAPER PUBLISHERS
6480 .INF-PUBLISHING, EXCEPT NEWSPAPERS AND SOFTWARE
6490 .INF-SOFTWARE PUBLISHING
6570 .INF-MOTION PICTURES AND VIDEO INDUSTRIES
 
 6590 .INF-SOUND RECORDING INDUSTRIES
6670 .INF-RADIO AND TELEVISION BROADCASTING AND CABLE
6675 .INF-INTERNET PUBLISHING AND BROADCASTING
6680 .INF-WIRED TELECOMMUNICATIONS CARRIERS
6690 .INF-OTHER TELECOMMUNICATION SERVICES
6692 .INF-INTERNET SERVICE PROVIDERS
6695 .INF-DATA PROCESSING, HOSTING, AND RELATED SERVICES
6770 .INF-LIBRARIES AND ARCHIVES
6780 .INF-OTHER INFORMATION SERVICES
6870 .FIN-BANKING AND RELATED ACTIVITIES
6880 .FIN-SAVINGS INSTITUTIONS, INCLUDING CREDIT UNIONS
6890 .FIN-NON-DEPOSITORY CREDIT AND RELATED ACTIVITIES
6970 .FIN-SECURITIES, COMMODITIES, FUNDS, TRUSTS, AND OTHER
     .FINANCIAL INVESTMENTS
6990 .FIN-INSURANCE CARRIERS AND RELATED ACTIVITIES
7070 .FIN-REAL ESTATE
7080 .FIN-AUTOMOTIVE EQUIPMENT RENTAL AND LEASING
7170 .FIN-VIDEO TAPE AND DISK RENTAL
7180 .FIN-OTHER CONSUMER GOODS RENTAL
7190 .FIN-COMMERCIAL, INDUSTRIAL, AND OTHER INTANGIBLE ASSETS
     .RENTAL AND LEASING
7270 .PRF-LEGAL SERVICES
7280 .PRF-ACCOUNTING, TAX PREPARATION, BOOKKEEPING AND PAYROLL
     .SERVICES
7290 .PRF-ARCHITECTURAL, ENGINEERING, AND RELATED SERVICES
7370 .PRF-SPECIALIZED DESIGN SERVICES
7380 .PRF-COMPUTER SYSTEMS DESIGN AND RELATED SERVICES
7390 .PRF-MANAGEMENT, SCIENTIFIC, AND TECHNICAL CONSULTING
     .SERVICES
7460 .PRF-SCIENTIFIC RESEARCH AND DEVELOPMENT SERVICES
7470 .PRF-ADVERTISING AND RELATED SERVICES
7480 .PRF-VETERINARY SERVICES
7490 .PRF-OTHER PROFESSIONAL, SCIENTIFIC, AND TECHNICAL SERVICES
7570 .PRF-MANAGEMENT OF COMPANIES AND ENTERPRISES
7580 .PRF-EMPLOYMENT SERVICES
7590 .PRF-BUSINESS SUPPORT SERVICES
7670 .PRF-TRAVEL ARRANGEMENTS AND RESERVATION SERVICES
7680 .PRF-INVESTIGATION AND SECURITY SERVICES
7690 .PRF-SERVICES TO BUILDINGS AND DWELLINGS, EX CONSTR CLN
7770 .PRF-LANDSCAPING SERVICES
7780 .PRF-OTHER ADMINISTRATIVE, AND OTHER SUPPORT SERVICES
7790 .PRF-WASTE MANAGEMENT AND REMEDIATION SERVICES
7860 .EDU-ELEMENTARY AND SECONDARY SCHOOLS
7870 .EDU-COLLEGES AND UNIVERSITIES, INCLUDING JUNIOR COLLEGES
7880 .EDU-BUSINESS, TECHNICAL, AND TRADE SCHOOLS AND TRAINING
7890 .EDU-OTHER SCHOOLS, INSTRUCTION, AND EDUCATIONAL SERVICES
7970 .MED-OFFICES OF PHYSICIANS
7980 .MED-OFFICES OF DENTISTS
7990 .MED-OFFICE OF CHIROPRACTORS
8070 .MED-OFFICES OF OPTOMETRISTS
8080 .MED-OFFICES OF OTHER HEALTH PRACTITIONERS
8090 .MED-OUTPATIENT CARE CENTERS
8170 .MED-HOME HEALTH CARE SERVICES
8180 .MED-OTHER HEALTH CARE SERVICES
8190 .MED-HOSPITALS
8270 .MED-NURSING CARE FACILITIES
8290 .MED-RESIDENTIAL CARE FACILITIES, WITHOUT NURSING
8370 .SCA-INDIVIDUAL AND FAMILY SERVICES
8380 .SCA-COMMUNITY FOOD AND HOUSING, AND EMERGENCY SERVICES
8390 .SCA-VOCATIONAL REHABILITATION SERVICES
8470 .SCA-CHILD DAY CARE SERVICES
 
             8560 .ENT-INDEPENDENT ARTISTS, PERFORMING ARTS, SPECTATOR SPORTS
                 .AND RELATED INDUSTRIES
            8570 .ENT-MUSEUMS, ART GALLERIES, HISTORICAL SITES, AND SIMILAR
                 .INSTITUTIONS
            8580 .ENT-BOWLING CENTERS
            8590 .ENT-OTHER AMUSEMENT, GAMBLING, AND RECREATION INDUSTRIES
            8660 .ENT-TRAVELER ACCOMMODATION
            8670 .ENT-RECREATIONAL VEHICLE PARKS AND CAMPS, AND ROOMING AND
                 .BOARDING HOUSES
            8680 .ENT-RESTAURANTS AND OTHER FOOD SERVICES
            8690 .ENT-DRINKING PLACES, ALCOHOLIC BEVERAGES
            8770 .SRV-AUTOMOTIVE REPAIR AND MAINTENANCE
            8780 .SRV-CAR WASHES
            8790 .SRV-ELECTRONIC AND PRECISION EQUIPMENT REPAIR AND
                 .MAINTENANCE
            8870 .SRV-COMMERCIAL AND INDUSTRIAL MACHINERY AND EQUIPMENT
                 .REPAIR AND MAINTENANCE
            8880 .SRV-PERSONAL AND HOUSEHOLD GOODS REPAIR AND MAINTENANCE
            8970 .SRV-BARBER SHOPS
            8980 .SRV-BEAUTY SALONS
            8990 .SRV-NAIL SALONS AND OTHER PERSONAL CARE SERVICES
            9070 .SRV-DRYCLEANING AND LAUNDRY SERVICES
            9080 .SRV-FUNERAL HOMES, CEMETERIES AND CREMATORIES
            9090 .SRV-OTHER PERSONAL SERVICES
            9160 .SRV-RELIGIOUS ORGANIZATIONS
            9170 .SRV-CIVIC, SOCIAL, ADVOCACY ORGANIZATIONS, AND GRANTMAKING
                 .AND GIVING SERVICES
            9180 .SRV-LABOR UNIONS
            9190 .SRV-BUSINESS, PROFESSIONAL, POLITICAL AND SIMILAR
                 .ORGANIZATIONS
            9290 .SRV-PRIVATE HOUSEHOLDS
            9370 .ADM-EXECUTIVE OFFICES AND LEGISLATIVE BODIES
            9380 .ADM-PUBLIC FINANCE ACTIVITIES
            9390 .ADM-OTHER GENERAL GOVERNMENT AND SUPPORT
            9470 .ADM-JUSTICE, PUBLIC ORDER, AND SAFETY ACTIVITIES
            9480 .ADM-ADMINISTRATION OF HUMAN RESOURCE PROGRAMS
            9490 .ADM-ADMINISTRATION OF ENVIRONMENTAL QUALITY AND HOUSING
                 .PROGRAMS
            9570 .ADM-ADMINISTRATION OF ECONOMIC PROGRAMS AND SPACE RESEARCH
            9590 .ADM-NATIONAL SECURITY AND INTERNATIONAL AFFAIRS
            9670 .MIL-U.S. ARMY
            9680 .MIL-U.S. AIR FORCE
            9690 .MIL-U.S. NAVY
            9770 .MIL-U.S. MARINES
            9780 .MIL-U.S. COAST GUARD
            9790 .MIL-U.S. ARMED FORCES, BRANCH NOT SPECIFIED
            9870 .MIL-MILITARY RESERVES OR NATIONAL GUARD
            9920 .UNEMPLOYED, WITH NO WORK EXPERIENCE IN THE LAST 5 YEARS **
JWAP        3
     Time of arrival at work - hour and minute
            bbb .N/A (not a worker; worker who worked at
                        .home)
            001 .12:00 a.m. to 12:04 a.m.
            002 .12:05 a.m. to 12:09 a.m.
            003 .12:10 a.m. to 12:14 a.m.
            004 .12:15 a.m. to 12:19 a.m.
            005 .12:20 a.m. to 12:24 a.m.
            006 .12:25 a.m. to 12:29 a.m.
            007 .12:30 a.m. to 12:39 a.m.
            008 .12:40 a.m. to 12:44 a.m.
 
 009 .12:45 a.m. to 12:49 a.m.
010 .12:50 a.m. to 12:59 a.m.
011 .1:00 a.m. to 1:04 a.m.
012 .1:05 a.m. to 1:09 a.m.
013 .1:10 a.m. to 1:14 a.m.
014 .1:15 a.m. to 1:19 a.m.
015 .1:20 a.m. to 1:24 a.m.
016 .1:25 a.m. to 1:29 a.m.
017 .1:30 a.m. to 1:34 a.m.
018 .1:35 a.m. to 1:39 a.m.
019 .1:40 a.m. to 1:44 a.m.
020 .1:45 a.m. to 1:49 a.m.
021 .1:50 a.m. to 1:59 a.m.
022 .2:00 a.m. to 2:04 a.m.
023 .2:05 a.m. to 2:09 a.m.
024 .2:10 a.m. to 2:14 a.m.
025 .2:15 a.m. to 2:19 a.m.
026 .2:20 a.m. to 2:24 a.m.
027 .2:25 a.m. to 2:29 a.m.
028 .2:30 a.m. to 2:34 a.m.
029 .2:35 a.m. to 2:39 a.m.
030 .2:40 a.m. to 2:44 a.m.
031 .2:45 a.m. to 2:49 a.m.
032 .2:50 a.m. to 2:54 a.m.
033 .2:55 a.m. to 2:59 a.m.
034 .3:00 a.m. to 3:04 a.m.
035 .3:05 a.m. to 3:09 a.m.
036 .3:10 a.m. to 3:14 a.m.
037 .3:15 a.m. to 3:19 a.m.
038 .3:20 a.m. to 3:24 a.m.
039 .3:25 a.m. to 3:29 a.m.
040 .3:30 a.m. to 3:34 a.m.
041 .3:35 a.m. to 3:39 a.m.
042 .3:40 a.m. to 3:44 a.m.
043 .3:45 a.m. to 3:49 a.m.
044 .3:50 a.m. to 3:54 a.m.
045 .3:55 a.m. to 3:59 a.m.
046 .4:00 a.m. to 4:04 a.m.
047 .4:05 a.m. to 4:09 a.m.
048 .4:10 a.m. to 4:14 a.m.
049 .4:15 a.m. to 4:19 a.m.
050 .4:20 a.m. to 4:24 a.m.
051 .4:25 a.m. to 4:29 a.m.
052 .4:30 a.m. to 4:34 a.m.
053 .4:35 a.m. to 4:39 a.m.
054 .4:40 a.m. to 4:44 a.m.
055 .4:45 a.m. to 4:49 a.m.
056 .4:50 a.m. to 4:54 a.m.
057 .4:55 a.m. to 4:59 a.m.
058 .5:00 a.m. to 5:04 a.m.
059 .5:05 a.m. to 5:09 a.m.
060 .5:10 a.m. to 5:14 a.m.
061 .5:15 a.m. to 5:19 a.m.
062 .5:20 a.m. to 5:24 a.m.
063 .5:25 a.m. to 5:29 a.m.
064 .5:30 a.m. to 5:34 a.m.
065 .5:35 a.m. to 5:39 a.m.
066 .5:40 a.m. to 5:44 a.m.
067 .5:45 a.m. to 5:49 a.m.
068 .5:50 a.m. to 5:54 a.m.
069 .5:55 a.m. to 5:59 a.m.
 
 070 .6:00 a.m. to 6:04 a.m.
071 .6:05 a.m. to 6:09 a.m.
072 .6:10 a.m. to 6:14 a.m.
073 .6:15 a.m. to 6:19 a.m.
074 .6:20 a.m. to 6:24 a.m.
075 .6:25 a.m. to 6:29 a.m.
076 .6:30 a.m. to 6:34 a.m.
077 .6:35 a.m. to 6:39 a.m.
078 .6:40 a.m. to 6:44 a.m.
079 .6:45 a.m. to 6:49 a.m.
080 .6:50 a.m. to 6:54 a.m.
081 .6:55 a.m. to 6:59 a.m.
082 .7:00 a.m. to 7:04 a.m.
083 .7:05 a.m. to 7:09 a.m.
084 .7:10 a.m. to 7:14 a.m.
085 .7:15 a.m. to 7:19 a.m.
086 .7:20 a.m. to 7:24 a.m.
087 .7:25 a.m. to 7:29 a.m.
088 .7:30 a.m. to 7:34 a.m.
089 .7:35 a.m. to 7:39 a.m.
090 .7:40 a.m. to 7:44 a.m.
091 .7:45 a.m. to 7:49 a.m.
092 .7:50 a.m. to 7:54 a.m.
093 .7:55 a.m. to 7:59 a.m.
094 .8:00 a.m. to 8:04 a.m.
095 .8:05 a.m. to 8:09 a.m.
096 .8:10 a.m. to 8:14 a.m.
097 .8:15 a.m. to 8:19 a.m.
098 .8:20 a.m. to 8:24 a.m.
099 .8:25 a.m. to 8:29 a.m.
100 .8:30 a.m. to 8:34 a.m.
101 .8:35 a.m. to 8:39 a.m.
102 .8:40 a.m. to 8:44 a.m.
103 .8:45 a.m. to 8:49 a.m.
104 .8:50 a.m. to 8:54 a.m.
105 .8:55 a.m. to 8:59 a.m.
106 .9:00 a.m. to 9:04 a.m.
107 .9:05 a.m. to 9:09 a.m.
108 .9:10 a.m. to 9:14 a.m.
109 .9:15 a.m. to 9:19 a.m.
110 .9:20 a.m. to 9:24 a.m.
111 .9:25 a.m. to 9:29 a.m.
112 .9:30 a.m. to 9:34 a.m.
113 .9:35 a.m. to 9:39 a.m.
114 .9:40 a.m. to 9:44 a.m.
115 .9:45 a.m. to 9:49 a.m.
116 .9:50 a.m. to 9:54 a.m.
117 .9:55 a.m. to 9:59 a.m.
118 .10:00 a.m. to 10:04 a.m.
119 .10:05 a.m. to 10:09 a.m.
120 .10:10 a.m. to 10:14 a.m.
121 .10:15 a.m. to 10:19 a.m.
122 .10:20 a.m. to 10:24 a.m.
123 .10:25 a.m. to 10:29 a.m.
124 .10:30 a.m. to 10:34 a.m.
125 .10:35 a.m. to 10:39 a.m.
126 .10:40 a.m. to 10:44 a.m.
127 .10:45 a.m. to 10:49 a.m.
128 .10:50 a.m. to 10:54 a.m.
129 .10:55 a.m. to 10:59 a.m.
130 .11:00 a.m. to 11:04 a.m.
 
 131 .11:05 a.m. to 11:09 a.m.
132 .11:10 a.m. to 11:14 a.m.
133 .11:15 a.m. to 11:19 a.m.
134 .11:20 a.m. to 11:24 a.m.
135 .11:25 a.m. to 11:29 a.m.
136 .11:30 a.m. to 11:34 a.m.
137 .11:35 a.m. to 11:39 a.m.
138 .11:40 a.m. to 11:44 a.m.
139 .11:45 a.m. to 11:49 a.m.
140 .11:50 a.m. to 11:54 a.m.
141 .11:55 a.m. to 11:59 a.m.
142 .12:00 p.m. to 12:04 p.m.
143 .12:05 p.m. to 12:09 p.m.
144 .12:10 p.m. to 12:14 p.m.
145 .12:15 p.m. to 12:19 p.m.
146 .12:20 p.m. to 12:24 p.m.
147 .12:25 p.m. to 12:29 p.m.
148 .12:30 p.m. to 12:34 p.m.
149 .12:35 p.m. to 12:39 p.m.
150 .12:40 p.m. to 12:44 p.m.
151 .12:45 p.m. to 12:49 p.m.
152 .12:50 p.m. to 12:54 p.m.
153 .12:55 p.m. to 12:59 p.m.
154 .1:00 p.m. to 1:04 p.m.
155 .1:05 p.m. to 1:09 p.m.
156 .1:10 p.m. to 1:14 p.m.
157 .1:15 p.m. to 1:19 p.m.
158 .1:20 p.m. to 1:24 p.m.
159 .1:25 p.m. to 1:29 p.m.
160 .1:30 p.m. to 1:34 p.m.
161 .1:35 p.m. to 1:39 p.m.
162 .1:40 p.m. to 1:44 p.m.
163 .1:45 p.m. to 1:49 p.m.
164 .1:50 p.m. to 1:54 p.m.
165 .1:55 p.m. to 1:59 p.m.
166 .2:00 p.m. to 2:04 p.m.
167 .2:05 p.m. to 2:09 p.m.
168 .2:10 p.m. to 2:14 p.m.
169 .2:15 p.m. to 2:19 p.m.
170 .2:20 p.m. to 2:24 p.m.
171 .2:25 p.m. to 2:29 p.m.
172 .2:30 p.m. to 2:34 p.m.
173 .2:35 p.m. to 2:39 p.m.
174 .2:40 p.m. to 2:44 p.m.
175 .2:45 p.m. to 2:49 p.m.
176 .2:50 p.m. to 2:54 p.m.
177 .2:55 p.m. to 2:59 p.m.
178 .3:00 p.m. to 3:04 p.m.
179 .3:05 p.m. to 3:09 p.m.
180 .3:10 p.m. to 3:14 p.m.
181 .3:15 p.m. to 3:19 p.m.
182 .3:20 p.m. to 3:24 p.m.
183 .3:25 p.m. to 3:29 p.m.
184 .3:30 p.m. to 3:34 p.m.
185 .3:35 p.m. to 3:39 p.m.
186 .3:40 p.m. to 3:44 p.m.
187 .3:45 p.m. to 3:49 p.m.
188 .3:50 p.m. to 3:54 p.m.
189 .3:55 p.m. to 3:59 p.m.
190 .4:00 p.m. to 4:04 p.m.
191 .4:05 p.m. to 4:09 p.m.
 
 192 .4:10 p.m. to 4:14 p.m.
193 .4:15 p.m. to 4:19 p.m.
194 .4:20 p.m. to 4:24 p.m.
195 .4:25 p.m. to 4:29 p.m.
196 .4:30 p.m. to 4:34 p.m.
197 .4:35 p.m. to 4:39 p.m.
198 .4:40 p.m. to 4:44 p.m.
199 .4:45 p.m. to 4:49 p.m.
200 .4:50 p.m. to 4:54 p.m.
201 .4:55 p.m. to 4:59 p.m.
202 .5:00 p.m. to 5:04 p.m.
203 .5:05 p.m. to 5:09 p.m.
204 .5:10 p.m. to 5:14 p.m.
205 .5:15 p.m. to 5:19 p.m.
206 .5:20 p.m. to 5:24 p.m.
207 .5:25 p.m. to 5:29 p.m.
208 .5:30 p.m. to 5:34 p.m.
209 .5:35 p.m. to 5:39 p.m.
210 .5:40 p.m. to 5:44 p.m.
211 .5:45 p.m. to 5:49 p.m.
212 .5:50 p.m. to 5:54 p.m.
213 .5:55 p.m. to 5:59 p.m.
214 .6:00 p.m. to 6:04 p.m.
215 .6:05 p.m. to 6:09 p.m.
216 .6:10 p.m. to 6:14 p.m.
217 .6:15 p.m. to 6:19 p.m.
218 .6:20 p.m. to 6:24 p.m.
219 .6:25 p.m. to 6:29 p.m.
220 .6:30 p.m. to 6:34 p.m.
221 .6:35 p.m. to 6:39 p.m.
222 .6:40 p.m. to 6:44 p.m.
223 .6:45 p.m. to 6:49 p.m.
224 .6:50 p.m. to 6:54 p.m.
225 .6:55 p.m. to 6:59 p.m.
226 .7:00 p.m. to 7:04 p.m.
227 .7:05 p.m. to 7:09 p.m.
228 .7:10 p.m. to 7:14 p.m.
229 .7:15 p.m. to 7:19 p.m.
230 .7:20 p.m. to 7:24 p.m.
231 .7:25 p.m. to 7:29 p.m.
232 .7:30 p.m. to 7:34 p.m.
233 .7:35 p.m. to 7:39 p.m.
234 .7:40 p.m. to 7:44 p.m.
235 .7:45 p.m. to 7:49 p.m.
236 .7:50 p.m. to 7:54 p.m.
237 .7:55 p.m. to 7:59 p.m.
238 .8:00 p.m. to 8:04 p.m.
239 .8:05 p.m. to 8:09 p.m.
240 .8:10 p.m. to 8:14 p.m.
241 .8:15 p.m. to 8:19 p.m.
242 .8:20 p.m. to 8:24 p.m.
243 .8:25 p.m. to 8:29 p.m.
244 .8:30 p.m. to 8:34 p.m.
245 .8:35 p.m. to 8:39 p.m.
246 .8:40 p.m. to 8:44 p.m.
247 .8:45 p.m. to 8:49 p.m.
248 .8:50 p.m. to 8:54 p.m.
249 .8:55 p.m. to 8:59 p.m.
250 .9:00 p.m. to 9:04 p.m.
251 .9:05 p.m. to 9:09 p.m.
252 .9:10 p.m. to 9:14 p.m.
 
             253 .9:15 p.m. to 9:19 p.m.
            254 .9:20 p.m. to 9:24 p.m.
            255 .9:25 p.m. to 9:29 p.m.
            256 .9:30 p.m. to 9:34 p.m.
            257 .9:35 p.m. to 9:39 p.m.
            258 .9:40 p.m. to 9:44 p.m.
            259 .9:45 p.m. to 9:49 p.m.
            260 .9:50 p.m. to 9:54 p.m.
            261 .9:55 p.m. to 9:59 p.m.
            262 .10:00 p.m. to 10:04 p.m.
            263 .10:05 p.m. to 10:09 p.m.
            264 .10:10 p.m. to 10:14 p.m.
            265 .10:15 p.m. to 10:19 p.m.
            266 .10:20 p.m. to 10:24 p.m.
            267 .10:25 p.m. to 10:29 p.m.
            268 .10:30 p.m. to 10:34 p.m.
            269 .10:35 p.m. to 10:39 p.m.
            270 .10:40 p.m. to 10:44 p.m.
            271 .10:45 p.m. to 10:49 p.m.
            272 .10:50 p.m. to 10:55 p.m.
            273 .10:55 p.m. to 10:59 p.m.
            274 .11:00 p.m. to 11:04 p.m.
            275 .11:05 p.m. to 11:09 p.m.
            276 .11:10 p.m. to 11:14 p.m.
            277 .11:15 p.m. to 11:19 p.m.
            278 .11:20 p.m. to 11:24 p.m.
            279 .11:25 p.m. to 11:29 p.m.
            280 .11:30 p.m. to 11:34 p.m.
            281 .11:35 p.m. to 11:39 p.m.
            282 .11:40 p.m. to 11:44 p.m.
            283 .11:45 p.m. to 11:49 p.m.
            284 .11:50 p.m. to 11:54 p.m.
            285 .11:55 p.m. to 11:59 p.m.
JWDP        3
     Time of departure for work - hour and minute
            bbb .N/A (not a worker; worker who worked at
                        .home)
            001 .12:00 a.m. to 12:29 a.m.
            002 .12:30 a.m. to 12:59 a.m.
            003 .1:00 a.m. to 1:29 a.m.
            004 .1:30 a.m. to 1:59 a.m.
            005 .2:00 a.m. to 2:29 a.m.
            006 .2:30 a.m. to 2:59 a.m.
            007 .3:00 a.m. to 3:09 a.m.
            008 .3:10 a.m. to 3:19 a.m.
            009 .3:20 a.m. to 3:29 a.m.
            010 .3:30 a.m. to 3:39 a.m.
            011 .3:40 a.m. to 3:49 a.m.
            012 .3:50 a.m. to 3:59 a.m.
            013 .4:00 a.m. to 4:09 a.m.
            014 .4:10 a.m. to 4:19 a.m.
            015 .4:20 a.m. to 4:29 a.m.
            016 .4:30 a.m. to 4:39 a.m.
            017 .4:40 a.m. to 4:49 a.m.
            018 .4:50 a.m. to 4:59 a.m.
            019 .5:00 a.m. to 5:04 a.m.
            020 .5:05 a.m. to 5:09 a.m.
            021 .5:10 a.m. to 5:14 a.m.
            022 .5:15 a.m. to 5:19 a.m.
            023 .5:20 a.m. to 5:24 a.m.
 
 024 .5:25 a.m. to 5:29 a.m.
025 .5:30 a.m. to 5:34 a.m.
026 .5:35 a.m. to 5:39 a.m.
027 .5:40 a.m. to 5:44 a.m.
028 .5:45 a.m. to 5:49 a.m.
029 .5:50 a.m. to 5:54 a.m.
030 .5:55 a.m. to 5:59 a.m.
031 .6:00 a.m. to 6:04 a.m.
032 .6:05 a.m. to 6:09 a.m.
033 .6:10 a.m. to 6:14 a.m.
034 .6:15 a.m. to 6:19 a.m.
035 .6:20 a.m. to 6:24 a.m.
036 .6:25 a.m. to 6:29 a.m.
037 .6:30 a.m. to 6:34 a.m.
038 .6:35 a.m. to 6:39 a.m.
039 .6:40 a.m. to 6:44 a.m.
040 .6:45 a.m. to 6:49 a.m.
041 .6:50 a.m. to 6:54 a.m.
042 .6:55 a.m. to 6:59 a.m.
043 .7:00 a.m. to 7:04 a.m.
044 .7:05 a.m. to 7:09 a.m.
045 .7:10 a.m. to 7:14 a.m.
046 .7:15 a.m. to 7:19 a.m.
047 .7:20 a.m. to 7:24 a.m.
048 .7:25 a.m. to 7:29 a.m.
049 .7:30 a.m. to 7:34 a.m.
050 .7:35 a.m. to 7:39 a.m.
051 .7:40 a.m. to 7:44 a.m.
052 .7:45 a.m. to 7:49 a.m.
053 .7:50 a.m. to 7:54 a.m.
054 .7:55 a.m. to 7:59 a.m.
055 .8:00 a.m. to 8:04 a.m.
056 .8:05 a.m. to 8:09 a.m.
057 .8:10 a.m. to 8:14 a.m.
058 .8:15 a.m. to 8:19 a.m.
059 .8:20 a.m. to 8:24 a.m.
060 .8:25 a.m. to 8:29 a.m.
061 .8:30 a.m. to 8:34 a.m.
062 .8:35 a.m. to 8:39 a.m.
063 .8:40 a.m. to 8:44 a.m.
064 .8:45 a.m. to 8:49 a.m.
065 .8:50 a.m. to 8:54 a.m.
066 .8:55 a.m. to 8:59 a.m.
067 .9:00 a.m. to 9:04 a.m.
068 .9:05 a.m. to 9:09 a.m.
069 .9:10 a.m. to 9:14 a.m.
070 .9:15 a.m. to 9:19 a.m.
071 .9:20 a.m. to 9:24 a.m.
072 .9:25 a.m. to 9:29 a.m.
073 .9:30 a.m. to 9:34 a.m.
074 .9:35 a.m. to 9:39 a.m.
075 .9:40 a.m. to 9:44 a.m.
076 .9:45 a.m. to 9:49 a.m.
077 .9:50 a.m. to 9:54 a.m.
078 .9:55 a.m. to 9:59 a.m.
079 .10:00 a.m. to 10:09 a.m.
080 .10:10 a.m. to 10:19 a.m.
081 .10:20 a.m. to 10:29 a.m.
082 .10:30 a.m. to 10:39 a.m.
083 .10:40 a.m. to 10:49 a.m.
084 .10:50 a.m. to 10:59 a.m.
 
 085 .11:00 a.m. to 11:09 a.m.
086 .11:10 a.m. to 11:19 a.m.
087 .11:20 a.m. to 11:29 a.m.
088 .11:30 a.m. to 11:39 a.m.
089 .11:40 a.m. to 11:49 a.m.
090 .11:50 a.m. to 11:59 a.m.
091 .12:00 p.m. to 12:09 p.m.
092 .12:10 p.m. to 12:19 p.m.
093 .12:20 p.m. to 12:29 p.m.
094 .12:30 p.m. to 12:39 p.m.
095 .12:40 p.m. to 12:49 p.m.
096 .12:50 p.m. to 12:59 p.m.
097 .1:00 p.m. to 1:09 p.m.
098 .1:10 p.m. to 1:19 p.m.
099 .1:20 p.m. to 1:29 p.m.
100 .1:30 p.m. to 1:39 p.m.
101 .1:40 p.m. to 1:49 p.m.
102 .1:50 p.m. to 1:59 p.m.
103 .2:00 p.m. to 2:09 p.m.
104 .2:10 p.m. to 2:19 p.m.
105 .2:20 p.m. to 2:29 p.m.
106 .2:30 p.m. to 2:39 p.m.
107 .2:40 p.m. to 2:49 p.m.
108 .2:50 p.m. to 2:59 p.m.
109 .3:00 p.m. to 3:09 p.m.
110 .3:10 p.m. to 3:19 p.m.
111 .3:20 p.m. to 3:29 p.m.
112 .3:30 p.m. to 3:39 p.m.
113 .3:40 p.m. to 3:49 p.m.
114 .3:50 p.m. to 3:59 p.m.
115 .4:00 p.m. to 4:09 p.m.
116 .4:10 p.m. to 4:19 p.m.
117 .4:20 p.m. to 4:29 p.m.
118 .4:30 p.m. to 4:39 p.m.
119 .4:40 p.m. to 4:49 p.m.
120 .4:50 p.m. to 4:59 p.m.
121 .5:00 p.m. to 5:09 p.m.
122 .5:10 p.m. to 5:19 p.m.
123 .5:20 p.m. to 5:29 p.m.
124 .5:30 p.m. to 5:39 p.m.
125 .5:40 p.m. to 5:49 p.m.
126 .5:50 p.m. to 5:59 p.m.
127 .6:00 p.m. to 6:09 p.m.
128 .6:10 p.m. to 6:19 p.m.
129 .6:20 p.m. to 6:29 p.m.
130 .6:30 p.m. to 6:39 p.m.
131 .6:40 p.m. to 6:49 p.m.
132 .6:50 p.m. to 6:59 p.m.
133 .7:00 p.m. to 7:29 p.m.
134 .7:30 p.m. to 7:59 p.m.
135 .8:00 p.m. to 8:29 p.m.
136 .8:30 p.m. to 8:59 p.m.
137 .9:00 p.m. to 9:09 p.m.
138 .9:10 p.m. to 9:19 p.m.
139 .9:20 p.m. to 9:29 p.m.
140 .9:30 p.m. to 9:39 p.m.
141 .9:40 p.m. to 9:49 p.m.
142 .9:50 p.m. to 9:59 p.m.
143 .10:00 p.m. to 10:09 p.m.
144 .10:10 p.m. to 10:19 p.m.
145 .10:20 p.m. to 10:29 p.m.
 
             146 .10:30  p.m. to 10:39 p.m.
            147 .10:40  p.m. to 10:49 p.m.
            148 .10:50  p.m. to 10:59 p.m.
            149 .11:00  p.m. to 11:29 p.m.
            150 .11:30  p.m. to 11:59 p.m.
LANP        3
     Language spoken at home
            bbb .N/A (less than 5 years old/speaks only English)
            601 .Jamaican Creole
            607 .German
            608 .Pennsylvania Dutch
            609 .Yiddish
            610 .Dutch
            611 .Afrikaans
            614 .Swedish
            615 .Danish
            616 .Norwegian
            619 .Italian
            620 .French
            622 .Patois
            623 .French Creole
            624 .Cajun
            625 .Spanish
            629 .Portuguese
            631 .Romanian
            635 .Irish Gaelic
            637 .Greek
            638 .Albanian
            639 .Russian
            641 .Ukrainian
            642 .Czech
            645 .Polish
            646 .Slovak
            647 .Bulgarian
            648 .Macedonian
            649 .Serbocroatian
            650 .Croatian
            651 .Serbian
            653 .Lithuanian
            654 .Lettish
            655 .Armenian
            656 .Persian
            657 .Pashto
            658 .Kurdish
            662 .India n.e.c.
            663 .Hindi
            664 .Bengali
            665 .Panjabi
            666 .Marathi
            667 .Gujarathi
            671 .Urdu
            674 .Nepali
            676 .Pakistan n.e.c.
            677 .Sinhalese
            679 .Finnish
            682 .Hungarian
            691 .Turkish
            701 .Telugu
            702 .Kannada
            703 .Malayalam
 
            704 .Tamil
           708 .Chinese
           711 .Cantonese
           712 .Mandarin
           714 .Formosan
           717 .Burmese
           720 .Thai
           721 .Miao-yao, Mien
           722 .Hmong
           723 .Japanese
           724 .Korean
           725 .Laotian
           726 .Mon-Khmer, Cambodian
           728 .Vietnamese
           732 .Indonesian
           739 .Malay
           742 .Tagalog
           743 .Bisayan
           744 .Sebuano
           746 .Ilocano
           752 .Chamorro
           767 .Samoan
           768 .Tongan
           776 .Hawaiian
           777 .Arabic
           778 .Hebrew
           779 .Syriac
           780 .Amharic
           783 .Cushite
           791 .Swahili
           792 .Bantu
           793 .Mande
           794 .Fulani
           796 .Kru, Ibo, Yoruba
           799 .African
           806 .Other Algonquian languages
           862 .Apache
           864 .Navaho
           907 .Dakota
           924 .Keres
           933 .Cherokee
           964 .Zuni
           966 .American Indian
           985 .Other Indo-European languages
           986 .Other Asian languages
           988 .Other Pacific Island languages
           989 .Other specified African languages
           990 .Aleut-Eskimo languages
           992 .South/Central American Indian languages
           993 .Other Specified North American Indian languages
           994 .Other languages
           996 .Uncodable
MIGPUMA    5
    Migration PUMA
                   bbbbb .N/A (person less than 1 year old/lived in
                         .same house 1 year ago)
                   00001 .Did not live in the United States or in Puerto
                        .Rico one year ago
                   00002 .Lived in Puerto Rico one year ago and current
 
                          .residence is in the U.S.
           00100..08200 .Assigned Migration PUMA.  Use with MIGSP.
MIGSP      3
    Migration recode - State or foreign country code
           bbb .N/A (person less than 1 year old/lived in
               .same house 1 year ago)
           001 .Alabama/AL
           002 .Alaska/AK
           004 .Arizona/AZ
           005 .Arkansas/AR
           006 .California/CA
           008 .Colorado/CO
           009 .Connecticut/CT
           010 .Delaware/DE
           011 .District of Columbia/DC
           012 .Florida/FL
           013 .Georgia/GA
           015 .Hawaii/HI
           016 .Idaho/ID
           017 .Illinois/IL
           018 .Indiana/IN
           019 .Iowa/IA
           020 .Kansas/KS
           021 .Kentucky/KY
           022 .Louisiana/LA
           023 .Maine/ME
           024 .Maryland/MD
           025 .Massachusetts/MA
           026 .Michigan/MI
           027 .Minnesota/MN
           028 .Mississippi/MS
           029 .Missouri/MO
           030 .Montana/MT
           031 .Nebraska/NE
           032 .Nevada/NV
           033 .New Hampshire/NH
           034 .New Jersey/NJ
           035 .New Mexico/NM
           036 .New York/NY
           037 .North Carolina/NC
           038 .North Dakota/ND
           039 .Ohio/OH
           040 .Oklahoma/OK
           041 .Oregon/OR
           042 .Pennsylvania/PA
           044 .Rhode Island/RI
           045 .South Carolina/SC
           046 .South Dakota/SD
           047 .Tennessee/TN
           048 .Texas/TX
           049 .Utah/UT
           050 .Vermont/VT
           051 .Virginia/VA
           053 .Washington/WA
           054 .West Virginia/WV
           055 .Wisconsin/WI
           056 .Wyoming/WY
           072 .Puerto Rico
           096 .US Island Areas, Not Specified
           109 .France
 
            110 .Germany
           111 .Northern Europe, Not Specified
           112 .Western Europe, Not Specified
           113 .Eastern Europe, Not Specified
           120 .Italy
           128 .Poland
           138 .United Kingdom, Excluding England
           139 .England
           163 .Russia
           164 .Ukraine
           169 .Other Europe, Not Specified
           207 .China, Hong Kong & Paracel Islands
           210 .India
           213 .Iraq
           214 .Israel
           215 .Japan
           217 .Korea
           233 .Philippines
           240 .Taiwan
           242 .Thailand
           247 .Vietnam
           251 .Eastern Asia, Not Specified
           252 .Western Asia, Not Specified
           253 .South Central Asia or Asia, Not Specified
           301 .Canada
           303 .Mexico
           312 .El Salvador
           313 .Guatemala
           314 .Honduras
           317 .Central America, Not Specified
           327 .Cuba
           329 .Domincan Republic
           333 .Jamaica
           344 .Caribbean and North America, Not Specified
           362 .Brazil
           364 .Colombia
           370 .Peru
           374 .South America, Not Specified
           462 .Africa
           463 .Eastern Africa, Not Specified
           464 .Northern Africa, Not Specified
           467 .Western Africa, Not Specified
           468 .Other Africa, Not Specified
           501 .Australia
           554 .Australian and New Zealand Subregions, Not Specified,
               .Oceania and at Sea
MSP        1
    Married, spouse present/spouse absent
           b .N/A (less than 15 years old)
           1 .Now married, spouse present
           2 .Now married, spouse absent
           3 .Widowed
           4 .Divorced
           5 .Separated
           6 .Never married
NAICSP     8
    NAICS Industry code
           bbbbbbbb .N/A (less than 16 years old/unemployed who
                     .never worked/NILF who last worked more than
 
        .5 years ago)
111    .AGR-CROP PRODUCTION
112    .AGR-ANIMAL PRODUCTION
1133   .AGR-LOGGING
113M   .AGR-FORESTRY EXCEPT LOGGING
114    .AGR-FISHING, HUNTING, AND TRAPPING
115    .AGR-SUPPORT ACTIVITIES FOR AGRICULTURE AND FORESTRY
211    .EXT-OIL AND GAS EXTRACTION
2121   .EXT-COAL MINING
2122   .EXT-METAL ORE MINING
2123   .EXT-NONMETALLIC MINERAL MINING AND QUARRYING
213    .EXT-SUPPORT ACTIVITIES FOR MINING
21S    .EXT-NOT SPECIFIED TYPE OF MINING
2211P  .UTL-ELECTRIC POWER GENERATION, TRANSMISSION AND
       .DISTRIBUTION
2212P  .UTL-NATURAL GAS DISTRIBUTION
22132  .UTL-SEWAGE TREATMENT FACILITIES
2213M  .UTL-WATER, STEAM, AIR CONDITIONING, AND IRRIGATION
       .SYSTEMS
221MP  .UTL-ELECTRIC AND GAS, AND OTHER COMBINATIONS
22S    .UTL-NOT SPECIFIED UTILITIES
23     .CON-CONSTRUCTION, INCL CLEANING DURING AND IMM AFTER
3113   .MFG-SUGAR AND CONFECTIONERY PRODUCTS
3114   .MFG-FRUIT AND VEGETABLE PRESERVING AND SPECIALTY FOODS
3115   .MFG-DAIRY PRODUCTS
3116   .MFG-ANIMAL SLAUGHTERING AND PROCESSING
311811 .MFG-RETAIL BAKERIES
3118Z  .MFG-BAKERIES, EXCEPT RETAIL
311M1  .MFG-ANIMAL FOOD, GRAIN AND OILSEED MILLING
311M2  .MFG-SEAFOOD AND OTHER MISCELLANEOUS FOODS, N.E.C.
311S   .MFG-NOT SPECIFIED FOOD INDUSTRIES
3121   .MFG-BEVERAGE
3122   .MFG-TOBACCO
3131   .MFG-FIBER, YARN, AND THREAD MILLS
3132Z  .MFG-FABRIC MILLS, EXCEPT KNITTING
3133   .MFG-TEXTILE AND FABRIC FINISHING AND COATING MILLS
31411  .MFG-CARPETS AND RUGS
314Z   .MFG-TEXTILE PRODUCT MILLS, EXCEPT CARPETS AND RUGS
3152   .MFG-CUT AND SEW APPAREL
3159   .MFG-APPAREL ACCESSORIES AND OTHER APPAREL
3162   .MFG-FOOTWEAR
316M   .MFG-LEATHER TANNING AND PRODUCTS, EXCEPT FOOTWEAR
31M    .MFG-KNITTING MILLS
3211   .MFG-SAWMILLS AND WOOD PRESERVATION
3212   .MFG-VENEER, PLYWOOD, AND ENGINEERED WOOD PRODUCTS
32199M .MFG-PREFABRICATED WOOD BUILDINGS AND MOBILE HOMES
3219ZM .MFG-MISCELLANEOUS WOOD PRODUCTS
3221   .MFG-PULP, PAPER, AND PAPERBOARD MILLS
32221  .MFG-PAPERBOARD CONTAINERS AND BOXES
3222M  .MFG-MISCELLANEOUS PAPER AND PULP PRODUCTS
323    .MFG-PRINTING AND RELATED SUPPORT ACTIVITIES
32411  .MFG-PETROLEUM REFINING
3241M  .MFG-MISCELLANEOUS PETROLEUM AND COAL PRODUCTS
3252   .MFG-RESIN, SYNTHETIC RUBBER AND FIBERS, AND FILAMENTS
3253   .MFG-AGRICULTURAL CHEMICALS
3254   .MFG-PHARMACEUTICALS AND MEDICINES
3255   .MFG-PAINT, COATING, AND ADHESIVES
3256   .MFG-SOAP, CLEANING COMPOUND, AND COSMETICS
325M   .MFG-INDUSTRIAL AND MISCELLANEOUS CHEMICALS
3261   .MFG-PLASTICS PRODUCTS
32621  .MFG-TIRES
 
 3262M   .MFG-RUBBER PRODUCTS, EXCEPT TIRES
32711   .MFG-POTTERY, CERAMICS, AND RELATED PRODUCTS
32712   .MFG-STRUCTURAL CLAY PRODUCTS
3272    .MFG-GLASS AND GLASS PRODUCTS
3279    .MFG-MISCELLANEOUS NONMETALLIC MINERAL PRODUCTS
327M    .MFG-CEMENT, CONCRETE, LIME, AND GYPSUM PRODUCTS
3313    .MFG-ALUMINUM PRODUCTION AND PROCESSING
3314    .MFG-NONFERROUS METAL, EXCEPT ALUMINUM, PRODUCTION AND
        .PROCESSING
3315    .MFG-FOUNDRIES
331M    .MFG-IRON AND STEEL MILLS AND STEEL PRODUCTS
3321    .MFG-METAL FORGINGS AND STAMPINGS
3322    .MFG-CUTLERY AND HAND TOOLS
3327    .MFG-MACHINE SHOPS; TURNED PRODUCTS; SCREWS, NUTS AND
        .BOLTS
3328    .MFG-COATING, ENGRAVING, HEAT TREATING AND ALLIED
        .ACTIVITIES
33299M  .MFG-ORDNANCE
332M    .MFG-STRUCTURAL METALS, AND TANK AND SHIPPING CONTAINERS
332MZ   .MFG-MISCELLANEOUS FABRICATED METAL PRODUCTS
33311   .MFG-AGRICULTURAL IMPLEMENTS
3331M   .MFG-CONSTRUCTION, MINING AND OIL FIELD MACHINERY
3333    .MFG-COMMERCIAL AND SERVICE INDUSTRY MACHINERY
3335    .MFG-METALWORKING MACHINERY
3336    .MFG-ENGINES, TURBINES, AND POWER TRANSMISSION EQUIPMENT
333M    .MFG-MACHINERY, N.E.C.
333S    .MFG-NOT SPECIFIED MACHINERY
3341    .MFG-COMPUTER AND PERIPHERAL EQUIPMENT
3345    .MFG-NAVIGATIONAL, MEASURING, ELECTROMEDICAL, AND CONTROL
        .INSTRUMENTS
334M1   .MFG-COMMUNICATIONS, AUDIO, AND VIDEO EQUIPMENT
334M2   .MFG-ELECTRONIC COMPONENTS AND PRODUCTS, N.E.C.
3352    .MFG-HOUSEHOLD APPLIANCES
335M    .MFG-ELECTRICAL LIGHTING, EQUIPMENT, AND SUPPLIES, N.E.C.
33641M1 .MFG-AIRCRAFT AND PARTS
33641M2 .MFG-AEROSPACE PRODUCTS AND PARTS
3365    .MFG-RAILROAD ROLLING STOCK
3366    .MFG-SHIP AND BOAT BUILDING
3369    .MFG-OTHER TRANSPORTATION EQUIPMENT
336M    .MFG-MOTOR VEHICLES AND MOTOR VEHICLE EQUIPMENT
337     .MFG-FURNITURE AND RELATED PRODUCTS
3391    .MFG-MEDICAL EQUIPMENT AND SUPPLIES
3399M   .MFG-TOYS, AMUSEMENT, AND SPORTING GOODS
3399ZM  .MFG-MISCELLANEOUS MANUFACTURING, N.E.C.
33MS    .MFG-NOT SPECIFIED METAL INDUSTRIES
3MS     .MFG-NOT SPECIFIED INDUSTRIES
4231    .WHL-MOTOR VEHICLES PARTS AND SUPPLIES MERCHANT
        .WHOLESALERS
4232    .WHL-FURNITURE AND HOME FURNISHING MERCHANT WHOLESALERS
4233    .WHL-LUMBER AND OTHER CONSTRUCTION MATERIALS MERCHANT
        .WHOLESALERS
4234    .WHL-PROFESSIONAL AND COMMERCIAL EQUIPMENT AND SUPPLIES
        .MERCHANT WHOLESALERS
4235    .WHL-METALS AND MINERALS, EXCEPT PETROLEUM, MERCHANT
        .WHOLESALERS
4236    .WHL-ELECTRICAL GOODS MERCHANT WHOLESALERS
4237    .WHL-HARDWARE, PLUMBING AND HEATING EQUIPMENT, AND
        .SUPPLIES MERCHANT WHOLESALERS
4238    .WHL-MACHINERY, EQUIPMENT, AND SUPPLIES MERCHANT
        .WHOLESALERS
42393   .WHL-RECYCLABLE MATERIAL MERCHANT WHOLESALERS
 
 4239Z  .WHL-MISCELLANEOUS DURABLE GOODS MERCHANT WHOLESALERS
4241   .WHL-PAPER AND PAPER PRODUCTS MERCHANT WHOLESALERS
4243   .WHL-APPAREL, FABRICS, AND NOTIONS MERCHANT WHOLESALERS
4244   .WHL-GROCERIES AND RELATED PRODUCTS MERCHANT WHOLESALERS
4245   .WHL-FARM PRODUCT RAW MATERIALS MERCHANT WHOLESALERS
4247   .WHL-PETROLEUM AND PETROLEUM PRODUCTS MERCHANT WHOLESALERS
4248   .WHL-ALCOHOLIC BEVERAGES MERCHANT WHOLESALERS
42491  .WHL-FARM SUPPLIES MERCHANT WHOLESALERS
4249Z  .WHL-MISCELLANEOUS NONDURABLE GOODS MERCHANT WHOLESALERS
424M   .WHL-DRUGS, SUNDRIES, AND CHEMICAL AND ALLIED PRODUCTS
       .MERCHANT WHOLESALERS
4251   .WHL-ELECTRONIC MARKETS AGENTS AND BROKERS
42S    .WHL-NOT SPECIFIED TRADE
4411   .RET-AUTOMOBILE DEALERS
4412   .RET-OTHER MOTOR VEHICLE DEALERS
4413   .RET-AUTO PARTS, ACCESSORIES, AND TIRE STORES
442    .RET-FURNITURE AND HOME FURNISHINGS STORES
443111 .RET-HOUSEHOLD APPLIANCE STORES
4431M  .RET-RADIO, TV, AND COMPUTER STORES
44413  .RET-HARDWARE STORES
4441Z  .RET-BUILDING MATERIAL AND SUPPLIES DEALERS
4442   .RET-LAWN AND GARDEN EQUIPMENT AND SUPPLIES STORES
4451   .RET-GROCERY STORES
4452   .RET-SPECIALTY FOOD STORES
4453   .RET-BEER, WINE, AND LIQUOR STORES
44611  .RET-PHARMACIES AND DRUG STORES
446Z   .RET-HEALTH AND PERSONAL CARE, EXCEPT DRUG, STORES
447    .RET-GASOLINE STATIONS
44821  .RET-SHOE STORES
4483   .RET-JEWELRY, LUGGAGE,AND LEATHER GOODS STORES
448ZM  .RET-CLOTHING AND ACCESSORIES, EXCEPT SHOE, STORES
45113  .RET-SEWING, NEEDLEWORK AND PIECE GOODS STORES
45121  .RET-BOOK STORES AND NEWS DEALERS
451M   .RET-MUSIC STORES
45211  .RET-DEPARTMENT AND DISCOUNT STORES
4529   .RET-MISCELLANEOUS GENERAL MERCHANDISE STORES
4531   .RET-FLORISTS
45321  .RET-OFFICE SUPPLIES AND STATIONARY STORES
45322  .RET-GIFT, NOVELTY, AND SOUVENIR SHOPS
4533   .RET-USED MERCHANDISE STORES
4539   .RET-MISCELLANEOUS STORES
454111 .RET-ELECTRONIC SHOPPING
454112 .RET-ELECTRONIC AUCTIONS
454113 .RET-MAIL-ORDER HOUSES
4542   .RET-VENDING MACHINE OPERATORS
45431  .RET-FUEL DEALERS
45439  .RET-OTHER DIRECT SELLING ESTABLISHMENTS
4M     .RET-SPORTING GOODS, CAMERA, AND HOBBY AND TOY STORES
4MS    .RET-NOT SPECIFIED TRADE
481    .TRN-AIR TRANSPORTATION
482    .TRN-RAIL TRANSPORTATION
483    .TRN-WATER TRANSPORTATION
484    .TRN-TRUCK TRANSPORTATION
4853   .TRN-TAXI AND LIMOUSINE SERVICE
485M   .TRN-BUS SERVICE AND URBAN TRANSIT
486    .TRN-PIPELINE TRANSPORTATION
487    .TRN-SCENIC AND SIGHTSEEING TRANSPORTATION
488    .TRN-SERVICES INCIDENTAL TO TRANSPORTATION
491    .TRN-POSTAL SERVICE
492    .TRN-COURIERS AND MESSENGERS
493    .TRN-WAREHOUSING AND STORAGE
 
 51111  .INF-NEWSPAPER PUBLISHERS
5111Z  .INF-PUBLISHING, EXCEPT NEWSPAPERS AND SOFTWARE
5112   .INF-SOFTWARE PUBLISHING
5121   .INF-MOTION PICTURES AND VIDEO INDUSTRIES
5122   .INF-SOUND RECORDING INDUSTRIES
5161   .INF-INTERNET PUBLISHING AND BROADCASTING
5171   .INF-WIRED TELECOMMUNICATIONS CARRIERS
517Z   .INF-OTHER TELECOMMUNICATION SERVICES
5181   .INF-INTERNET SERVICE PROVIDERS
5182   .INF-DATA PROCESSING, HOSTING, AND RELATED SERVICES
51912  .INF-LIBRARIES AND ARCHIVES
5191Z  .INF-OTHER INFORMATION SERVICES
51M    .INF-RADIO AND TELEVISION BROADCASTING AND CABLE
5221M  .FIN-SAVINGS INSTITUTIONS, INCLUDING CREDIT UNIONS
522M   .FIN-NON-DEPOSITORY CREDIT AND RELATED ACTIVITIES
524    .FIN-INSURANCE CARRIERS AND RELATED ACTIVITIES
52M1   .FIN-BANKING AND RELATED ACTIVITIES
52M2   .FIN-SECURITIES, COMMODITIES, FUNDS, TRUSTS, AND OTHER
       .FINANCIAL INVESTMENTS
531    .FIN-REAL ESTATE
5321   .FIN-AUTOMOTIVE EQUIPMENT RENTAL AND LEASING
53223  .FIN-VIDEO TAPE AND DISK RENTAL
532M   .FIN-OTHER CONSUMER GOODS RENTAL
53M    .FIN-COMMERCIAL, INDUSTRIAL, AND OTHER INTANGIBLE ASSETS
       .RENTAL AND LEASING
5411   .PRF-LEGAL SERVICES
5412   .PRF-ACCOUNTING, TAX PREPARATION, BOOKKEEPING AND PAYROLL
       .SERVICES
5413   .PRF-ARCHITECTURAL, ENGINEERING, AND RELATED SERVICES
5414   .PRF-SPECIALIZED DESIGN SERVICES
5415   .PRF-COMPUTER SYSTEMS DESIGN AND RELATED SERVICES
5416   .PRF-MANAGEMENT, SCIENTIFIC AND TECHNICAL CONSULTING
       .SERVICES
5417   .PRF-SCIENTIFIC RESEARCH AND DEVELOPMENT SERVICES
5418   .PRF-ADVERTISING AND RELATED SERVICES
54194  .PRF-VETERINARY SERVICES
5419Z  .PRF-OTHER PROFESSIONAL, SCIENTIFIC AND TECHNICAL SERVICES
55     .PRF-MANAGEMENT OF COMPANIES AND ENTERPRISES
5613   .PRF-EMPLOYMENT SERVICES
5614   .PRF-BUSINESS SUPPORT SERVICES
5615   .PRF-TRAVEL ARRANGEMENTS AND RESERVATION SERVICES
5616   .PRF-INVESTIGATION AND SECURITY SERVICES
56173  .PRF-LANDSCAPING SERVICES
5617Z  .PRF-SERVICES TO BUILDINGS AND DWELLINGS, EX CONSTR CLN
561M   .PRF-OTHER ADMINISTRATIVE, AND OTHER SUPPORT SERVICES
562    .PRF-WASTE MANAGEMENT AND REMEDIATION SERVICES
6111   .EDU-ELEMENTARY AND SECONDARY SCHOOLS
611M1  .EDU-COLLEGES AND UNIVERSITIES, INCLUDING JUNIOR COLLEGES
611M2  .EDU-BUSINESS, TECHNICAL, AND TRADE SCHOOLS AND TRAINING
611M3  .EDU-OTHER SCHOOLS, INSTRUCTION, AND EDUCATIONAL SERVICES
6211   .MED-OFFICES OF PHYSICIANS
6212   .MED-OFFICES OF DENTISTS
62131  .MED-OFFICE OF CHIROPRACTORS
62132  .MED-OFFICES OF OPTOMETRISTS
6213ZM .MED-OFFICES OF OTHER HEALTH PRACTITIONERS
6214   .MED-OUTPATIENT CARE CENTERS
6216   .MED-HOME HEALTH CARE SERVICES
621M   .MED-OTHER HEALTH CARE SERVICES
622    .MED-HOSPITALS
6231   .MED-NURSING CARE FACILITIES
623M   .MED-RESIDENTIAL CARE FACILITIES, WITHOUT NURSING
 
            6241      .SCA-INDIVIDUAL AND FAMILY SERVICES
           6242      .SCA-COMMUNITY FOOD AND HOUSING, AND EMERGENCY SERVICES
           6243      .SCA-VOCATIONAL REHABILITATION SERVICES
           6244      .SCA-CHILD DAY CARE SERVICES
           711       .ENT-INDEPENDENT ARTISTS, PERFORMING ARTS, SPECTATOR
                     .SPORTS AND RELATED INDUSTRIES
           712       .ENT-MUSEUMS, ART GALLERIES, HISTORICAL SITES, AND SIMILAR
                     .INSTITUTIONS
           71395     .ENT-BOWLING CENTERS
           713Z      .ENT-OTHER AMUSEMENT, GAMBLING, AND RECREATION INDUSTRIES
           7211      .ENT-TRAVELER ACCOMMODATION
           721M      .ENT-RECREATIONAL VEHICLE PARKS AND CAMPS, AND ROOMING AND
                     .BOARDING HOUSES
           7224      .ENT-DRINKING PLACES, ALCOHOLIC BEVERAGES
           722Z      .ENT-RESTAURANTS AND OTHER FOOD SERVICES
           811192    .SRV-CAR WASHES
           8111Z     .SRV-AUTOMOTIVE REPAIR AND MAINTENANCE
           8112      .SRV-ELECTRONIC AND PRECISION EQUIPMENT REPAIR AND
                     .MAINTENANCE
           8113      .SRV-COMMERCIAL AND INDUSTRIAL MACHINERY AND EQUIPMENT
                     .REPAIR AND MAINTENANCE
           8114      .SRV-PERSONAL AND HOUSEHOLD GOODS REPAIR AND MAINTENANCE
           812111    .SRV-BARBER SHOPS
           812112    .SRV-BEAUTY SALONS
           8121M     .SRV-NAIL SALONS AND OTHER PERSONAL CARE SERVICES
           8122      .SRV-FUNERAL HOMES, CEMETERIES AND CREMATORIES
           8123      .SRV-DRYCLEANING AND LAUNDRY SERVICES
           8129      .SRV-OTHER PERSONAL SERVICES
           8131      .SRV-RELIGIOUS ORGANIZATIONS
           81393     .SRV-LABOR UNIONS
           8139Z     .SRV-BUSINESS, PROFESSIONAL, POLITICAL AND SIMILAR
                     .ORGANIZATIONS
           813M      .SRV-CIVIC, SOCIAL, ADVOCACY ORGANIZATIONS, AND
                     .GRANTMAKING AND GIVING SERVICES
           814       .SRV-PRIVATE HOUSEHOLDS
           92113     .ADM-PUBLIC FINANCE ACTIVITIES
           92119     .ADM-OTHER GENERAL GOVERNMENT AND SUPPORT
           9211MP    .ADM-EXECUTIVE OFFICES AND LEGISLATIVE BODIES
           923       .ADM-ADMINISTRATION OF HUMAN RESOURCE PROGRAMS
           928110P1  .MIL-U.S. ARMY
           928110P2  .MIL-U.S. AIR FORCE
           928110P3  .MIL-U.S. NAVY
           928110P4  .MIL-U.S. MARINES
           928110P5  .MIL-U.S. COAST GUARD
           928110P6  .MIL-U.S. ARMED FORCES, BRANCH NOT SPECIFIED
           928110P7  .MIL-MILITARY RESERVES OR NATIONAL GUARD
           928P      .ADM-NATIONAL SECURITY AND INTERNATIONAL AFFAIRS
           92M1      .ADM-ADMINISTRATION OF ENVIRONMENTAL QUALITY AND HOUSING
                     .PROGRAMS
           92M2      .ADM-ADMINISTRATION OF ECONOMIC PROGRAMS AND SPACE
                     .RESEARCH
           92MP      .ADM-JUSTICE, PUBLIC ORDER, AND SAFETY ACTIVITIES
           9920      .UNEMPLOYED, WITH NO WORK EXPERIENCE IN THE LAST 5 YEARS
                     .**
NATIVITY   1
    Nativity
           1 .Native
           2 .Foreign born
NOP        1
 
      Nativity of parent
            b .N/A (greater than 17 years old/not an own child of householder,
              .and not child in subfamily)
            1 .Living with two parents: Both parents NATIVE
            2 .Living with two parents: Father only FOREIGN BORN
            3 .Living with two parents: Mother only FOREIGN BORN
            4 .Living with two parents: BOTH parents FOREIGN BORN
            5 .Living with father only: Father NATIVE
            6 .Living with father only: Father FOREIGN BORN
            7 .Living with mother only: Mother NATIVE
            8 .Living with mother only: Mother FOREIGN BORN
OC          1
     Own child
            0 .No (includes GQ)
            1 .Yes
OCCP        4
     Occupation recode
            bbbb .N/A (less than 16 years old/unemployed who
                  .never worked/NILF who last worked more than
                  .5 years ago)
            0010 .MGR-CHIEF EXECUTIVES AND LEGISLATORS
            0020 .MGR-GENERAL AND OPERATIONS MANAGERS
            0040 .MGR-ADVERTISING AND PROMOTIONS MANAGERS
            0050 .MGR-MARKETING AND SALES MANAGERS
            0060 .MGR-PUBLIC RELATIONS MANAGERS
            0100 .MGR-ADMINISTRATIVE SERVICES MANAGERS
            0110 .MGR-COMPUTER AND INFORMATION SYSTEMS MANAGERS
            0120 .MGR-FINANCIAL MANAGERS
            0130 .MGR-HUMAN RESOURCES MANAGERS
            0140 .MGR-INDUSTRIAL PRODUCTION MANAGERS
            0150 .MGR-PURCHASING MANAGERS
            0160 .MGR-TRANSPORTATION, STORAGE, AND DISTRIBUTION MANAGERS
            0200 .MGR-FARM, RANCH, AND OTHER AGRICULTURAL MANAGERS
            0210 .MGR-FARMERS AND RANCHERS
            0220 .MGR-CONSTRUCTION MANAGERS
            0230 .MGR-EDUCATION ADMINISTRATORS
            0300 .MGR-ENGINEERING MANAGERS
            0310 .MGR-FOOD SERVICE MANAGERS
            0320 .MGR-FUNERAL DIRECTORS
            0330 .MGR-GAMING MANAGERS
            0340 .MGR-LODGING MANAGERS
            0350 .MGR-MEDICAL AND HEALTH SERVICES MANAGERS
            0360 .MGR-NATURAL SCIENCES MANAGERS
            0410 .MGR-PROPERTY, REAL ESTATE, AND COMMUNITY ASSOCIATION
                  .MANAGERS
            0420 .MGR-SOCIAL AND COMMUNITY SERVICE MANAGERS
            0430 .MGR-MISCELLANEOUS MANAGERS, INCLUDING POSTMASTERS AND MAIL
                  .SUPERINTENDENTS
            0500 .BUS-AGENTS AND BUSINESS MANAGERS OF ARTISTS, PERFORMERS,
                  .AND ATHLETES
            0510 .BUS-PURCHASING AGENTS AND BUYERS, FARM PRODUCTS
            0520 .BUS-WHOLESALE AND RETAIL BUYERS, EXCEPT FARM PRODUCTS
            0530 .BUS-PURCHASING AGENTS, EXCEPT WHOLESALE, RETAIL, AND FARM
                  .PRODUCTS
            0540 .BUS-CLAIMS ADJUSTERS, APPRAISERS, EXAMINERS, AND
                  .INVESTIGATORS
            0560 .BUS-COMPLIANCE OFFICERS, EXCEPT AGRICULTURE, CONSTRUCTION,
                  .HEALTH AND SAFETY, AND TRANSPORTATION
            0600 .BUS-COST ESTIMATORS
 
 0620 .BUS-HUMAN RESOURCES, TRAINING, AND LABOR RELATIONS
     .SPECIALISTS
0700 .BUS-LOGISTICIANS
0710 .BUS-MANAGEMENT ANALYSTS
0720 .BUS-MEETING AND CONVENTION PLANNERS
0730 .BUS-OTHER BUSINESS OPERATIONS SPECIALISTS
0800 .FIN-ACCOUNTANTS AND AUDITORS
0810 .FIN-APPRAISERS AND ASSESSORS OF REAL ESTATE
0820 .FIN-BUDGET ANALYSTS
0830 .FIN-CREDIT ANALYSTS
0840 .FIN-FINANCIAL ANALYSTS
0850 .FIN-PERSONAL FINANCIAL ADVISORS
0860 .FIN-INSURANCE UNDERWRITERS
0900 .FIN-FINANCIAL EXAMINERS
0910 .FIN-LOAN COUNSELORS AND OFFICERS
0930 .FIN-TAX EXAMINERS, COLLECTORS, AND REVENUE AGENTS
0940 .FIN-TAX PREPARERS
0950 .FIN-FINANCIAL SPECIALISTS, ALL OTHER
1000 .CMM-COMPUTER SCIENTISTS AND SYSTEMS ANALYSTS
1010 .CMM-COMPUTER PROGRAMMERS
1020 .CMM-COMPUTER SOFTWARE ENGINEERS
1040 .CMM-COMPUTER SUPPORT SPECIALISTS
1060 .CMM-DATABASE ADMINISTRATORS
1100 .CMM-NETWORK AND COMPUTER SYSTEMS ADMINISTRATORS
1110 .CMM-NETWORK SYSTEMS AND DATA COMMUNICATIONS ANALYSTS
1200 .CMM-ACTUARIES
1220 .CMM-OPERATIONS RESEARCH ANALYSTS
1240 .CMM-MISCELLANEOUS MATHEMATICAL SCIENCE OCCUPATIONS,
     .INCLUDING MATHEMATICIANS AND STATISTICIANS
1300 .ENG-ARCHITECTS, EXCEPT NAVAL
1310 .ENG-SURVEYORS, CARTOGRAPHERS, AND PHOTOGRAMMETRISTS
1320 .ENG-AEROSPACE ENGINEERS
1340 .ENG-BIOMEDICAL AND AGRICULTURAL ENGINEERS
1350 .ENG-CHEMICAL ENGINEERS
1360 .ENG-CIVIL ENGINEERS
1400 .ENG-COMPUTER HARDWARE ENGINEERS
1410 .ENG-ELECTRICAL AND ELECTRONICS ENGINEERS
1420 .ENG-ENVIRONMENTAL ENGINEERS
1430 .ENG-INDUSTRIAL ENGINEERS, INCLUDING HEALTH AND SAFETY
1440 .ENG-MARINE ENGINEERS AND NAVAL ARCHITECTS
1450 .ENG-MATERIALS ENGINEERS
1460 .ENG-MECHANICAL ENGINEERS
1520 .ENG-PETROLEUM, MINING AND GEOLOGICAL ENGINEERS, INCLUDING
     .MINING SAFETY ENGINEERS
1530 .ENG-MISCELLANEOUS ENGINEERS, INCLUDING NUCLEAR ENGINEERS
1540 .ENG-DRAFTERS
1550 .ENG-ENGINEERING TECHNICIANS, EXCEPT DRAFTERS
1560 .ENG-SURVEYING AND MAPPING TECHNICIANS
1600 .SCI-AGRICULTURAL AND FOOD SCIENTISTS
1610 .SCI-BIOLOGICAL SCIENTISTS
1640 .SCI-CONSERVATION SCIENTISTS AND FORESTERS
1650 .SCI-MEDICAL SCIENTISTS
1700 .SCI-ASTRONOMERS AND PHYSICISTS
1710 .SCI-ATMOSPHERIC AND SPACE SCIENTISTS
1720 .SCI-CHEMISTS AND MATERIALS SCIENTISTS
1740 .SCI-ENVIRONMENTAL SCIENTISTS AND GEOSCIENTISTS
1760 .SCI-PHYSICAL SCIENTISTS, ALL OTHER
1800 .SCI-ECONOMISTS
1810 .SCI-MARKET AND SURVEY RESEARCHERS
1820 .SCI-PSYCHOLOGISTS
1840 .SCI-URBAN AND REGIONAL PLANNERS
 
 1860 .SCI-MISCELLANEOUS SOCIAL SCIENTISTS, INCLUDING SOCIOLOGISTS
1900 .SCI-AGRICULTURAL AND FOOD SCIENCE TECHNICIANS
1910 .SCI-BIOLOGICAL TECHNICIANS
1920 .SCI-CHEMICAL TECHNICIANS
1930 .SCI-GEOLOGICAL AND PETROLEUM TECHNICIANS
1960 .SCI-MISCELLANEOUS LIFE, PHYSICAL, AND SOCIAL SCIENCE
     .TECHNICIANS, INCLUDING SOCIAL SCIENCE RESEARCH ASSISTANTS
     .AND NUCLEAR TECHNICIANS
2000 .CMS-COUNSELORS
2010 .CMS-SOCIAL WORKERS
2020 .CMS-MISCELLANEOUS COMMUNITY AND SOCIAL SERVICE SPECIALISTS
2040 .CMS-CLERGY
2050 .CMS-DIRECTORS, RELIGIOUS ACTIVITIES AND EDUCATION
2060 .CMS-RELIGIOUS WORKERS, ALL OTHER
2100 .LGL-LAWYERS AND JUDGES, MAGISTRATES, AND OTHER JUDICIAL
     .WORKERS
2140 .LGL-PARALEGALS AND LEGAL ASSISTANTS
2150 .LGL-MISCELLANEOUS LEGAL SUPPORT WORKERS
2200 .EDU-POSTSECONDARY TEACHERS
2300 .EDU-PRESCHOOL AND KINDERGARTEN TEACHERS
2310 .EDU-ELEMENTARY AND MIDDLE SCHOOL TEACHERS
2320 .EDU-SECONDARY SCHOOL TEACHERS
2330 .EDU-SPECIAL EDUCATION TEACHERS
2340 .EDU-OTHER TEACHERS AND INSTRUCTORS
2400 .EDU-ARCHIVISTS, CURATORS, AND MUSEUM TECHNICIANS
2430 .EDU-LIBRARIANS
2440 .EDU-LIBRARY TECHNICIANS
2540 .EDU-TEACHER ASSISTANTS
2550 .EDU-OTHER EDUCATION, TRAINING, AND LIBRARY WORKERS
2600 .ENT-ARTISTS AND RELATED WORKERS
2630 .ENT-DESIGNERS
2700 .ENT-ACTORS
2710 .ENT-PRODUCERS AND DIRECTORS
2720 .ENT-ATHLETES, COACHES, UMPIRES, AND RELATED WORKERS
2740 .ENT-DANCERS AND CHOREOGRAPHERS
2750 .ENT-MUSICIANS, SINGERS, AND RELATED WORKERS
2760 .ENT-ENTERTAINERS AND PERFORMERS, SPORTS AND RELATED
     .WORKERS, ALL OTHER
2800 .ENT-ANNOUNCERS
2810 .ENT-NEWS ANALYSTS, REPORTERS AND CORRESPONDENTS
2820 .ENT-PUBLIC RELATIONS SPECIALISTS
2830 .ENT-EDITORS
2840 .ENT-TECHNICAL WRITERS
2850 .ENT-WRITERS AND AUTHORS
2860 .ENT-MISCELLANEOUS MEDIA AND COMMUNICATION WORKERS
2900 .ENT-BROADCAST AND SOUND ENGINEERING TECHNICIANS AND RADIO
     .OPERATORS, AND MEDIA AND COMMUNICATION EQUIPMENT WORKERS,
     .ALL OTHER
2910 .ENT-PHOTOGRAPHERS
2920 .ENT-TELEVISION, VIDEO, AND MOTION PICTURE CAMERA OPERATORS
     .AND EDITORS
3000 .MED-CHIROPRACTORS
3010 .MED-DENTISTS
3030 .MED-DIETITIANS AND NUTRITIONISTS
3040 .MED-OPTOMETRISTS
3050 .MED-PHARMACISTS
3060 .MED-PHYSICIANS AND SURGEONS
3110 .MED-PHYSICIAN ASSISTANTS
3120 .MED-PODIATRISTS
3130 .MED-REGISTERED NURSES
3140 .MED-AUDIOLOGISTS
 
 3150 .MED-OCCUPATIONAL THERAPISTS
3160 .MED-PHYSICAL THERAPISTS
3200 .MED-RADIATION THERAPISTS
3210 .MED-RECREATIONAL THERAPISTS
3220 .MED-RESPIRATORY THERAPISTS
3230 .MED-SPEECH-LANGUAGE PATHOLOGISTS
3240 .MED-THERAPISTS, ALL OTHER
3250 .MED-VETERINARIANS
3260 .MED-HEALTH DIAGNOSING AND TREATING PRACTITIONERS, ALL OTHER
3300 .MED-CLINICAL LABORATORY TECHNOLOGISTS AND TECHNICIANS
3310 .MED-DENTAL HYGIENISTS
3320 .MED-DIAGNOSTIC RELATED TECHNOLOGISTS AND TECHNICIANS
3400 .MED-EMERGENCY MEDICAL TECHNICIANS AND PARAMEDICS
3410 .MED-HEALTH DIAGNOSING AND TREATING PRACTITIONER SUPPORT
     .TECHNICIANS
3500 .MED-LICENSED PRACTICAL AND LICENSED VOCATIONAL NURSES
3510 .MED-MEDICAL RECORDS AND HEALTH INFORMATION TECHNICIANS
3520 .MED-OPTICIANS, DISPENSING
3530 .MED-MISCELLANEOUS HEALTH TECHNOLOGISTS AND TECHNICIANS
3540 .MED-OTHER HEALTHCARE PRACTITIONERS AND TECHNICAL
     .OCCUPATIONS
3600 .HLS-NURSING, PSYCHIATRIC, AND HOME HEALTH AIDES
3610 .HLS-OCCUPATIONAL THERAPIST ASSISTANTS AND AIDES
3620 .HLS-PHYSICAL THERAPIST ASSISTANTS AND AIDES
3630 .HLS-MASSAGE THERAPISTS
3640 .HLS-DENTAL ASSISTANTS
3650 .HLS-MEDICAL ASSISTANTS AND OTHER HEALTHCARE SUPPORT
     .OCCUPATIONS, EXCEPT DENTAL ASSISTANTS
3700 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF CORRECTIONAL
     .OFFICERS
3710 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF POLICE AND
     .DETECTIVES
3720 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF FIRE FIGHTING AND
     .PREVENTION WORKERS
3730 .PRT-SUPERVISORS, PROTECTIVE SERVICE WORKERS, ALL OTHER
3740 .PRT-FIRE FIGHTERS
3750 .PRT-FIRE INSPECTORS
3800 .PRT-BAILIFFS, CORRECTIONAL OFFICERS, AND JAILERS
3820 .PRT-DETECTIVES AND CRIMINAL INVESTIGATORS
3840 .PRT-MISCELLANEOUS LAW ENFORCEMENT WORKERS
3850 .PRT-POLICE OFFICERS
3900 .PRT-ANIMAL CONTROL WORKERS
3910 .PRT-PRIVATE DETECTIVES AND INVESTIGATORS
3920 .PRT-SECURITY GUARDS AND GAMING SURVEILLANCE OFFICERS
3940 .PRT-CROSSING GUARDS
3950 .PRT-LIFEGUARDS AND OTHER PROTECTIVE SERVICE WORKERS
4000 .EAT-CHEFS AND HEAD COOKS
4010 .EAT-FIRST-LINE SUPERVISORS/MANAGERS OF FOOD PREPARATION AND
     .SERVING WORKERS
4020 .EAT-COOKS
4030 .EAT-FOOD PREPARATION WORKERS
4040 .EAT-BARTENDERS
4050 .EAT-COMBINED FOOD PREPARATION AND SERVING WORKERS,
     .INCLUDING FAST FOOD
4060 .EAT-COUNTER ATTENDANTS, CAFETERIA, FOOD CONCESSION, AND
     .COFFEE SHOP
4110 .EAT-WAITERS AND WAITRESSES
4120 .EAT-FOOD SERVERS, NONRESTAURANT
4130 .EAT-MISCELLANEOUS FOOD PREPARATION AND SERVING RELATED
     .WORKERS, INCLUDING DINING ROOM AND CAFETERIA ATTENDANTS AND
     .BARTENDER HELPERS
 
 4140 .EAT-DISHWASHERS
4150 .EAT-HOSTS AND HOSTESSES, RESTAURANT, LOUNGE, AND COFFEE
     .SHOP
4200 .CLN-FIRST-LINE SUPERVISORS/MANAGERS OF HOUSEKEEPING AND
     .JANITORIAL WORKERS
4210 .CLN-FIRST-LINE SUPERVISORS/MANAGERS OF LANDSCAPING, LAWN
     .SERVICE, AND GROUNDSKEEPING WORKERS
4220 .CLN-JANITORS AND BUILDING CLEANERS
4230 .CLN-MAIDS AND HOUSEKEEPING CLEANERS
4240 .CLN-PEST CONTROL WORKERS
4250 .CLN-GROUNDS MAINTENANCE WORKERS
4300 .PRS-FIRST-LINE SUPERVISORS/MANAGERS OF GAMING WORKERS
4320 .PRS-FIRST-LINE SUPERVISORS/MANAGERS OF PERSONAL SERVICE
     .WORKERS
4340 .PRS-ANIMAL TRAINERS
4350 .PRS-NONFARM ANIMAL CARETAKERS
4400 .PRS-GAMING SERVICES WORKERS
4410 .PRS-MOTION PICTURE PROJECTIONISTS
4420 .PRS-USHERS, LOBBY ATTENDANTS, AND TICKET TAKERS
4430 .PRS-MISCELLANEOUS ENTERTAINMENT ATTENDANTS AND RELATED
     .WORKERS
4460 .PRS-FUNERAL SERVICE WORKERS
4500 .PRS-BARBERS
4510 .PRS-HAIRDRESSERS, HAIRSTYLISTS, AND COSMETOLOGISTS
4520 .PRS-MISCELLANEOUS PERSONAL APPEARANCE WORKERS
4530 .PRS-BAGGAGE PORTERS, BELLHOPS, AND CONCIERGES
4540 .PRS-TOUR AND TRAVEL GUIDES
4550 .PRS-TRANSPORTATION ATTENDANTS
4600 .PRS-CHILD CARE WORKERS
4610 .PRS-PERSONAL AND HOME CARE AIDES
4620 .PRS-RECREATION AND FITNESS WORKERS
4640 .PRS-RESIDENTIAL ADVISORS
4650 .PRS-PERSONAL CARE AND SERVICE WORKERS, ALL OTHER
4700 .SAL-FIRST-LINE SUPERVISORS/MANAGERS OF RETAIL SALES WORKERS
4710 .SAL-FIRST-LINE SUPERVISORS/MANAGERS OF NON-RETAIL SALES
     .WORKERS
4720 .SAL-CASHIERS
4740 .SAL-COUNTER AND RENTAL CLERKS
4750 .SAL-PARTS SALESPERSONS
4760 .SAL-RETAIL SALESPERSONS
4800 .SAL-ADVERTISING SALES AGENTS
4810 .SAL-INSURANCE SALES AGENTS
4820 .SAL-SECURITIES, COMMODITIES, AND FINANCIAL SERVICES SALES
     .AGENTS
4830 .SAL-TRAVEL AGENTS
4840 .SAL-SALES REPRESENTATIVES, SERVICES, ALL OTHER
4850 .SAL-SALES REPRESENTATIVES, WHOLESALE AND MANUFACTURING
4900 .SAL-MODELS, DEMONSTRATORS, AND PRODUCT PROMOTERS
4920 .SAL-REAL ESTATE BROKERS AND SALES AGENTS
4930 .SAL-SALES ENGINEERS
4940 .SAL-TELEMARKETERS
4950 .SAL-DOOR-TO-DOOR SALES WORKERS, NEWS AND STREET VENDORS,
     .AND RELATED WORKERS
4960 .SAL-SALES AND RELATED WORKERS, ALL OTHER
5000 .OFF-FIRST-LINE SUPERVISORS/MANAGERS OF OFFICE AND
     .ADMINISTRATIVE SUPPORT WORKERS
5010 .OFF-SWITCHBOARD OPERATORS, INCLUDING ANSWERING SERVICE
5020 .OFF-TELEPHONE OPERATORS
5030 .OFF-COMMUNICATIONS EQUIPMENT OPERATORS, ALL OTHER
5100 .OFF-BILL AND ACCOUNT COLLECTORS
5110 .OFF-BILLING AND POSTING CLERKS AND MACHINE OPERATORS
 
 5120 .OFF-BOOKKEEPING, ACCOUNTING, AND AUDITING CLERKS
5130 .OFF-GAMING CAGE WORKERS
5140 .OFF-PAYROLL AND TIMEKEEPING CLERKS
5150 .OFF-PROCUREMENT CLERKS
5160 .OFF-TELLERS
5200 .OFF-BROKERAGE CLERKS
5220 .OFF-COURT, MUNICIPAL, AND LICENSE CLERKS
5230 .OFF-CREDIT AUTHORIZERS, CHECKERS, AND CLERKS
5240 .OFF-CUSTOMER SERVICE REPRESENTATIVES
5250 .OFF-ELIGIBILITY INTERVIEWERS, GOVERNMENT PROGRAMS
5260 .OFF-FILE CLERKS
5300 .OFF-HOTEL, MOTEL, AND RESORT DESK CLERKS
5310 .OFF-INTERVIEWERS, EXCEPT ELIGIBILITY AND LOAN
5320 .OFF-LIBRARY ASSISTANTS, CLERICAL
5330 .OFF-LOAN INTERVIEWERS AND CLERKS
5340 .OFF-NEW ACCOUNTS CLERKS
5350 .OFF-CORRESPONDENCE CLERKS AND ORDER CLERKS
5360 .OFF-HUMAN RESOURCES ASSISTANTS, EXCEPT PAYROLL AND
     .TIMEKEEPING
5400 .OFF-RECEPTIONISTS AND INFORMATION CLERKS
5410 .OFF-RESERVATION AND TRANSPORTATION TICKET AGENTS AND TRAVEL
     .CLERKS
5420 .OFF-INFORMATION AND RECORD CLERKS, ALL OTHER
5500 .OFF-CARGO AND FREIGHT AGENTS
5510 .OFF-COURIERS AND MESSENGERS
5520 .OFF-DISPATCHERS
5530 .OFF-METER READERS, UTILITIES
5540 .OFF-POSTAL SERVICE CLERKS
5550 .OFF-POSTAL SERVICE MAIL CARRIERS
5560 .OFF-POSTAL SERVICE MAIL SORTERS, PROCESSORS, AND PROCESSING
     .MACHINE OPERATORS
5600 .OFF-PRODUCTION, PLANNING, AND EXPEDITING CLERKS
5610 .OFF-SHIPPING, RECEIVING, AND TRAFFIC CLERKS
5620 .OFF-STOCK CLERKS AND ORDER FILLERS
5630 .OFF-WEIGHERS, MEASURERS, CHECKERS, AND SAMPLERS,
     .RECORDKEEPING
5700 .OFF-SECRETARIES AND ADMINISTRATIVE ASSISTANTS
5800 .OFF-COMPUTER OPERATORS
5810 .OFF-DATA ENTRY KEYERS
5820 .OFF-WORD PROCESSORS AND TYPISTS
5840 .OFF-INSURANCE CLAIMS AND POLICY PROCESSING CLERKS
5850 .OFF-MAIL CLERKS AND MAIL MACHINE OPERATORS, EXCEPT POSTAL
     .SERVICE
5860 .OFF-OFFICE CLERKS, GENERAL
5900 .OFF-OFFICE MACHINE OPERATORS, EXCEPT COMPUTER
5910 .OFF-PROOFREADERS AND COPY MARKERS
5920 .OFF-STATISTICAL ASSISTANTS
5930 .OFF-MISCELLANEOUS OFFICE AND ADMINISTRATIVE SUPPORT
     .WORKERS, INCLUDING DESKTOP PUBLISHERS
6000 .FFF-FIRST-LINE SUPERVISORS/MANAGERS OF FARMING, FISHING,
     .AND FORESTRY WORKERS
6010 .FFF-AGRICULTURAL INSPECTORS
6040 .FFF-GRADERS AND SORTERS, AGRICULTURAL PRODUCTS
6050 .FFF-MISCELLANEOUS AGRICULTURAL WORKERS, INCLUDING ANIMAL
     .BREEDERS
6100 .FFF-FISHING AND HUNTING WORKERS
6120 .FFF-FOREST AND CONSERVATION WORKERS
6130 .FFF-LOGGING WORKERS
6200 .CON-FIRST-LINE SUPERVISORS/MANAGERS OF CONSTRUCTION TRADES
     .AND EXTRACTION WORKERS
6210 .CON-BOILERMAKERS
 
 6220 .CON-BRICKMASONS, BLOCKMASONS, AND STONEMASONS
6230 .CON-CARPENTERS
6240 .CON-CARPET, FLOOR, AND TILE INSTALLERS AND FINISHERS
6250 .CON-CEMENT MASONS, CONCRETE FINISHERS, AND TERRAZZO WORKERS
6260 .CON-CONSTRUCTION LABORERS
6300 .CON-PAVING, SURFACING, AND TAMPING EQUIPMENT OPERATORS
6320 .CON-CONSTRUCTION EQUIPMENT OPERATORS, EXCEPT PAVING,
     .SURFACING, AND TAMPING EQUIPMENT OPERATORS
6330 .CON-DRYWALL INSTALLERS, CEILING TILE INSTALLERS, AND TAPERS
6350 .CON-ELECTRICIANS
6360 .CON-GLAZIERS
6400 .CON-INSULATION WORKERS
6420 .CON-PAINTERS, CONSTRUCTION AND MAINTENANCE
6430 .CON-PAPERHANGERS
6440 .CON-PIPELAYERS, PLUMBERS, PIPEFITTERS, AND STEAMFITTERS
6460 .CON-PLASTERERS AND STUCCO MASONS
6500 .CON-REINFORCING IRON AND REBAR WORKERS
6510 .CON-ROOFERS
6520 .CON-SHEET METAL WORKERS
6530 .CON-STRUCTURAL IRON AND STEEL WORKERS
6600 .CON-HELPERS, CONSTRUCTION TRADES
6660 .CON-CONSTRUCTION AND BUILDING INSPECTORS
6700 .CON-ELEVATOR INSTALLERS AND REPAIRERS
6710 .CON-FENCE ERECTORS
6720 .CON-HAZARDOUS MATERIALS REMOVAL WORKERS
6730 .CON-HIGHWAY MAINTENANCE WORKERS
6740 .CON-RAIL-TRACK LAYING AND MAINTENANCE EQUIPMENT OPERATORS
6760 .CON-MISCELLANEOUS CONSTRUCTION WORKERS, INCLUDING SEPTIC
     .TANK SERVICERS AND SEWER PIPE CLEANERS
6800 .EXT-DERRICK, ROTARY DRILL, AND SERVICE UNIT OPERATORS, AND
     .ROUSTABOUTS, OIL, GAS, AND MINING
6820 .EXT-EARTH DRILLERS, EXCEPT OIL AND GAS
6830 .EXT-EXPLOSIVES WORKERS, ORDNANCE HANDLING EXPERTS, AND
     .BLASTERS
6840 .EXT-MINING MACHINE OPERATORS
6940 .EXT-MISCELLANEOUS EXTRACTION WORKERS, INCLUDING ROOF
     .BOLTERS AND HELPERS
7000 .RPR-FIRST-LINE SUPERVISORS/MANAGERS OF MECHANICS,
     .INSTALLERS, AND REPAIRERS
7010 .RPR-COMPUTER, AUTOMATED TELLER, AND OFFICE MACHINE
     .REPAIRERS
7020 .RPR-RADIO AND TELECOMMUNICATIONS EQUIPMENT INSTALLERS AND
     .REPAIRERS
7030 .RPR-AVIONICS TECHNICIANS
7040 .RPR-ELECTRIC MOTOR, POWER TOOL, AND RELATED REPAIRERS
7100 .RPR-ELECTRICAL AND ELECTRONICS REPAIRERS, TRANSPORTATION
     .EQUIPMENT, AND INDUSTRIAL AND UTILITY
7110 .RPR-ELECTRONIC EQUIPMENT INSTALLERS AND REPAIRERS, MOTOR
     .VEHICLES
7120 .RPR-ELECTRONIC HOME ENTERTAINMENT EQUIPMENT INSTALLERS AND
     .REPAIRERS
7130 .RPR-SECURITY AND FIRE ALARM SYSTEMS INSTALLERS
7140 .RPR-AIRCRAFT MECHANICS AND SERVICE TECHNICIANS
7150 .RPR-AUTOMOTIVE BODY AND RELATED REPAIRERS
7160 .RPR-AUTOMOTIVE GLASS INSTALLERS AND REPAIRERS
7200 .RPR-AUTOMOTIVE SERVICE TECHNICIANS AND MECHANICS
7210 .RPR-BUS AND TRUCK MECHANICS AND DIESEL ENGINE SPECIALISTS
7220 .RPR-HEAVY VEHICLE AND MOBILE EQUIPMENT SERVICE TECHNICIANS
     .AND MECHANICS
7240 .RPR-SMALL ENGINE MECHANICS
7260 .RPR-MISCELLANEOUS VEHICLE AND MOBILE EQUIPMENT MECHANICS,
 
      .INSTALLERS, AND REPAIRERS
7300 .RPR-CONTROL AND VALVE INSTALLERS AND REPAIRERS
7310 .RPR-HEATING, AIR CONDITIONING, AND REFRIGERATION MECHANICS
     .AND INSTALLERS
7320 .RPR-HOME APPLIANCE REPAIRERS
7330 .RPR-INDUSTRIAL AND REFRACTORY MACHINERY MECHANICS
7340 .RPR-MAINTENANCE AND REPAIR WORKERS, GENERAL
7350 .RPR-MAINTENANCE WORKERS, MACHINERY
7360 .RPR-MILLWRIGHTS
7410 .RPR-ELECTRICAL POWER-LINE INSTALLERS AND REPAIRERS
7420 .RPR-TELECOMMUNICATIONS LINE INSTALLERS AND REPAIRERS
7430 .RPR-PRECISION INSTRUMENT AND EQUIPMENT REPAIRERS
7510 .RPR-COIN, VENDING, AND AMUSEMENT MACHINE SERVICERS AND
     .REPAIRERS
7540 .RPR-LOCKSMITHS AND SAFE REPAIRERS
7550 .RPR-MANUFACTURED BUILDING AND MOBILE HOME INSTALLERS
7560 .RPR-RIGGERS
7610 .RPR-HELPERS--INSTALLATION, MAINTENANCE, AND REPAIR WORKERS
7620 .RPR-OTHER INSTALLATION, MAINTENANCE, AND REPAIR WORKERS,
     .INCLUDING COMMERCIAL DIVERS, AND SIGNAL AND TRACK SWITCH
     .REPAIRERS
7700 .PRD-FIRST-LINE SUPERVISORS/MANAGERS OF PRODUCTION AND
     .OPERATING WORKERS
7710 .PRD-AIRCRAFT STRUCTURE, SURFACES, RIGGING, AND SYSTEMS
     .ASSEMBLERS
7720 .PRD-ELECTRICAL, ELECTRONICS, AND ELECTROMECHANICAL
     .ASSEMBLERS
7730 .PRD-ENGINE AND OTHER MACHINE ASSEMBLERS
7740 .PRD-STRUCTURAL METAL FABRICATORS AND FITTERS
7750 .PRD-MISCELLANEOUS ASSEMBLERS AND FABRICATORS
7800 .PRD-BAKERS
7810 .PRD-BUTCHERS AND OTHER MEAT, POULTRY, AND FISH PROCESSING
     .WORKERS
7830 .PRD-FOOD AND TOBACCO ROASTING, BAKING, AND DRYING MACHINE
     .OPERATORS AND TENDERS
7840 .PRD-FOOD BATCHMAKERS
7850 .PRD-FOOD COOKING MACHINE OPERATORS AND TENDERS
7900 .PRD-COMPUTER CONTROL PROGRAMMERS AND OPERATORS
7920 .PRD-EXTRUDING AND DRAWING MACHINE SETTERS, OPERATORS, AND
     .TENDERS, METAL AND PLASTIC
7930 .PRD-FORGING MACHINE SETTERS, OPERATORS, AND TENDERS, METAL
     .AND PLASTIC
7940 .PRD-ROLLING MACHINE SETTERS, OPERATORS, AND TENDERS, METAL
     .AND PLASTIC
7950 .PRD-CUTTING, PUNCHING, AND PRESS MACHINE SETTERS,
     .OPERATORS, AND TENDERS, METAL AND PLASTIC
7960 .PRD-DRILLING AND BORING MACHINE TOOL SETTERS, OPERATORS,
     .AND TENDERS, METAL AND PLASTIC
8000 .PRD-GRINDING, LAPPING, POLISHING, AND BUFFING MACHINE TOOL
     .SETTERS, OPERATORS, AND TENDERS, METAL AND PLASTIC
8010 .PRD-LATHE AND TURNING MACHINE TOOL SETTERS, OPERATORS, AND
     .TENDERS, METAL AND PLASTIC
8030 .PRD-MACHINISTS
8040 .PRD-METAL FURNACE AND KILN OPERATORS AND TENDERS
8060 .PRD-MODEL MAKERS AND PATTERNMAKERS, METAL AND PLASTIC
8100 .PRD-MOLDERS AND MOLDING MACHINE SETTERS, OPERATORS, AND
     .TENDERS, METAL AND PLASTIC
8130 .PRD-TOOL AND DIE MAKERS
8140 .PRD-WELDING, SOLDERING, AND BRAZING WORKERS
8150 .PRD-HEAT TREATING EQUIPMENT SETTERS, OPERATORS, AND
     .TENDERS, METAL AND PLASTIC
 
 8200 .PRD-PLATING AND COATING MACHINE SETTERS, OPERATORS, AND
     .TENDERS, METAL AND PLASTIC
8210 .PRD-TOOL GRINDERS, FILERS, AND SHARPENERS
8220 .PRD-MISCELLANEOUS METAL WORKERS AND PLASTIC WORKERS,
     .INCLUDING MILLING AND PLANING MACHINE SETTERS, AND MULTIPLE
     .MACHINE TOOL SETTERS, AND LAY-OUT WORKERS
8230 .PRD-BOOKBINDERS AND BINDERY WORKERS
8240 .PRD-JOB PRINTERS
8250 .PRD-PREPRESS TECHNICIANS AND WORKERS
8260 .PRD-PRINTING MACHINE OPERATORS
8300 .PRD-LAUNDRY AND DRY-CLEANING WORKERS
8310 .PRD-PRESSERS, TEXTILE, GARMENT, AND RELATED MATERIALS
8320 .PRD-SEWING MACHINE OPERATORS
8330 .PRD-SHOE AND LEATHER WORKERS AND REPAIRERS
8340 .PRD-SHOE MACHINE OPERATORS AND TENDERS
8350 .PRD-TAILORS, DRESSMAKERS, AND SEWERS
8400 .PRD-TEXTILE BLEACHING AND DYEING, AND CUTTING MACHINE
     .SETTERS, OPERATORS, AND TENDERS
8410 .PRD-TEXTILE KNITTING AND WEAVING MACHINE SETTERS,
     .OPERATORS, AND TENDERS
8420 .PRD-TEXTILE WINDING, TWISTING, AND DRAWING OUT MACHINE
     .SETTERS, OPERATORS, AND TENDERS
8450 .PRD-UPHOLSTERERS
8460 .PRD-MISCELLANEOUS TEXTILE, APPAREL, AND FURNISHINGS
     .WORKERS, EXCEPT UPHOLSTERERS
8500 .PRD-CABINETMAKERS AND BENCH CARPENTERS
8510 .PRD-FURNITURE FINISHERS
8530 .PRD-SAWING MACHINE SETTERS, OPERATORS, AND TENDERS, WOOD
8540 .PRD-WOODWORKING MACHINE SETTERS, OPERATORS, AND TENDERS,
     .EXCEPT SAWING
8550 .PRD-MISCELLANEOUS WOODWORKERS, INCLUDING MODEL MAKERS AND
     .PATTERNMAKERS
8600 .PRD-POWER PLANT OPERATORS, DISTRIBUTORS, AND DISPATCHERS
8610 .PRD-STATIONARY ENGINEERS AND BOILER OPERATORS
8620 .PRD-WATER AND LIQUID WASTE TREATMENT PLANT AND SYSTEM
     .OPERATORS
8630 .PRD-MISCELLANEOUS PLANT AND SYSTEM OPERATORS
8640 .PRD-CHEMICAL PROCESSING MACHINE SETTERS, OPERATORS, AND
     .TENDERS
8650 .PRD-CRUSHING, GRINDING, POLISHING, MIXING, AND BLENDING
     .WORKERS
8710 .PRD-CUTTING WORKERS
8720 .PRD-EXTRUDING, FORMING, PRESSING, AND COMPACTING MACHINE
     .SETTERS, OPERATORS, AND TENDERS
8730 .PRD-FURNACE, KILN, OVEN, DRIER, AND KETTLE OPERATORS AND
     .TENDERS
8740 .PRD-INSPECTORS, TESTERS, SORTERS, SAMPLERS, AND WEIGHERS
8750 .PRD-JEWELERS AND PRECIOUS STONE AND METAL WORKERS
8760 .PRD-MEDICAL, DENTAL, AND OPHTHALMIC LABORATORY TECHNICIANS
8800 .PRD-PACKAGING AND FILLING MACHINE OPERATORS AND TENDERS
8810 .PRD-PAINTING WORKERS
8830 .PRD-PHOTOGRAPHIC PROCESS WORKERS AND PROCESSING MACHINE
     .OPERATORS
8850 .PRD-CEMENTING AND GLUING MACHINE OPERATORS AND TENDERS
8860 .PRD-CLEANING, WASHING, AND METAL PICKLING EQUIPMENT
     .OPERATORS AND TENDERS
8910 .PRD-ETCHERS AND ENGRAVERS
8920 .PRD-MOLDERS, SHAPERS, AND CASTERS, EXCEPT METAL AND PLASTIC
8930 .PRD-PAPER GOODS MACHINE SETTERS, OPERATORS, AND TENDERS
8940 .PRD-TIRE BUILDERS
8950 .PRD-HELPERS-PRODUCTION WORKERS
 
             8960 .PRD-OTHER PRODUCTION WORKERS, INCLUDING SEMICONDUCTOR
                 .PROCESSORS AND COOLING AND FREEZING EQUIPMENT OPERATORS
            9000 .TRN-SUPERVISORS, TRANSPORTATION AND MATERIAL MOVING WORKERS
            9030 .TRN-AIRCRAFT PILOTS AND FLIGHT ENGINEERS
            9040 .TRN-AIR TRAFFIC CONTROLLERS AND AIRFIELD OPERATIONS
                 .SPECIALISTS
            9110 .TRN-AMBULANCE DRIVERS AND ATTENDANTS, EXCEPT EMERGENCY
                 .MEDICAL TECHNICIANS
            9120 .TRN-BUS DRIVERS
            9130 .TRN-DRIVER/SALES WORKERS AND TRUCK DRIVERS
            9140 .TRN-TAXI DRIVERS AND CHAUFFEURS
            9150 .TRN-MOTOR VEHICLE OPERATORS, ALL OTHER
            9200 .TRN-LOCOMOTIVE ENGINEERS AND OPERATORS
            9230 .TRN-RAILROAD BRAKE, SIGNAL, AND SWITCH OPERATORS
            9240 .TRN-RAILROAD CONDUCTORS AND YARDMASTERS
            9260 .TRN-SUBWAY, STREETCAR, AND OTHER RAIL TRANSPORTATION
                 .WORKERS
            9300 .TRN-SAILORS AND MARINE OILERS, AND SHIP ENGINEERS
            9310 .TRN-SHIP AND BOAT CAPTAINS AND OPERATORS
            9350 .TRN-PARKING LOT ATTENDANTS
            9360 .TRN-SERVICE STATION ATTENDANTS
            9410 .TRN-TRANSPORTATION INSPECTORS
            9420 .TRN-MISCELLANEOUS TRANSPORTATION WORKERS, INCLUDING BRIDGE
                 .AND LOCK TENDERS AND TRAFFIC TECHNICIANS
            9510 .TRN-CRANE AND TOWER OPERATORS
            9520 .TRN-DREDGE, EXCAVATING, AND LOADING MACHINE OPERATORS
            9560 .TRN-CONVEYOR OPERATORS AND TENDERS, AND HOIST AND WINCH
                 .OPERATORS
            9600 .TRN-INDUSTRIAL TRUCK AND TRACTOR OPERATORS
            9610 .TRN-CLEANERS OF VEHICLES AND EQUIPMENT
            9620 .TRN-LABORERS AND FREIGHT, STOCK, AND MATERIAL MOVERS, HAND
            9630 .TRN-MACHINE FEEDERS AND OFFBEARERS
            9640 .TRN-PACKERS AND PACKAGERS, HAND
            9650 .TRN-PUMPING STATION OPERATORS
            9720 .TRN-REFUSE AND RECYCLABLE MATERIAL COLLECTORS
            9750 .TRN-MISCELLANEOUS MATERIAL MOVING WORKERS, INCLUDING
                 .SHUTTLE CAR OPERATORS, AND TANK CAR, TRUCK, AND SHIP
                 .LOADERS
            9800 .MIL-MILITARY OFFICER SPECIAL AND TACTICAL OPERATIONS
                 .LEADERS/MANAGERS
            9810 .MIL-FIRST-LINE ENLISTED MILITARY SUPERVISORS/MANAGERS
            9820 .MIL-MILITARY ENLISTED TACTICAL OPERATIONS AND AIR/WEAPONS
                 .SPECIALISTS AND CREW MEMBERS
            9830 .MIL-MILITARY, RANK NOT SPECIFIED **
            9920 .UNEMPLOYED, WITH NO WORK EXPERIENCE IN THE LAST 5 YEARS **
PAOC        1
     Presence and age of own children
            b .N/A (male/female under 16   years old/GQ)
            1 .With own children under 6   years only
            2 .With own children 6 to 17   years only
            3 .With own children under 6   years and 6 to 17 years
            4 .No own children
PERNP       7
     Total person's earnings
                     bbbbbbb  .N/A (less than 15 years old)
                     0000000  .No earnings
                     -009999  .Loss of $9999 or more
            -000001..-009998  .Loss $1 to $9998
                     0000001  .$1 or breakeven
 
             0000002..9999999 .$2 to $9999999
                              .(Rounded & top-coded components)
PINCP       7
     Total person's income (signed)
                     bbbbbbb .N/A (less than 15 years old)
                     0000000 .None
                     -019998 .Loss of $19998 or more
            -000001..-019997 .Loss $1 to $19997
                     0000001 .$1 or breakeven
            0000002..9999999 .$2 to $9999999
                              .(Rounded & top-coded components)
POBP        3
     Place of birth (Recode)
            001 .Alabama/AL
            002 .Alaska/AK
            004 .Arizona/AZ
            005 .Arkansas/AR
            006 .California/CA
            008 .Colorado/CO
            009 .Connecticut/CT
            010 .Delaware/DE
            011 .District of Columbia/DC
            012 .Florida/FL
            013 .Georgia/GA
            015 .Hawaii/HI
            016 .Idaho/ID
            017 .Illinois/IL
            018 .Indiana/IN
            019 .Iowa/IA
            020 .Kansas/KS
            021 .Kentucky/KY
            022 .Louisiana/LA
            023 .Maine/ME
            024 .Maryland/MD
            025 .Massachusetts/MA
            026 .Michigan/MI
            027 .Minnesota/MN
            028 .Mississippi/MS
            029 .Missouri/MO
            030 .Montana/MT
            031 .Nebraska/NE
            032 .Nevada/NV
            033 .New Hampshire/NH
            034 .New Jersey/NJ
            035 .New Mexico/NM
            036 .New York/NY
            037 .North Carolina/NC
            038 .North Dakota/ND
            039 .Ohio/OH
            040 .Oklahoma/OK
            041 .Oregon/OR
            042 .Pennsylvania/PA
            044 .Rhode Island/RI
            045 .South Carolina/SC
            046 .South Dakota/SD
            047 .Tennessee/TN
            048 .Texas/TX
            049 .Utah/UT
            050 .Vermont/VT
 
 051 .Virginia/VA
053 .Washington/WA
054 .West Virginia/WV
055 .Wisconsin/WI
056 .Wyoming/WY
060 .American Samoa
066 .Guam
072 .Puerto Rico
078 .US Virgin Islands
100 .Albania
102 .Austria
103 .Belgium
104 .Bulgaria
105 .Czechoslovakia
106 .Denmark
108 .Finland
109 .France
110 .Germany
116 .Greece
117 .Hungary
118 .Iceland
119 .Ireland
120 .Italy
126 .Netherlands
127 .Norway
128 .Poland
129 .Portugal
130 .Azores Islands
132 .Romania
134 .Spain
136 .Sweden
137 .Switzerland
138 .United Kingdom, Not Specified
139 .England
140 .Scotland
142 .Northern Ireland
147 .Yugoslavia
148 .Czech Republic
149 .Slovakia
150 .Bosnia and Herzegovina
151 .Croatia
152 .Macedonia
155 .Estonia
156 .Latvia
157 .Lithuania
158 .Armenia
159 .Azerbaijan
160 .Belarus
161 .Georgia
162 .Moldova
163 .Russia
164 .Ukraine
165 .USSR
166 .Europe, Not Specified
169 .Other Europe, Not Specified
200 .Afghanistan
202 .Bangladesh
205 .Myanmar
206 .Cambodia
207 .China
209 .Hong Kong
 
 210 .India
211 .Indonesia
212 .Iran
213 .Iraq
214 .Israel
215 .Japan
216 .Jordan
217 .Korea
218 .Kazakhstan
222 .Kuwait
223 .Laos
224 .Lebanon
226 .Malaysia
229 .Nepal
231 .Pakistan
233 .Philippines
235 .Saudi Arabia
236 .Singapore
238 .Sri Lanka
239 .Syria
240 .Taiwan
242 .Thailand
243 .Turkey
246 .Uzbekistan
247 .Vietnam
248 .Yemen
249 .Asia
251 .Eastern Asia, Not Specified
253 .Other South Central Asia, Not Specified
254 .Other Asia, Not Specified
300 .Bermuda
301 .Canada
303 .Mexico
310 .Belize
311 .Costa Rica
312 .El Salvador
313 .Guatemala
314 .Honduras
315 .Nicaragua
316 .Panama
321 .Antigua & Barbuda
323 .Bahamas
324 .Barbados
327 .Cuba
328 .Dominica
329 .Dominican Republic
330 .Grenada
332 .Haiti
333 .Jamaica
338 .St. Kitts-Nevis
339 .St. Lucia
340 .St. Vincent & the Grenadines
341 .Trinidad & Tobago
343 .West Indies
344 .Caribbean, Not Specified
360 .Argentina
361 .Bolivia
362 .Brazil
363 .Chile
364 .Colombia
365 .Ecuador
 
            368 .Guyana
           369 .Paraguay
           370 .Peru
           372 .Uruguay
           373 .Venezuela
           374 .South America
           399 .Americas, Not Specified
           400 .Algeria
           407 .Cameroon
           408 .Cape Verde
           414 .Egypt
           416 .Ethiopia
           417 .Eritrea
           421 .Ghana
           423 .Guinea
           427 .Kenya
           429 .Liberia
           436 .Morocco
           440 .Nigeria
           444 .Senegal
           447 .Sierra Leone
           448 .Somalia
           449 .South Africa
           451 .Sudan
           453 .Tanzania
           457 .Uganda
           461 .Zimbabwe
           462 .Africa
           463 .Eastern Africa, Not Specified
           464 .Northern Africa, Not Specified
           467 .Western Africa, Not Specified
           468 .Other Africa, Not Specified
           501 .Australia
           508 .Fiji
           512 .Micronesia
           515 .New Zealand
           523 .Tonga
           527 .Samoa
           554 .Other US Island Areas, Oceania, Not Specified, or at Sea
POVPIP     3
    Person poverty status recode
           bbb .N/A
           000..500 .Percent of poverty status value
                501 .501 percent or more of poverty status value
POWPUMA    5
    Place of work PUMA
                  bbbbb .N/A (not a worker--not in the labor force,
                         .force, including persons under 16 years;
                         .unemployed; civilian employed, with a job not
                         .at work; Armed Forces, with a job but not at work)
                  00001 .Did not work in the United States or in Puerto
                        .Rico
           00100..08200 .Assigned Place of work PUMA.   Use with POWSP.
POWSP      3
    Place of work - State or foreign country recode
           bbb .N/A (not a worker--not in the labor force,
 
     .including persons under 16 years; unemployed;
    .employed, with a job not at work; Armed Forces,
    .with a job but not at work)
001 .Alabama/AL
002 .Alaska/AK
004 .Arizona/AZ
005 .Arkansas/AR
006 .California/CA
008 .Colorado/CO
009 .Connecticut/CT
010 .Delaware/DE
011 .District of Columbia/DC
012 .Florida/FL
013 .Georgia/GA
015 .Hawaii/HI
016 .Idaho/ID
017 .Illinois/IL
018 .Indiana/IN
019 .Iowa/IA
020 .Kansas/KS
021 .Kentucky/KY
022 .Louisiana/LA
023 .Maine/ME
024 .Maryland/MD
025 .Massachusetts/MA
026 .Michigan/MI
027 .Minnesota/MN
028 .Mississippi/MS
029 .Missouri/MO
030 .Montana/MT
031 .Nebraska/NE
032 .Nevada/NV
033 .New Hampshire/NH
034 .New Jersey/NJ
035 .New Mexico/NM
036 .New York/NY
037 .North Carolina/NC
038 .North Dakota/ND
039 .Ohio/OH
040 .Oklahoma/OK
041 .Oregon/OR
042 .Pennsylvania/PA
044 .Rhode Island/RI
045 .South Carolina/SC
046 .South Dakota/SD
047 .Tennessee/TN
048 .Texas/TX
049 .Utah/UT
050 .Vermont/VT
051 .Virginia/VA
053 .Washington/WA
054 .West Virginia/WV
055 .Wisconsin/WI
056 .Wyoming/WY
072 .Puerto Rico
166 .Europe
213 .Iraq
251 .Eastern Asia
254 .Other Asia, Not Specified
303 .Mexico
399 .Americas, Not Specified
 
            555 .Other US Island Areas Not Specified, Africa, Oceania, at
               .Sea, or Abroad, Not Specified
QTRBIR     1
    Quarter of birth
           1 .January through March
           2 .April through June
           3 .July through September
           4 .October through December
RAC1P      1
    Recoded detailed race code
           1 .White alone
           2 .Black or African American alone
           3 .American Indian alone
           4 .Alaska Native alone
           5 .American Indian and Alaska Native tribes specified; or American
             .Indian or Alaska native, not specified and no other races
           6 .Asian alone
           7 .Native Hawaiian and Other Pacific Islander alone
           8 .Some other race alone
           9 .Two or more major race groups
RAC2P      2
    Recoded detailed race code
           01 .White alone
           02 .Black or African American alone
           03 .Apache alone
           04 .Blackfeet alone
           05 .Cherokee alone
           06 .Cheyenne alone
           07 .Chickasaw alone
           08 .Chippewa alone
           09 .Choctaw alone
           10 .Colville alone
           11 .Comanche alone
           12 .Creek alone
           13 .Crow alone
           14 .Delaware alone
           15 .Houma alone
           16 .Iroquois alone
           17 .Lumbee alone
           18 .Menominee alone
           19 .Navajo alone
           20 .Paiute alone
           21 .Pima alone
           22 .Potawatomi alone
           23 .Pueblo alone
           24 .Puget Sound Salish alone
           25 .Seminole alone
           26 .Sioux alone
           27 .Tohono O'Odham alone
           28 .Yakama alone
           29 .Yaqui alone
           30 .Yuman alone
           31 .Other specified American Indian tribes alone
           32 .Combinations of American Indian tribes only
           33 .American Indian or Alaska Native, tribe not specified, or
              .American Indian and Alaska Native
           34 .Alaska Athabascan alone
           35 .Aleut alone
 
            36 .Eskimo alone
           37 .Tlingit-Haida alone
           38 .Alaska Native tribes alone or in combination with other
              .Alaska Native tribes
           39 .American Indian and Alaska Native, not specified
           40 .Asian Indian alone
           41 .Bangladeshi alone
           42 .Cambodian alone
           43 .Chinese alone
           44 .Filipino alone
           45 .Hmong alone
           46 .Indonesian alone
           47 .Japanese alone
           48 .Korean alone
           49 .Laotian alone
           50 .Malaysian alone
           51 .Pakistani alone
           52 .Sri Lankan alone
           53 .Thai alone
           54 .Vietnamese alone
           55 .Other specified Asian alone
           56 .Asian, not specified
           57 .Combinations of Asian groups only
           58 .Native Hawaiian alone
           59 .Samoan alone
           60 .Tongan alone
           61 .Other Polynesian alone or in combination with other
              .Polynesian groups
           62 .Guamanian or Chamorro alone
           63 .Other Micronesian alone or in combination with other
              .Micronesian groups
           64 .Melanesian alone or in combination with other Melanesian
              .groups
           65 .Other Native Hawaiian and Other Pacific Islander groups
              .alone or in combination with other Native Hawaiian and Other
              .Pacific Islander groups only
           66 .Some other race alone
           67 .Two or more races
RAC3P      2
    Recoded detailed race code
           01 .Some other race alone
           02 .Other Pacific Islander alone
           03 .Samoan alone
           04 .Guamanian or Chamorro alone
           05 .Native Hawaiian alone
           06 .Native Hawaiian and Other Pacific Islander groups only
           07 .Other Asian; Some other race
           08 .Other Asian alone
           09 .Vietnamese alone
           10 .Korean alone
           11 .Japanese; Some other race
           12 .Japanese; Native Hawaiian
           13 .Japanese; Korean
           14 .Japanese alone
           15 .Filipino; Some other race
           16 .Filipino; Native Hawaiian
           17 .Filipino; Japanese
           18 .Filipino alone
           19 .Chinese; Some other race
           20 .Chinese; Native Hawaiian
 
 21 .Chinese; Other Asian
22 .Chinese; Vietnamese
23 .Chinese; Japanese
24 .Chinese; Filipino; Native Hawaiian
25 .Chinese; Filipino
26 .Chinese alone
27 .Asian Indian; Some other race
28 .Asian Indian; Other Asian
29 .Asian Indian alone
30 .Asian groups and/or Native Hawaiian and Other Pacific
   .Islander groups; Some other race
31 .Asian groups; Native Hawaiian and Other Pacific Islander
   .groups
32 .Asian groups only
33 .American Indian and Alaska Native; Some other race
34 .American Indian and Alaska Native alone
35 .American Indian and Alaska Native race; Asian groups and/or
   .Native Hawaiian and Other Pacific Islander groups and/or
   .Some other race
36 .Black or African American; Some other race
37 .Black or African American; Other Asian
38 .Black or African American; Korean
39 .Black or African American; Japanese
40 .Black or African American; Filipino
41 .Black or African American; Chinese
42 .Black or African American; Asian Indian
43 .Black or African American; American Indian and Alaska Native
44 .Black or African American alone
45 .Black or African American race; Native Hawaiian and Other
   .Pacific Islander groups
46 .Black or African American race; Asian groups and/or Native
   .Hawaiian and Other Pacific Islander groups and/or Some other
   .race
47 .Black or African American race; American Indian and Alaska
   .Native race; Asian groups and/or Native Hawaiian and Other
   .Pacific Islander groups and/or Some other race
48 .White; Some other race
49 .White; Other Pacific Islander
50 .White; Samoan
51 .White; Guamanian or Chamorro
52 .White; Native Hawaiian
53 .White; Other Asian
54 .White; Vietnamese
55 .White; Korean
56 .White; Japanese; Native Hawaiian
57 .White; Japanese
58 .White; Filipino; Native Hawaiian
59 .White; Filipino
60 .White; Chinese; Native Hawaiian
61 .White; Chinese; Filipino; Native Hawaiian
62 .White; Chinese
63 .White; Asian Indian
64 .White; American Indian and Alaska Native; Some other race
65 .White; American Indian and Alaska Native
66 .White; Black or African American; Some other race
67 .White; Black or African American; American Indian and Alaska
   .Native
68 .White; Black or African American
69 .White alone
70 .White race; two or more Asian groups
71 .White race; Black or African American race and/or American
 
               .Indian and Alaska Native race and/or Asian groups and/or
              .Native Hawaiian and Other Pacific Islander groups
           72 .White race; Some other race; Black or African American race
              .and/or American Indian and Alaska Native race and/or Asian
              .groups and/or Native Hawaiian and Other Pacific Islander
              .groups
RACAIAN    1
    American Indian and Alaska Native recode (American Indian and
    Alaska Native alone or in combination with one or more other
    races)
           0 .No
           1 .Yes
RACASN     1
    Asian recode (Asian alone or in combination with one or more
    other races)
           0 .No
           1 .Yes
RACBLK     1
    Black or African American recode (Black alone or in
    combination with one or more other races)
           0 .No
           1 .Yes
RACNHPI    1
    Native Hawaiian and Other Pacific Islander recode (Native
    Hawaiian and Other Pacific Islander alone or in combination
    with one or more other races)
           0 .No
           1 .Yes
RACNUM     1
    Number of major race groups represented
           1..6 .Race groups
RACSOR     1
    Some other race recode (Some other race alone or in
    combination with one or more other races)
           0 .No
           1 .Yes
RACWHT     1
    White recode (White alone or in combination with one or more
    other races)
           0 .No
           1 .Yes
RC         1
    Related child
           0 .No (includes GQ)
           1 .Yes
SFN        1
    Subfamily number
           b .N/A (GQ/not in a subfamily)
           1 .In subfamily 1
           2 .In subfamily 2
           3 .In subfamily 3
           4 .In subfamily 4
 
 SFR         1
     Subfamily relationship
            b .N/A (GQ/not in a subfamily)
            1 .Husband/wife no children
            2 .Husband/wife with children
            3 .Parent in a parent/child subfamily
            4 .Child in a married-couple subfamily
            5 .Child in a mother-child subfamily
            6 .Child in a father-child subfamily
SOCP        6
     SOC Occupation code
            bbbbbb .N/A (less than 16 years old/unemployed
                   .who never worked/NILF who last worked
                   .more than 5 years ago)
            111021 .MGR-GENERAL AND OPERATIONS MANAGERS
            1110XX .MGR-CHIEF EXECUTIVES AND LEGISLATORS *
            112011 .MGR-ADVERTISING AND PROMOTIONS MANAGERS
            112020 .MGR-MARKETING AND SALES MANAGERS
            112031 .MGR-PUBLIC RELATIONS MANAGERS
            113011 .MGR-ADMINISTRATIVE SERVICES MANAGERS
            113021 .MGR-COMPUTER AND INFORMATION SYSTEMS MANAGERS
            113031 .MGR-FINANCIAL MANAGERS
            113040 .MGR-HUMAN RESOURCES MANAGERS
            113051 .MGR-INDUSTRIAL PRODUCTION MANAGERS
            113061 .MGR-PURCHASING MANAGERS
            113071 .MGR-TRANSPORTATION, STORAGE, AND DISTRIBUTION MANAGERS
            119011 .MGR-FARM, RANCH, AND OTHER AGRICULTURAL MANAGERS
            119012 .MGR-FARMERS AND RANCHERS
            119021 .MGR-CONSTRUCTION MANAGERS
            119030 .MGR-EDUCATION ADMINISTRATORS
            119041 .MGR-ENGINEERING MANAGERS
            119051 .MGR-FOOD SERVICE MANAGERS
            119061 .MGR-FUNERAL DIRECTORS
            119071 .MGR-GAMING MANAGERS
            119081 .MGR-LODGING MANAGERS
            119111 .MGR-MEDICAL AND HEALTH SERVICES MANAGERS
            119121 .MGR-NATURAL SCIENCES MANAGERS
            119141 .MGR-PROPERTY, REAL ESTATE, AND COMMUNITY ASSOCIATION
                   .MANAGERS
            119151 .MGR-SOCIAL AND COMMUNITY SERVICE MANAGERS
            1191XX .MGR-MISCELLANEOUS MANAGERS, INCLUDING POSTMASTERS AND MAIL
                   .SUPERINTENDENTS
            131011 .BUS-AGENTS AND BUSINESS MANAGERS OF ARTISTS, PERFORMERS,
                   .AND ATHLETES
            131021 .BUS-PURCHASING AGENTS AND BUYERS, FARM PRODUCTS
            131022 .BUS-WHOLESALE AND RETAIL BUYERS, EXCEPT FARM PRODUCTS
            131023 .BUS-PURCHASING AGENTS, EXCEPT WHOLESALE, RETAIL, AND FARM
                   .PRODUCTS
            131030 .BUS-CLAIMS ADJUSTERS, APPRAISERS, EXAMINERS, AND
                   .INVESTIGATORS
            131041 .BUS-COMPLIANCE OFFICERS, EXCEPT AGRICULTURE, CONSTRUCTION,
                   .HEALTH AND SAFETY, AND TRANSPORTATION
            131051 .BUS-COST ESTIMATORS
            131070 .BUS-HUMAN RESOURCES, TRAINING, AND LABOR RELATIONS
                   .SPECIALISTS
            131081 .BUS-LOGISTICIANS
            131111 .BUS-MANAGEMENT ANALYSTS
            131121 .BUS-MEETING AND CONVENTION PLANNERS
            131XXX .BUS-OTHER BUSINESS OPERATIONS SPECIALISTS *
            132011 .FIN-ACCOUNTANTS AND AUDITORS
 
 132021 .FIN-APPRAISERS AND ASSESSORS OF REAL ESTATE
132031 .FIN-BUDGET ANALYSTS
132041 .FIN-CREDIT ANALYSTS
132051 .FIN-FINANCIAL ANALYSTS
132052 .FIN-PERSONAL FINANCIAL ADVISORS
132053 .FIN-INSURANCE UNDERWRITERS
132061 .FIN-FINANCIAL EXAMINERS
132070 .FIN-LOAN COUNSELORS AND OFFICERS
132081 .FIN-TAX EXAMINERS, COLLECTORS, AND REVENUE AGENTS
132082 .FIN-TAX PREPARERS
132099 .FIN-FINANCIAL SPECIALISTS, ALL OTHER
151021 .CMM-COMPUTER PROGRAMMERS
151030 .CMM-COMPUTER SOFTWARE ENGINEERS
151041 .CMM-COMPUTER SUPPORT SPECIALISTS
151061 .CMM-DATABASE ADMINISTRATORS
151071 .CMM-NETWORK AND COMPUTER SYSTEMS ADMINISTRATORS
151081 .CMM-NETWORK SYSTEMS AND DATA COMMUNICATIONS ANALYSTS
1510XX .CMM-COMPUTER SCIENTISTS AND SYSTEMS ANALYSTS *
152011 .CMM-ACTUARIES
152031 .CMM-OPERATIONS RESEARCH ANALYSTS
1520XX .CMM-MISCELLANEOUS MATHEMATICAL SCIENCE OCCUPATIONS,
       .INCLUDING MATHEMATICIANS AND STATISTICIANS *
171010 .ENG-ARCHITECTS, EXCEPT NAVAL
171020 .ENG-SURVEYORS, CARTOGRAPHERS, AND PHOTOGRAMMETRISTS
172011 .ENG-AEROSPACE ENGINEERS
172041 .ENG-CHEMICAL ENGINEERS
172051 .ENG-CIVIL ENGINEERS
172061 .ENG-COMPUTER HARDWARE ENGINEERS
172070 .ENG-ELECTRICAL AND ELECTRONICS ENGINEERS
172081 .ENG-ENVIRONMENTAL ENGINEERS
1720XX .ENG-BIOMEDICAL AND AGRICULTURAL ENGINEERS *
172110 .ENG-INDUSTRIAL ENGINEERS, INCLUDING HEALTH AND SAFETY
172121 .ENG-MARINE ENGINEERS AND NAVAL ARCHITECTS
172131 .ENG-MATERIALS ENGINEERS
172141 .ENG-MECHANICAL ENGINEERS
1721XX .ENG-PETROLEUM, MINING AND GEOLOGICAL ENGINEERS, INCLUDING
       .MINING SAFETY ENGINEERS *
1721YY .ENG-MISCELLANEOUS ENGINEERS, INCLUDING NUCLEAR ENGINEERS *
173010 .ENG-DRAFTERS
173020 .ENG-ENGINEERING TECHNICIANS, EXCEPT DRAFTERS
173031 .ENG-SURVEYING AND MAPPING TECHNICIANS
191010 .SCI-AGRICULTURAL AND FOOD SCIENTISTS
191020 .SCI-BIOLOGICAL SCIENTISTS
191030 .SCI-CONSERVATION SCIENTISTS AND FORESTERS
191040 .SCI-MEDICAL SCIENTISTS
192010 .SCI-ASTRONOMERS AND PHYSICISTS
192021 .SCI-ATMOSPHERIC AND SPACE SCIENTISTS
192030 .SCI-CHEMISTS AND MATERIALS SCIENTISTS
192040 .SCI-ENVIRONMENTAL SCIENTISTS AND GEOSCIENTISTS
192099 .SCI-PHYSICAL SCIENTISTS, ALL OTHER
193011 .SCI-ECONOMISTS
193020 .SCI-MARKET AND SURVEY RESEARCHERS
193030 .SCI-PSYCHOLOGISTS
193051 .SCI-URBAN AND REGIONAL PLANNERS
1930XX .SCI-MISCELLANEOUS SOCIAL SCIENTISTS, INCLUDING SOCIOLOGISTS
       .*
194011 .SCI-AGRICULTURAL AND FOOD SCIENCE TECHNICIANS
194021 .SCI-BIOLOGICAL TECHNICIANS
194031 .SCI-CHEMICAL TECHNICIANS
194041 .SCI-GEOLOGICAL AND PETROLEUM TECHNICIANS
1940XX .SCI-MISCELLANEOUS LIFE, PHYSICAL, AND SOCIAL SCIENCE
 
        .TECHNICIANS, INCLUDING SOCIAL SCIENCE RESEARCH ASSISTANTS
       .AND NUCLEAR TECHNICIANS *
211010 .CMS-COUNSELORS
211020 .CMS-SOCIAL WORKERS
211090 .CMS-MISCELLANEOUS COMMUNITY AND SOCIAL SERVICE SPECIALISTS
212011 .CMS-CLERGY
212021 .CMS-DIRECTORS, RELIGIOUS ACTIVITIES AND EDUCATION
212099 .CMS-RELIGIOUS WORKERS, ALL OTHER
2310XX .LGL-LAWYERS AND JUDGES, MAGISTRATES, AND OTHER JUDICIAL
       .WORKERS
232011 .LGL-PARALEGALS AND LEGAL ASSISTANTS
232090 .LGL-MISCELLANEOUS LEGAL SUPPORT WORKERS
251000 .EDU-POSTSECONDARY TEACHERS
252010 .EDU-PRESCHOOL AND KINDERGARTEN TEACHERS
252020 .EDU-ELEMENTARY AND MIDDLE SCHOOL TEACHERS
252030 .EDU-SECONDARY SCHOOL TEACHERS
252040 .EDU-SPECIAL EDUCATION TEACHERS
253000 .EDU-OTHER TEACHERS AND INSTRUCTORS
254010 .EDU-ARCHIVISTS, CURATORS, AND MUSEUM TECHNICIANS
254021 .EDU-LIBRARIANS
254031 .EDU-LIBRARY TECHNICIANS
259041 .EDU-TEACHER ASSISTANTS
2590XX .EDU-OTHER EDUCATION, TRAINING, AND LIBRARY WORKERS *
271010 .ENT-ARTISTS AND RELATED WORKERS
271020 .ENT-DESIGNERS
272011 .ENT-ACTORS
272012 .ENT-PRODUCERS AND DIRECTORS
272020 .ENT-ATHLETES, COACHES, UMPIRES, AND RELATED WORKERS
272030 .ENT-DANCERS AND CHOREOGRAPHERS
272040 .ENT-MUSICIANS, SINGERS, AND RELATED WORKERS
272099 .ENT-ENTERTAINERS AND PERFORMERS, SPORTS AND RELATED
       .WORKERS, ALL OTHER
273010 .ENT-ANNOUNCERS
273020 .ENT-NEWS ANALYSTS, REPORTERS AND CORRESPONDENTS
273031 .ENT-PUBLIC RELATIONS SPECIALISTS
273041 .ENT-EDITORS
273042 .ENT-TECHNICAL WRITERS
273043 .ENT-WRITERS AND AUTHORS
273090 .ENT-MISCELLANEOUS MEDIA AND COMMUNICATION WORKERS
274021 .ENT-PHOTOGRAPHERS
274030 .ENT-TELEVISION, VIDEO, AND MOTION PICTURE CAMERA OPERATORS
       .AND EDITORS
2740XX .ENT-BROADCAST AND SOUND ENGINEERING TECHNICIANS AND RADIO
       .OPERATORS, AND MEDIA AND COMMUNICATION EQUIPMENT WORKERS,
       .ALL OTHER *
291011 .MED-CHIROPRACTORS
291020 .MED-DENTISTS
291031 .MED-DIETITIANS AND NUTRITIONISTS
291041 .MED-OPTOMETRISTS
291051 .MED-PHARMACISTS
291060 .MED-PHYSICIANS AND SURGEONS
291071 .MED-PHYSICIAN ASSISTANTS
291081 .MED-PODIATRISTS
291111 .MED-REGISTERED NURSES
291121 .MED-AUDIOLOGISTS
291122 .MED-OCCUPATIONAL THERAPISTS
291123 .MED-PHYSICAL THERAPISTS
291124 .MED-RADIATION THERAPISTS
291125 .MED-RECREATIONAL THERAPISTS
291126 .MED-RESPIRATORY THERAPISTS
291127 .MED-SPEECH-LANGUAGE PATHOLOGISTS
 
 291129 .MED-THERAPISTS, ALL OTHER
291131 .MED-VETERINARIANS
291199 .MED-HEALTH DIAGNOSING AND TREATING PRACTITIONERS, ALL OTHER
292010 .MED-CLINICAL LABORATORY TECHNOLOGISTS AND TECHNICIANS
292021 .MED-DENTAL HYGIENISTS
292030 .MED-DIAGNOSTIC RELATED TECHNOLOGISTS AND TECHNICIANS
292041 .MED-EMERGENCY MEDICAL TECHNICIANS AND PARAMEDICS
292050 .MED-HEALTH DIAGNOSING AND TREATING PRACTITIONER SUPPORT
       .TECHNICIANS
292061 .MED-LICENSED PRACTICAL AND LICENSED VOCATIONAL NURSES
292071 .MED-MEDICAL RECORDS AND HEALTH INFORMATION TECHNICIANS
292081 .MED-OPTICIANS, DISPENSING
292090 .MED-MISCELLANEOUS HEALTH TECHNOLOGISTS AND TECHNICIANS
299000 .MED-OTHER HEALTHCARE PRACTITIONERS AND TECHNICAL
       .OCCUPATIONS
311010 .HLS-NURSING, PSYCHIATRIC, AND HOME HEALTH AIDES
312010 .HLS-OCCUPATIONAL THERAPIST ASSISTANTS AND AIDES
312020 .HLS-PHYSICAL THERAPIST ASSISTANTS AND AIDES
319011 .HLS-MASSAGE THERAPISTS
319091 .HLS-DENTAL ASSISTANTS
31909X .HLS-MEDICAL ASSISTANTS AND OTHER HEALTHCARE SUPPORT
       .OCCUPATIONS, EXCEPT DENTAL ASSISTANTS *
331011 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF CORRECTIONAL
       .OFFICERS
331012 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF POLICE AND
       .DETECTIVES
331021 .PRT-FIRST-LINE SUPERVISORS/MANAGERS OF FIRE FIGHTING AND
       .PREVENTION WORKERS
331099 .PRT-SUPERVISORS, PROTECTIVE SERVICE WORKERS, ALL OTHER
332011 .PRT-FIRE FIGHTERS
332020 .PRT-FIRE INSPECTORS
333010 .PRT-BAILIFFS, CORRECTIONAL OFFICERS, AND JAILERS
333021 .PRT-DETECTIVES AND CRIMINAL INVESTIGATORS
333050 .PRT-POLICE OFFICERS
3330XX .PRT-MISCELLANEOUS LAW ENFORCEMENT WORKERS *
339011 .PRT-ANIMAL CONTROL WORKERS
339021 .PRT-PRIVATE DETECTIVES AND INVESTIGATORS
339030 .PRT-SECURITY GUARDS AND GAMING SURVEILLANCE OFFICERS
339091 .PRT-CROSSING GUARDS
33909X .PRT-LIFEGUARDS AND OTHER PROTECTIVE SERVICE WORKERS *
351011 .EAT-CHEFS AND HEAD COOKS
351012 .EAT-FIRST-LINE SUPERVISORS/MANAGERS OF FOOD PREPARATION AND
       .SERVING WORKERS
352010 .EAT-COOKS
352021 .EAT-FOOD PREPARATION WORKERS
353011 .EAT-BARTENDERS
353021 .EAT-COMBINED FOOD PREPARATION AND SERVING WORKERS,
       .INCLUDING FAST FOOD
353022 .EAT-COUNTER ATTENDANTS, CAFETERIA, FOOD CONCESSION, AND
       .COFFEE SHOP
353031 .EAT-WAITERS AND WAITRESSES
353041 .EAT-FOOD SERVERS, NONRESTAURANT
359021 .EAT-DISHWASHERS
359031 .EAT-HOSTS AND HOSTESSES, RESTAURANT, LOUNGE, AND COFFEE
       .SHOP
3590XX .EAT-MISCELLANEOUS FOOD PREPARATION AND SERVING RELATED
       .WORKERS, INCLUDING DINING ROOM AND CAFETERIA ATTENDANTS AND
       .BARTENDER HELPERS *
371011 .CLN-FIRST-LINE SUPERVISORS/MANAGERS OF HOUSEKEEPING AND
       .JANITORIAL WORKERS
371012 .CLN-FIRST-LINE SUPERVISORS/MANAGERS OF LANDSCAPING, LAWN
 
        .SERVICE, AND GROUNDSKEEPING WORKERS
372012 .CLN-MAIDS AND HOUSEKEEPING CLEANERS
37201X .CLN-JANITORS AND BUILDING CLEANERS *
372021 .CLN-PEST CONTROL WORKERS
373010 .CLN-GROUNDS MAINTENANCE WORKERS
391010 .PRS-FIRST-LINE SUPERVISORS/MANAGERS OF GAMING WORKERS
391021 .PRS-FIRST-LINE SUPERVISORS/MANAGERS OF PERSONAL SERVICE
       .WORKERS
392011 .PRS-ANIMAL TRAINERS
392021 .PRS-NONFARM ANIMAL CARETAKERS
393010 .PRS-GAMING SERVICES WORKERS
393021 .PRS-MOTION PICTURE PROJECTIONISTS
393031 .PRS-USHERS, LOBBY ATTENDANTS, AND TICKET TAKERS
393090 .PRS-MISCELLANEOUS ENTERTAINMENT ATTENDANTS AND RELATED
       .WORKERS
394000 .PRS-FUNERAL SERVICE WORKERS
395011 .PRS-BARBERS
395012 .PRS-HAIRDRESSERS, HAIRSTYLISTS, AND COSMETOLOGISTS
395090 .PRS-MISCELLANEOUS PERSONAL APPEARANCE WORKERS
396010 .PRS-BAGGAGE PORTERS, BELLHOPS, AND CONCIERGES
396020 .PRS-TOUR AND TRAVEL GUIDES
396030 .PRS-TRANSPORTATION ATTENDANTS
399011 .PRS-CHILD CARE WORKERS
399021 .PRS-PERSONAL AND HOME CARE AIDES
399030 .PRS-RECREATION AND FITNESS WORKERS
399041 .PRS-RESIDENTIAL ADVISORS
399099 .PRS-PERSONAL CARE AND SERVICE WORKERS, ALL OTHER
411011 .SAL-FIRST-LINE SUPERVISORS/MANAGERS OF RETAIL SALES WORKERS
411012 .SAL-FIRST-LINE SUPERVISORS/MANAGERS OF NON-RETAIL SALES
       .WORKERS
412010 .SAL-CASHIERS
412021 .SAL-COUNTER AND RENTAL CLERKS
412022 .SAL-PARTS SALESPERSONS
412031 .SAL-RETAIL SALESPERSONS
413011 .SAL-ADVERTISING SALES AGENTS
413021 .SAL-INSURANCE SALES AGENTS
413031 .SAL-SECURITIES, COMMODITIES, AND FINANCIAL SERVICES SALES
       .AGENTS
413041 .SAL-TRAVEL AGENTS
413099 .SAL-SALES REPRESENTATIVES, SERVICES, ALL OTHER
414010 .SAL-SALES REPRESENTATIVES, WHOLESALE AND MANUFACTURING
419010 .SAL-MODELS, DEMONSTRATORS, AND PRODUCT PROMOTERS
419020 .SAL-REAL ESTATE BROKERS AND SALES AGENTS
419031 .SAL-SALES ENGINEERS
419041 .SAL-TELEMARKETERS
419091 .SAL-DOOR-TO-DOOR SALES WORKERS, NEWS AND STREET VENDORS,
       .AND RELATED WORKERS
419099 .SAL-SALES AND RELATED WORKERS, ALL OTHER
431011 .OFF-FIRST-LINE SUPERVISORS/MANAGERS OF OFFICE AND
       .ADMINISTRATIVE SUPPORT WORKERS
432011 .OFF-SWITCHBOARD OPERATORS, INCLUDING ANSWERING SERVICE
432021 .OFF-TELEPHONE OPERATORS
432099 .OFF-COMMUNICATIONS EQUIPMENT OPERATORS, ALL OTHER
433011 .OFF-BILL AND ACCOUNT COLLECTORS
433021 .OFF-BILLING AND POSTING CLERKS AND MACHINE OPERATORS
433031 .OFF-BOOKKEEPING, ACCOUNTING, AND AUDITING CLERKS
433041 .OFF-GAMING CAGE WORKERS
433051 .OFF-PAYROLL AND TIMEKEEPING CLERKS
433061 .OFF-PROCUREMENT CLERKS
433071 .OFF-TELLERS
434011 .OFF-BROKERAGE CLERKS
 
 434031 .OFF-COURT, MUNICIPAL, AND LICENSE CLERKS
434041 .OFF-CREDIT AUTHORIZERS, CHECKERS, AND CLERKS
434051 .OFF-CUSTOMER SERVICE REPRESENTATIVES
434061 .OFF-ELIGIBILITY INTERVIEWERS, GOVERNMENT PROGRAMS
434071 .OFF-FILE CLERKS
434081 .OFF-HOTEL, MOTEL, AND RESORT DESK CLERKS
434111 .OFF-INTERVIEWERS, EXCEPT ELIGIBILITY AND LOAN
434121 .OFF-LIBRARY ASSISTANTS, CLERICAL
434131 .OFF-LOAN INTERVIEWERS AND CLERKS
434141 .OFF-NEW ACCOUNTS CLERKS
434161 .OFF-HUMAN RESOURCES ASSISTANTS, EXCEPT PAYROLL AND
       .TIMEKEEPING
434171 .OFF-RECEPTIONISTS AND INFORMATION CLERKS
434181 .OFF-RESERVATION AND TRANSPORTATION TICKET AGENTS AND TRAVEL
       .CLERKS
434199 .OFF-INFORMATION AND RECORD CLERKS, ALL OTHER
434XXX .OFF-CORRESPONDENCE CLERKS AND ORDER CLERKS *
435011 .OFF-CARGO AND FREIGHT AGENTS
435021 .OFF-COURIERS AND MESSENGERS
435030 .OFF-DISPATCHERS
435041 .OFF-METER READERS, UTILITIES
435051 .OFF-POSTAL SERVICE CLERKS
435052 .OFF-POSTAL SERVICE MAIL CARRIERS
435053 .OFF-POSTAL SERVICE MAIL SORTERS, PROCESSORS, AND PROCESSING
       .MACHINE OPERATORS
435061 .OFF-PRODUCTION, PLANNING, AND EXPEDITING CLERKS
435071 .OFF-SHIPPING, RECEIVING, AND TRAFFIC CLERKS
435081 .OFF-STOCK CLERKS AND ORDER FILLERS
435111 .OFF-WEIGHERS, MEASURERS, CHECKERS, AND SAMPLERS,
       .RECORDKEEPING
436010 .OFF-SECRETARIES AND ADMINISTRATIVE ASSISTANTS
439011 .OFF-COMPUTER OPERATORS
439021 .OFF-DATA ENTRY KEYERS
439022 .OFF-WORD PROCESSORS AND TYPISTS
439041 .OFF-INSURANCE CLAIMS AND POLICY PROCESSING CLERKS
439051 .OFF-MAIL CLERKS AND MAIL MACHINE OPERATORS, EXCEPT POSTAL
       .SERVICE
439061 .OFF-OFFICE CLERKS, GENERAL
439071 .OFF-OFFICE MACHINE OPERATORS, EXCEPT COMPUTER
439081 .OFF-PROOFREADERS AND COPY MARKERS
439111 .OFF-STATISTICAL ASSISTANTS
439XXX .OFF-MISCELLANEOUS OFFICE AND ADMINISTRATIVE SUPPORT
       .WORKERS, INCLUDING DESKTOP PUBLISHERS *
451010 .FFF-FIRST-LINE SUPERVISORS/MANAGERS OF FARMING, FISHING,
       .AND FORESTRY WORKERS
452011 .FFF-AGRICULTURAL INSPECTORS
452041 .FFF-GRADERS AND SORTERS, AGRICULTURAL PRODUCTS
4520XX .FFF-MISCELLANEOUS AGRICULTURAL WORKERS, INCLUDING ANIMAL
       .BREEDERS *
453000 .FFF-FISHING AND HUNTING WORKERS
454011 .FFF-FOREST AND CONSERVATION WORKERS
454020 .FFF-LOGGING WORKERS
471011 .CON-FIRST-LINE SUPERVISORS/MANAGERS OF CONSTRUCTION TRADES
       .AND EXTRACTION WORKERS
472011 .CON-BOILERMAKERS
472020 .CON-BRICKMASONS, BLOCKMASONS, AND STONEMASONS
472031 .CON-CARPENTERS
472040 .CON-CARPET, FLOOR, AND TILE INSTALLERS AND FINISHERS
472050 .CON-CEMENT MASONS, CONCRETE FINISHERS, AND TERRAZZO WORKERS
472061 .CON-CONSTRUCTION LABORERS
472071 .CON-PAVING, SURFACING, AND TAMPING EQUIPMENT OPERATORS
 
 47207X .CON-CONSTRUCTION EQUIPMENT OPERATORS, EXCEPT PAVING,
       .SURFACING, AND TAMPING EQUIPMENT OPERATORS *
472080 .CON-DRYWALL INSTALLERS, CEILING TILE INSTALLERS, AND TAPERS
472111 .CON-ELECTRICIANS
472121 .CON-GLAZIERS
472130 .CON-INSULATION WORKERS
472141 .CON-PAINTERS, CONSTRUCTION AND MAINTENANCE
472142 .CON-PAPERHANGERS
472150 .CON-PIPELAYERS, PLUMBERS, PIPEFITTERS, AND STEAMFITTERS
472161 .CON-PLASTERERS AND STUCCO MASONS
472171 .CON-REINFORCING IRON AND REBAR WORKERS
472181 .CON-ROOFERS
472211 .CON-SHEET METAL WORKERS
472221 .CON-STRUCTURAL IRON AND STEEL WORKERS
473010 .CON-HELPERS, CONSTRUCTION TRADES
474011 .CON-CONSTRUCTION AND BUILDING INSPECTORS
474021 .CON-ELEVATOR INSTALLERS AND REPAIRERS
474031 .CON-FENCE ERECTORS
474041 .CON-HAZARDOUS MATERIALS REMOVAL WORKERS
474051 .CON-HIGHWAY MAINTENANCE WORKERS
474061 .CON-RAIL-TRACK LAYING AND MAINTENANCE EQUIPMENT OPERATORS
4740XX .CON-MISCELLANEOUS CONSTRUCTION WORKERS, INCLUDING SEPTIC
       .TANK SERVICERS AND SEWER PIPE CLEANERS *
475021 .EXT-EARTH DRILLERS, EXCEPT OIL AND GAS
475031 .EXT-EXPLOSIVES WORKERS, ORDNANCE HANDLING EXPERTS, AND
       .BLASTERS
475040 .EXT-MINING MACHINE OPERATORS
4750XX .EXT-MISCELLANEOUS EXTRACTION WORKERS, INCLUDING ROOF
       .BOLTERS AND HELPERS *
4750YY .EXT-DERRICK, ROTARY DRILL, AND SERVICE UNIT OPERATORS, AND
       .ROUSTABOUTS, OIL, GAS, AND MINING *
491011 .RPR-FIRST-LINE SUPERVISORS/MANAGERS OF MECHANICS,
       .INSTALLERS, AND REPAIRERS
492011 .RPR-COMPUTER, AUTOMATED TELLER, AND OFFICE MACHINE
       .REPAIRERS
492020 .RPR-RADIO AND TELECOMMUNICATIONS EQUIPMENT INSTALLERS AND
       .REPAIRERS
492091 .RPR-AVIONICS TECHNICIANS
492092 .RPR-ELECTRIC MOTOR, POWER TOOL, AND RELATED REPAIRERS
492096 .RPR-ELECTRONIC EQUIPMENT INSTALLERS AND REPAIRERS, MOTOR
       .VEHICLES
492097 .RPR-ELECTRONIC HOME ENTERTAINMENT EQUIPMENT INSTALLERS AND
       .REPAIRERS
492098 .RPR-SECURITY AND FIRE ALARM SYSTEMS INSTALLERS
49209X .RPR-ELECTRICAL AND ELECTRONICS REPAIRERS, TRANSPORTATION
       .EQUIPMENT, AND INDUSTRIAL AND UTILITY *
493011 .RPR-AIRCRAFT MECHANICS AND SERVICE TECHNICIANS
493021 .RPR-AUTOMOTIVE BODY AND RELATED REPAIRERS
493022 .RPR-AUTOMOTIVE GLASS INSTALLERS AND REPAIRERS
493023 .RPR-AUTOMOTIVE SERVICE TECHNICIANS AND MECHANICS
493031 .RPR-BUS AND TRUCK MECHANICS AND DIESEL ENGINE SPECIALISTS
493040 .RPR-HEAVY VEHICLE AND MOBILE EQUIPMENT SERVICE TECHNICIANS
       .AND MECHANICS
493050 .RPR-SMALL ENGINE MECHANICS
493090 .RPR-MISCELLANEOUS VEHICLE AND MOBILE EQUIPMENT MECHANICS,
       .INSTALLERS, AND REPAIRERS
499010 .RPR-CONTROL AND VALVE INSTALLERS AND REPAIRERS
499021 .RPR-HEATING, AIR CONDITIONING, AND REFRIGERATION MECHANICS
       .AND INSTALLERS
499031 .RPR-HOME APPLIANCE REPAIRERS
499042 .RPR-MAINTENANCE AND REPAIR WORKERS, GENERAL
 
 499043 .RPR-MAINTENANCE WORKERS, MACHINERY
499044 .RPR-MILLWRIGHTS
49904X .RPR-INDUSTRIAL AND REFRACTORY MACHINERY MECHANICS *
499051 .RPR-ELECTRICAL POWER-LINE INSTALLERS AND REPAIRERS
499052 .RPR-TELECOMMUNICATIONS LINE INSTALLERS AND REPAIRERS
499060 .RPR-PRECISION INSTRUMENT AND EQUIPMENT REPAIRERS
499091 .RPR-COIN, VENDING, AND AMUSEMENT MACHINE SERVICERS AND
       .REPAIRERS
499094 .RPR-LOCKSMITHS AND SAFE REPAIRERS
499095 .RPR-MANUFACTURED BUILDING AND MOBILE HOME INSTALLERS
499096 .RPR-RIGGERS
499098 .RPR-HELPERS--INSTALLATION, MAINTENANCE, AND REPAIR WORKERS
49909X .RPR-OTHER INSTALLATION, MAINTENANCE, AND REPAIR WORKERS,
       .INCLUDING COMMERCIAL DIVERS, AND SIGNAL AND TRACK SWITCH
       .REPAIRERS *
511011 .PRD-FIRST-LINE SUPERVISORS/MANAGERS OF PRODUCTION AND
       .OPERATING WORKERS
512011 .PRD-AIRCRAFT STRUCTURE, SURFACES, RIGGING, AND SYSTEMS
       .ASSEMBLERS
512020 .PRD-ELECTRICAL, ELECTRONICS, AND ELECTROMECHANICAL
       .ASSEMBLERS
512031 .PRD-ENGINE AND OTHER MACHINE ASSEMBLERS
512041 .PRD-STRUCTURAL METAL FABRICATORS AND FITTERS
512090 .PRD-MISCELLANEOUS ASSEMBLERS AND FABRICATORS
513011 .PRD-BAKERS
513020 .PRD-BUTCHERS AND OTHER MEAT, POULTRY, AND FISH PROCESSING
       .WORKERS
513091 .PRD-FOOD AND TOBACCO ROASTING, BAKING, AND DRYING MACHINE
       .OPERATORS AND TENDERS
513092 .PRD-FOOD BATCHMAKERS
513093 .PRD-FOOD COOKING MACHINE OPERATORS AND TENDERS
514010 .PRD-COMPUTER CONTROL PROGRAMMERS AND OPERATORS
514021 .PRD-EXTRUDING AND DRAWING MACHINE SETTERS, OPERATORS, AND
       .TENDERS, METAL AND PLASTIC
514022 .PRD-FORGING MACHINE SETTERS, OPERATORS, AND TENDERS, METAL
       .AND PLASTIC
514023 .PRD-ROLLING MACHINE SETTERS, OPERATORS, AND TENDERS, METAL
       .AND PLASTIC
514031 .PRD-CUTTING, PUNCHING, AND PRESS MACHINE SETTERS,
       .OPERATORS, AND TENDERS, METAL AND PLASTIC
514032 .PRD-DRILLING AND BORING MACHINE TOOL SETTERS, OPERATORS,
       .AND TENDERS, METAL AND PLASTIC
514033 .PRD-GRINDING, LAPPING, POLISHING, AND BUFFING MACHINE TOOL
       .SETTERS, OPERATORS, AND TENDERS, METAL AND PLASTIC
514034 .PRD-LATHE AND TURNING MACHINE TOOL SETTERS, OPERATORS, AND
       .TENDERS, METAL AND PLASTIC
514041 .PRD-MACHINISTS
514050 .PRD-METAL FURNACE AND KILN OPERATORS AND TENDERS
514060 .PRD-MODEL MAKERS AND PATTERNMAKERS, METAL AND PLASTIC
514070 .PRD-MOLDERS AND MOLDING MACHINE SETTERS, OPERATORS, AND
       .TENDERS, METAL AND PLASTIC
514111 .PRD-TOOL AND DIE MAKERS
514120 .PRD-WELDING, SOLDERING, AND BRAZING WORKERS
514191 .PRD-HEAT TREATING EQUIPMENT SETTERS, OPERATORS, AND
       .TENDERS, METAL AND PLASTIC
514193 .PRD-PLATING AND COATING MACHINE SETTERS, OPERATORS, AND
       .TENDERS, METAL AND PLASTIC
514194 .PRD-TOOL GRINDERS, FILERS, AND SHARPENERS
514XXX .PRD-MISCELLANEOUS METAL WORKERS AND PLASTIC WORKERS,
       .INCLUDING MILLING AND PLANING MACHINE SETTERS, AND MULTIPLE
       .MACHINE TOOL SETTERS, AND LAY-OUT WORKERS *
 
 515010 .PRD-BOOKBINDERS AND BINDERY WORKERS
515021 .PRD-JOB PRINTERS
515022 .PRD-PREPRESS TECHNICIANS AND WORKERS
515023 .PRD-PRINTING MACHINE OPERATORS
516011 .PRD-LAUNDRY AND DRY-CLEANING WORKERS
516021 .PRD-PRESSERS, TEXTILE, GARMENT, AND RELATED MATERIALS
516031 .PRD-SEWING MACHINE OPERATORS
516041 .PRD-SHOE AND LEATHER WORKERS AND REPAIRERS
516042 .PRD-SHOE MACHINE OPERATORS AND TENDERS
516050 .PRD-TAILORS, DRESSMAKERS, AND SEWERS
516063 .PRD-TEXTILE KNITTING AND WEAVING MACHINE SETTERS,
       .OPERATORS, AND TENDERS
516064 .PRD-TEXTILE WINDING, TWISTING, AND DRAWING OUT MACHINE
       .SETTERS, OPERATORS, AND TENDERS
51606X .PRD-TEXTILE BLEACHING AND DYEING, AND CUTTING MACHINE
       .SETTERS, OPERATORS, AND TENDERS *
516093 .PRD-UPHOLSTERERS
51609X .PRD-MISCELLANEOUS TEXTILE, APPAREL, AND FURNISHINGS
       .WORKERS, EXCEPT UPHOLSTERERS *
517011 .PRD-CABINETMAKERS AND BENCH CARPENTERS
517021 .PRD-FURNITURE FINISHERS
517041 .PRD-SAWING MACHINE SETTERS, OPERATORS, AND TENDERS, WOOD
517042 .PRD-WOODWORKING MACHINE SETTERS, OPERATORS, AND TENDERS,
       .EXCEPT SAWING
5170XX .PRD-MISCELLANEOUS WOODWORKERS, INCLUDING MODEL MAKERS AND
       .PATTERNMAKERS *
518010 .PRD-POWER PLANT OPERATORS, DISTRIBUTORS, AND DISPATCHERS
518021 .PRD-STATIONARY ENGINEERS AND BOILER OPERATORS
518031 .PRD-WATER AND LIQUID WASTE TREATMENT PLANT AND SYSTEM
       .OPERATORS
518090 .PRD-MISCELLANEOUS PLANT AND SYSTEM OPERATORS
519010 .PRD-CHEMICAL PROCESSING MACHINE SETTERS, OPERATORS, AND
       .TENDERS
519020 .PRD-CRUSHING, GRINDING, POLISHING, MIXING, AND BLENDING
       .WORKERS
519030 .PRD-CUTTING WORKERS
519041 .PRD-EXTRUDING, FORMING, PRESSING, AND COMPACTING MACHINE
       .SETTERS, OPERATORS, AND TENDERS
519051 .PRD-FURNACE, KILN, OVEN, DRIER, AND KETTLE OPERATORS AND
       .TENDERS
519061 .PRD-INSPECTORS, TESTERS, SORTERS, SAMPLERS, AND WEIGHERS
519071 .PRD-JEWELERS AND PRECIOUS STONE AND METAL WORKERS
519080 .PRD-MEDICAL, DENTAL, AND OPHTHALMIC LABORATORY TECHNICIANS
519111 .PRD-PACKAGING AND FILLING MACHINE OPERATORS AND TENDERS
519120 .PRD-PAINTING WORKERS
519130 .PRD-PHOTOGRAPHIC PROCESS WORKERS AND PROCESSING MACHINE
       .OPERATORS
519191 .PRD-CEMENTING AND GLUING MACHINE OPERATORS AND TENDERS
519192 .PRD-CLEANING, WASHING, AND METAL PICKLING EQUIPMENT
       .OPERATORS AND TENDERS
519194 .PRD-ETCHERS AND ENGRAVERS
519195 .PRD-MOLDERS, SHAPERS, AND CASTERS, EXCEPT METAL AND PLASTIC
519196 .PRD-PAPER GOODS MACHINE SETTERS, OPERATORS, AND TENDERS
519197 .PRD-TIRE BUILDERS
519198 .PRD-HELPERS-PRODUCTION WORKERS
5191XX .PRD-OTHER PRODUCTION WORKERS, INCLUDING SEMICONDUCTOR
       .PROCESSORS AND COOLING AND FREEZING EQUIPMENT OPERATORS *
531000 .TRN-SUPERVISORS, TRANSPORTATION AND MATERIAL MOVING WORKERS
532010 .TRN-AIRCRAFT PILOTS AND FLIGHT ENGINEERS
532020 .TRN-AIR TRAFFIC CONTROLLERS AND AIRFIELD OPERATIONS
       .SPECIALISTS
 
             533011 .TRN-AMBULANCE DRIVERS AND ATTENDANTS, EXCEPT EMERGENCY
                   .MEDICAL TECHNICIANS
            533020 .TRN-BUS DRIVERS
            533030 .TRN-DRIVER/SALES WORKERS AND TRUCK DRIVERS
            533041 .TRN-TAXI DRIVERS AND CHAUFFEURS
            533099 .TRN-MOTOR VEHICLE OPERATORS, ALL OTHER
            534010 .TRN-LOCOMOTIVE ENGINEERS AND OPERATORS
            534021 .TRN-RAILROAD BRAKE, SIGNAL, AND SWITCH OPERATORS
            534031 .TRN-RAILROAD CONDUCTORS AND YARDMASTERS
            5340XX .TRN-SUBWAY, STREETCAR, AND OTHER RAIL TRANSPORTATION
                   .WORKERS *
            535020 .TRN-SHIP AND BOAT CAPTAINS AND OPERATORS
            5350XX .TRN-SAILORS AND MARINE OILERS, AND SHIP ENGINEERS *
            536021 .TRN-PARKING LOT ATTENDANTS
            536031 .TRN-SERVICE STATION ATTENDANTS
            536051 .TRN-TRANSPORTATION INSPECTORS
            5360XX .TRN-MISCELLANEOUS TRANSPORTATION WORKERS, INCLUDING BRIDGE
                   .AND LOCK TENDERS AND TRAFFIC TECHNICIANS *
            537021 .TRN-CRANE AND TOWER OPERATORS
            537030 .TRN-DREDGE, EXCAVATING, AND LOADING MACHINE OPERATORS
            537051 .TRN-INDUSTRIAL TRUCK AND TRACTOR OPERATORS
            537061 .TRN-CLEANERS OF VEHICLES AND EQUIPMENT
            537062 .TRN-LABORERS AND FREIGHT, STOCK, AND MATERIAL MOVERS, HAND
            537063 .TRN-MACHINE FEEDERS AND OFFBEARERS
            537064 .TRN-PACKERS AND PACKAGERS, HAND
            537070 .TRN-PUMPING STATION OPERATORS
            537081 .TRN-REFUSE AND RECYCLABLE MATERIAL COLLECTORS
            5370XX .TRN-CONVEYOR OPERATORS AND TENDERS, AND HOIST AND WINCH
                   .OPERATORS *
            5371XX .TRN-MISCELLANEOUS MATERIAL MOVING WORKERS, INCLUDING
                   .SHUTTLE CAR OPERATORS, AND TANK CAR, TRUCK, AND SHIP
                   .LOADERS *
            551010 .MIL-MILITARY OFFICER SPECIAL AND TACTICAL OPERATIONS
                   .LEADERS/MANAGERS
            552010 .MIL-FIRST-LINE ENLISTED MILITARY SUPERVISORS/MANAGERS
            553010 .MIL-MILITARY ENLISTED TACTICAL OPERATIONS AND AIR/WEAPONS
                   .SPECIALISTS AND CREW MEMBERS
            559830 .MIL-MILITARY, RANK NOT SPECIFIED **
            999920 .UNEMPLOYED, WITH NO WORK EXPERIENCE IN THE LAST 5 YEARS **
VPS         2
     Veteran period of service
            bb .N/A (less than 18 years old, no active duty)
               .War Times:
            01 .Gulf War
            02 .Gulf War and Vietnam era
            03 .Vietnam era
            04 .Vietnam era and Korean War
            05 .Vietnam era, Korean War, and WWII
            06 .Korean War
            07 .Korean War and WWII
            08 .WWII
               .Peace Times:
            09 .Post-Vietnam era only
            10 .Between Vietnam and Korean War only
            11 .Between Korean War and WWII only
            12 .Pre-WWII only
WAOB        1
     World area of birth
            1 .US state (POB = 001-059)
 
            2 .PR and US Island Areas (POB = 060-099)
           3 .Latin America (POB = 303,310-399)
           4 .Asia (POB = 158-159,161,200-299)
           5 .Europe (POB = 100-157,160,162-199)
           6 .Africa (POB = 400-499)
           7 .Northern America (POB = 300-302,304-309)
           8 .Oceania and at Sea (POB = 500-554)
FAGEP      1
    Age allocation flag
           0 .No
           1 .Yes
FANCP      1
    Ancestry allocation flag
           0 .No
           1 .Yes
FCITP      1
    Citizenship allocation flag
           0 .No
           1 .Yes
FCOWP      1
    Class of worker allocation flag
           0 .No
           1 .Yes
FDDRSP     1
    Difficulty dressing allocation flag
           0 .No
           1 .Yes
FDEYEP     1
    Vision difficulty allocation flag
           0 .No
           1 .Yes
FDOUTP     1
    Difficulty going out allocation flag
           0 .No
           1 .Yes
FDPHYP     1
    Physical difficulty allocation flag
           0 .No
           1 .Yes
FDREMP     1
    Difficulty remembering allocation flag
           0 .No
           1 .Yes
FDWRKP     1
    Difficulty working allocation flag
           0 .No
           1 .Yes
FENGP      1
    Ability to speak English allocation flag
 
            0 .No
           1 .Yes
FESRP      1
    Employment status recode allocation flag
           0 .No
           1 .Yes
FFERP      1
    Children born within the past 12 months allocation flag
           0 .No
           1 .Yes
FGCLP      1
    Grandchildren living in house allocation flag
           0 .No
           1 .Yes
FGCMP      1
    Months responsible for grandchildren allocation flag
           0 .No
           1 .Yes
FGCRP      1
    Responsible for grandchildren allocation flag
           0 .No
           1 .Yes
FHISP      1
    Detailed Hispanic origin allocation flag
           0 .No
           1 .Yes
FINDP      1
    Industry allocation flag
           0 .No
           1 .Yes
FINTP      1
    Interest, dividend, and net rental income allocation
      flag
           0 .No
           1 .Yes
FJWDP      1
    Time of departure to work allocation flag
           0 .No
           1 .Yes
FJWMNP     1
    Travel time to work allocation flag
           0 .No
           1 .Yes
FJWRIP     1
    Vehicle occupancy allocation flag
           0 .No
           1 .Yes
FJWTRP     1
    Means of transportation to work allocation flag
 
             0 .No
            1 .Yes
FLANP       1
     Language spoken at home allocation flag
            0 .No
            1 .Yes
FLANXP      1
     Language other than English allocation flag
            0 .No
            1 .Yes
FMARP       1
     Marital status allocation flag
            0 .No
            1 .Yes
FMIGP       1
     Mobility status allocation flag
            0 .No
            1 .Yes
FMIGSP      1
     Migration state allocation flag
            0 .No
            1 .Yes
FMILPP      1
     Military periods of service allocation flag
            0 .No
            1 .Yes
FMILSP      1
     Military service allocation flag
            0 .No
            1 .Yes
FMILYP      1
     Years of military service allocation flag
            0 .No
            1 .Yes
FOCCP       1
     Occupation allocation flag
            0 .No
            1 .Yes
FOIP        1
     All other income allocation flag
            0 .No
            1 .Yes
FPAP        1
     Public assistance income allocation flag
            0 .No
            1 .Yes
FPOBP       1
     Place of birth
allocatin flag
 
             0 .No
            1 .Yes
FPOWSP      1
     Place of work state allocation flag
            0 .No
            1 .Yes
FRACP       1
     Detailed race allocation flag
            0 .No
            1 .Yes
FRELP       1
     Relationship allocation flag
            0 .No
            1 .Yes
FRETP       1
     Retirement income allocation flag
            0 .No
            1 .Yes
FSCHGP      1
     Grade attending allocation flag
            0 .No
            1 .Yes
FSCHLP      1
     Highest education allocation flag
            0 .No
            1 .Yes
FSCHP       1
     School enrollment allocation flag
            0 .No
            1 .Yes
FSEMP       1
     Self-employment income allocation flag
            0 .No
            1 .Yes
FSEXP       1
     Sex allocation flag
            0 .No
            1 .Yes
FSSIP       1
     Supplementary Security Income allocation flag
            0 .No
            1 .Yes
FSSP        1
     Social Security income allocation flag
            0 .No
            1 .Yes
FWAGP       1
     Wages and salary income allocation flag
            0 .No
 
            1 .Yes
FWKHP      1
    Usual hours worked per week past 12 months allocation flag
           0 .No
           1 .Yes
FWKLP      1
    Last worked allocation flag
           0 .No
           1 .Yes
FWKWP      1
    Weeks worked past 12 months allocation flag
           0 .No
           1 .Yes
FYOEP      1
    Year of entry allocation flag
           0 .No
           1 .Yes
PWGTP1     4
    Person's Weight replicate 1
           0001..9999 .Integer weight of person
PWGTP2     4
    Person's Weight replicate 2
           0001..9999 .Integer weight of person
PWGTP3     4
    Person's Weight replicate 3
           0001..9999 .Integer weight of person
PWGTP4     4
    Person's Weight replicate 4
           0001..9999 .Integer weight of person
PWGTP5     4
    Person's Weight replicate 5
           0001..9999 .Integer weight of person
PWGTP6     4
    Person's Weight replicate 6
           0001..9999 .Integer weight of person
PWGTP7     4
    Person's Weight replicate 7
           0001..9999 .Integer weight of person
PWGTP8     4
    Person's Weight replicate 8
           0001..9999 .Integer weight of person
PWGTP9     4
    Person's Weight replicate 9
           0001..9999 .Integer weight of person
PWGTP10    4
    Person's Weight replicate 10
           0001..9999 .Integer weight of person
 
 PWGTP11    4
    Person's Weight replicate 11
           0001..9999 .Integer weight of person
PWGTP12    4
    Person's Weight replicate 12
           0001..9999 .Integer weight of person
PWGTP13    4
    Person's Weight replicate 13
           0001..9999 .Integer weight of person
PWGTP14    4
    Person's Weight replicate 14
           0001..9999 .Integer weight of person
PWGTP15    4
    Person's Weight replicate 15
           0001..9999 .Integer weight of person
PWGTP16    4
    Person's Weight replicate 16
           0001..9999 .Integer weight of person
PWGTP17    4
    Person's Weight replicate 17
           0001..9999 .Integer weight of person
PWGTP18    4
    Person's Weight replicate 18
           0001..9999 .Integer weight of person
PWGTP19    4
    Person's Weight replicate 19
           0001..9999 .Integer weight of person
PWGTP20    4
    Person's Weight replicate 20
           0001..9999 .Integer weight of person
PWGTP21    4
    Person's Weight replicate 21
           0001..9999 .Integer weight of person
PWGTP22    4
    Person's Weight replicate 22
           0001..9999 .Integer weight of person
PWGTP23    4
    Person's Weight replicate 23
           0001..9999 .Integer weight of person
PWGTP24    4
    Person's Weight replicate 24
           0001..9999 .Integer weight of person
PWGTP25    4
    Person's Weight replicate 25
           0001..9999 .Integer weight of person
PWGTP26    4
 
     Person's Weight replicate 26
           0001..9999 .Integer weight of person
PWGTP27    4
    Person's Weight replicate 27
           0001..9999 .Integer weight of person
PWGTP28    4
    Person's Weight replicate 28
           0001..9999 .Integer weight of person
PWGTP29    4
    Person's Weight replicate 29
           0001..9999 .Integer weight of person
PWGTP30    4
    Person's Weight replicate 30
           0001..9999 .Integer weight of person
PWGTP31    4
    Person's Weight replicate 31
           0001..9999 .Integer weight of person
PWGTP32    4
    Person's Weight replicate 32
           0001..9999 .Integer weight of person
PWGTP33    4
    Person's Weight replicate 33
           0001..9999 .Integer weight of person
PWGTP34    4
    Person's Weight replicate 34
           0001..9999 .Integer weight of person
PWGTP35    4
    Person's Weight replicate 35
           0001..9999 .Integer weight of person
PWGTP36    4
    Person's Weight replicate 36
           0001..9999 .Integer weight of person
PWGTP37    4
    Person's Weight replicate 37
           0001..9999 .Integer weight of person
PWGTP38    4
    Person's Weight replicate 38
           0001..9999 .Integer weight of person
PWGTP39    4
    Person's Weight replicate 39
           0001..9999 .Integer weight of person
PWGTP40    4
    Person's Weight replicate 40
           0001..9999 .Integer weight of person
PWGTP41    4
    Person's Weight replicate 41
 
            0001..9999 .Integer weight of person
PWGTP42    4
    Person's Weight replicate 42
           0001..9999 .Integer weight of person
PWGTP43    4
    Person's Weight replicate 43
           0001..9999 .Integer weight of person
PWGTP44    4
    Person's Weight replicate 44
           0001..9999 .Integer weight of person
PWGTP45    4
    Person's Weight replicate 45
           0001..9999 .Integer weight of person
PWGTP46    4
    Person's Weight replicate 46
           0001..9999 .Integer weight of person
PWGTP47    4
    Person's Weight replicate 47
           0001..9999 .Integer weight of person
PWGTP48    4
    Person's Weight replicate 48
           0001..9999 .Integer weight of person
PWGTP49    4
    Person's Weight replicate 49
           0001..9999 .Integer weight of person
PWGTP50    4
    Person's Weight replicate 50
           0001..9999 .Integer weight of person
PWGTP51    4
    Person's Weight replicate 51
           0001..9999 .Integer weight of person
PWGTP52    4
    Person's Weight replicate 52
           0001..9999 .Integer weight of person
PWGTP53    4
    Person's Weight replicate 53
           0001..9999 .Integer weight of person
PWGTP54    4
    Person's Weight replicate 54
           0001..9999 .Integer weight of person
PWGTP55    4
    Person's Weight replicate 55
           0001..9999 .Integer weight of person
PWGTP56    4
    Person's Weight replicate 56
           0001..9999 .Integer weight of person
 
 PWGTP57    4
    Person's Weight replicate 57
           0001..9999 .Integer weight of person
PWGTP58    4
    Person's Weight replicate 58
           0001..9999 .Integer weight of person
PWGTP59    4
    Person's Weight replicate 59
           0001..9999 .Integer weight of person
PWGTP60    4
    Person's Weight replicate 60
           0001..9999 .Integer weight of person
PWGTP61    4
    Person's Weight replicate 61
           0001..9999 .Integer weight of person
PWGTP62    4
    Person's Weight replicate 62
           0001..9999 .Integer weight of person
PWGTP63    4
    Person's Weight replicate 63
           0001..9999 .Integer weight of person
PWGTP64    4
    Person's Weight replicate 64
           0001..9999 .Integer weight of person
PWGTP65    4
    Person's Weight replicate 65
           0001..9999 .Integer weight of person
PWGTP66    4
    Person's Weight replicate 66
           0001..9999 .Integer weight of person
PWGTP67    4
    Person's Weight replicate 67
           0001..9999 .Integer weight of person
PWGTP68    4
    Person's Weight replicate 68
           0001..9999 .Integer weight of person
PWGTP69    4
    Person's Weight replicate 69
           0001..9999 .Integer weight of person
PWGTP70    4
    Person's Weight replicate 70
           0001..9999 .Integer weight of person
PWGTP71    4
    Person's Weight replicate 71
           0001..9999 .Integer weight of person
PWGTP72    4
 
     Person's Weight replicate 72
           0001..9999 .Integer weight of person
PWGTP73    4
    Person's Weight replicate 73
           0001..9999 .Integer weight of person
PWGTP74    4
    Person's Weight replicate 74
           0001..9999 .Integer weight of person
PWGTP75    4
    Person's Weight replicate 75
           0001..9999 .Integer weight of person
PWGTP76    4
    Person's Weight replicate 76
           0001..9999 .Integer weight of person
PWGTP77    4
    Person's Weight replicate 77
           0001..9999 .Integer weight of person
PWGTP78    4
    Person's Weight replicate 78
           0001..9999 .Integer weight of person
PWGTP79    4
    Person's Weight replicate 79
           0001..9999 .Integer weight of person
PWGTP80    4
    Person's Weight replicate 80
           0001..9999 .Integer weight of person
*  For 35 Census codes that were aggregated or used the XX description for
   SOC, the Census description is used for the SOC title.
** These codes are pseudo codes developed by the Census Bureau and are not
   official or equivalent NAICS or SOC codes.
Legend to Identify NAICS Equivalents
     M = Multiple NAICS codes
     P = Part of a NAICS code - NAICS code split between two or more Census
         codes
     S = Not specified Industry in NAICS sector - Specific to Census codes
         only
     Z = Exception to NAICS code - Part of NAICS industry but has a unique
         Census code
Note for both Industry and Occupation lists in Data Dictionary:
NOTE: For additional information on NAICS and SOC groupings
within major categories visit our website at:
www.census.gov/hhes/www/ioindex.html.
NOTE: The World Area of Birth (WAOB) in the PUMS dataset is based upon the
Place of Birth (POB) variable in the full dataset, not the Place of Birth
recode created for PUMS (POBP). POB contains more detailed place of birth
 
 information than POBP.  Therefore, WAOB cannot be recoded from POBP.
Data users should be aware that the area groupings for WAOB are based
primarily upon United States administrative boundaries, not geographic
boundaries. For example, Guam is grouped into ""Puerto Rico and the U.S.
Island Areas,"" although geographically it is located in Oceania. Not all
places of birth are listed individually in POBP. Therefore the user
cannot recreate an exact world area of birth variable from POBP based upon
geographic location of birth.
*** Refer to top-coding documentation at:
http://www.census.gov/acs/www/Products/PUMS/C2SS/minmaxval6.htm
"
"./04_ExploratoryAnalysis/exploratoryGraphs/ExploratoryGraphs.pdf","Exploratory Graphs
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Why do we use graphs in data analysis?
· To understand data properties
· To find patterns in data
· To suggest modeling strategies
· To ""debug"" analyses
· To communicate results
                                       2/23
 
 Exploratory graphs
· To understand data properties
· To find patterns in data
· To suggest modeling strategies
· To ""debug"" analyses
· To communicate results
                                 3/23
 
 Characteristics of exploratory graphs
· They are made quickly
· A large number are made
· The goal is for personal understanding
· Axes/legends are generally cleaned up (later)
· Color/size are primarily used for information
                                                4/23
 
 Air Pollution in the United States
· The U.S. Environmental Protection Agency (EPA) sets national ambient air quality standards for
  outdoor air pollution
    - U.S. National Ambient Air Quality Standards
· For fine particle pollution (PM2.5), the ""annual mean, averaged over 3 years"" cannot exceed
  12 μg/m3 .
· Data on daily PM2.5 are available from the U.S. EPA web site
    - EPA Air Quality System
· Question: Are there any counties in the U.S. that exceed that national standard for fine particle
  pollution?
                                                                                               5/23
 
 Data
Annual average PM2.5 averaged over the period 2008 through 2010
 pollution <- read.csv(""data/avgpm25.csv"", colClasses = c(""numeric"", ""character"",
     ""factor"", ""numeric"", ""numeric""))
 head(pollution)
 ##     pm25  fips region longitude latitude
 ## 1  9.771 01003   east     -87.75    30.59
 ## 2  9.994 01027   east     -85.84    33.27
 ## 3 10.689 01033   east     -87.73    34.73
 ## 4 11.337 01049   east     -85.80    34.46
 ## 5 12.120 01055   east     -86.03    34.02
 ## 6 10.828 01069   east     -85.35    31.19
Do any counties exceed the standard of 12 μg/m3 ?
                                                                                  6/23
 
 Simple Summaries of Data
One dimension
 · Five-number summary
 · Boxplots
 · Histograms
 · Density plot
 · Barplot
                         7/23
 
 Five Number Summary
summary(pollution$pm25)
##    Min. 1st Qu.  Median Mean 3rd Qu.  Max.
##    3.38    8.55   10.00 9.84   11.40 18.40
                                              8/23
 
 Boxplot
boxplot(pollution$pm25, col = ""blue"")
                                      9/23
 
 Histogram
hist(pollution$pm25, col = ""green"")
                                    10/23
 
 Histogram
hist(pollution$pm25, col = ""green"")
rug(pollution$pm25)
                                    11/23
 
 Histogram
hist(pollution$pm25, col = ""green"", breaks = 100)
rug(pollution$pm25)
                                                  12/23
 
 Overlaying Features
boxplot(pollution$pm25, col = ""blue"")
abline(h = 12)
                                      13/23
 
 Overlaying Features
hist(pollution$pm25, col = ""green"")
abline(v = 12, lwd = 2)
abline(v = median(pollution$pm25), col = ""magenta"", lwd = 4)
                                                             14/23
 
 Barplot
barplot(table(pollution$region), col = ""wheat"", main = ""Number of Counties in Each Region"")
                                                                                         15/23
 
 Simple Summaries of Data
Two dimensions
 · Multiple/overlayed 1-D plots (Lattice/ggplot2)
 · Scatterplots
 · Smooth scatterplots
> 2 dimensions
 · Overlayed/multiple 2-D plots; coplots
 · Use color, size, shape to add dimensions
 · Spinning plots
 · Actual 3-D plots (not that useful)
                                                  16/23
 
 Multiple Boxplots
boxplot(pm25 ~ region, data = pollution, col = ""red"")
                                                      17/23
 
 Multiple Histograms
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
hist(subset(pollution, region == ""east"")$pm25, col = ""green"")
hist(subset(pollution, region == ""west"")$pm25, col = ""green"")
                                                              18/23
 
 Scatterplot
with(pollution, plot(latitude, pm25))
abline(h = 12, lwd = 2, lty = 2)
                                      19/23
 
 Scatterplot - Using Color
with(pollution, plot(latitude, pm25, col = region))
abline(h = 12, lwd = 2, lty = 2)
                                                    20/23
 
 Multiple Scatterplots
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))
with(subset(pollution, region == ""west""), plot(latitude, pm25, main = ""West""))
with(subset(pollution, region == ""east""), plot(latitude, pm25, main = ""East""))
                                                                               21/23
 
 Summary
· Exploratory plots are ""quick and dirty""
· Let you summarize the data (usually graphically) and highlight any broad features
· Explore basic questions and hypotheses (and perhaps rule them out)
· Suggest modeling strategies for the ""next step""
                                                                                    22/23
 
 Further resources
· R Graph Gallery
· R Bloggers
                  23/23
"
"./04_ExploratoryAnalysis/ggplot2/ppt/ggplot2.pdf","   Plo$ng	  with	  ggplot2	  
          Exploratory	  Data	  Analysis	  
                                	  
Roger	  D.	  Peng,	  Associate	  Professor	  of	  Biosta4s4cs	  
Johns	  Hopkins	  Bloomberg	  School	  of	  Public	  Health	  
 
                     What	  is	  ggplot2?	  
• An	  implementa:on	  of	  the	  Grammar	  of	  Graphics	  
  by	  Leland	  Wilkinson	  
• Wri@en	  by	  Hadley	  Wickham	  (while	  he	  was	  a	  
  graduate	  student	  at	  Iowa	  State)	  
• A	  “third”	  graphics	  system	  for	  R	  (along	  with	  base	  
  and	  la&ce)	  
• Available	  from	  CRAN	  via	  install.packages()!
• Web	  site:	  h@p://ggplot2.org	  (be@er	  documenta:on)	  
 
                    What	  is	  ggplot2?	  
• Grammar	  of	  graphics	  represents	  and	  
  abstrac:on	  of	  graphics	  ideas/objects	  
• Think	  “verb”,	  “noun”,	  “adjec:ve”	  for	  graphics	  
• Allows	  for	  a	  “theory”	  of	  graphics	  on	  which	  to	  
  build	  new	  graphics	  and	  graphics	  objects	  
• “Shorten	  the	  distance	  from	  mind	  to	  page”	  
 
                  Grammar	  of	  Graphics	  
“In	  brief,	  the	  grammar	  tells	  us	  that	  a	  sta:s:cal	  
graphic	  is	  a	  mapping	  from	  data	  to	  aesthe/c	  
a@ributes	  (colour,	  shape,	  size)	  of	  geometric	  
objects	  (points,	  lines,	  bars).	  The	  plot	  may	  also	  
contain	  sta:s:cal	  transforma:ons	  of	  the	  data	  
and	  is	  drawn	  on	  a	  speciﬁc	  coordinate	  system”	  
                                               from	  ggplot2	  book	  
 
               The	  Basics:	  qplot()!
• Works	  much	  like	  the	  plot	  func:on	  in	  base	  
  graphics	  system	  
• Looks	  for	  data	  in	  a	  data	  frame,	  similar	  to	  
  la$ce,	  or	  in	  the	  parent	  environment	  
• Plots	  are	  made	  up	  of	  aesthe4cs	  (size,	  shape,	  
  color)	  and	  geoms	  (points,	  lines)	  
 
                  The	  Basics:	  qplot()	  
• Factors	  are	  important	  for	  indica:ng	  subsets	  of	  
  the	  data	  (if	  they	  are	  to	  have	  diﬀerent	  
  proper:es);	  they	  should	  be	  labeled	  
• The	  qplot()	  hides	  what	  goes	  on	  underneath,	  
  which	  is	  okay	  for	  most	  opera:ons	  
• ggplot()	  is	  the	  core	  func:on	  and	  very	  ﬂexible	  
  for	  doing	  things	  qplot()	  cannot	  do	  
 
 Example	  Dataset	  
                       Factor	  label	  informa:on	  
                     important	  for	  annota:on	  
 
                        ggplot2	  “Hello,	  world!”	  
                   ●
                   ●
      40
               ●
               ●
               ●
               ●
           ●   ●
           ●
                       ●
                                 ●
                               ● ●                                                                                                     x	  coord	     y	  coord	  
      30       ●       ●       ●
           ●   ●       ●   ●   ● ●                       ●
hwy                    ●
                       ●   ●
                                 ●
                               ● ●           ●
                                                 ●
                                                 ●
                                                         ●
                                                         ●
                                                                   ●
                                                                   ●
                                                                                                                                                           data	  frame	  
               ●       ●   ●   ● ●     ●   ● ●           ● ●       ●    ●                                 ●                ●
               ●                 ●     ●   ● ●           ●         ●                              ●                        ●
                               ● ●   ● ●   ●     ●                      ●                                                          ●
                                 ●     ●                           ●    ●    ●       ●                    ●
                                     ●     ●     ●                 ●                 ●
                                                                   ●                 ●
      20                             ●                                   ●                         ● ●
                                                     ●         ●         ●           ● ●           ●
                                                               ●         ●   ●   ●                   ●   ● ●
                                                 ● ●                   ● ●   ●       ● ●     ●   ●   ●     ●       ●           ●
                                                                                     ● ●         ●   ●
                                                                        ●            ● ●         ● ● ●    ●    ●
                                                                                                   ●                   ●
                                                                                         ●
                       2                   3                            4                    5                     6               7
                                                                             displ
 
                Modifying	  aesthe:cs	  
                   ●
                                                                                                                                                 auto	  legend	  
      40
                   ●
                                                                                                                                                  placement	  
               ●
               ●
               ●
               ●
           ●   ●
           ●                     ●                                                                                                 drv
                       ●       ●●
      30       ●       ●       ●                                                                                                    ●    4
hwy
           ●   ●       ●   ●   ●●                        ●
                       ●         ●               ●       ●        ●
                       ●   ●   ●●            ●   ●       ●        ●                                                                 ●    f
               ●       ●   ●   ●●      ●   ●●            ●●       ●    ●                               ●               ●
               ●                 ●     ●   ●●            ●        ●                             ●                      ●
                               ●●    ●●    ●     ●                     ●                                                       ●    ●    r
                                 ●     ●                          ●    ●   ●       ●                   ●
                                     ●     ●     ●                ●                ●
                                                                  ●                ●
      20                             ●
                                                     ●        ●
                                                                       ●
                                                                       ●           ●●
                                                                                                ●●
                                                                                                ●
                                                              ●        ●   ●   ●                  ●   ●●
                                                 ●●                   ●●   ●       ●●      ●   ● ●     ●       ●           ●
                                                                                   ●●          ● ●
                                                                       ●           ●●          ●●●     ●   ●
                                                                                                ●                  ●
                                                                                       ●
                       2                   3                           4                   5                   6               7             color	  aesthe:c	  
                                                                           displ
                           qplot(displ,	  hwy,	  data	  =	  mpg,	  color	  =	  drv)	  
 
                                    Adding	  a	  geom	  
                       ●
                       ●
      40
                   ●
                   ●
                   ●
                   ●
               ●   ●
               ●                     ●
                           ●       ● ●
      30           ●       ●       ●
hwy
               ●   ●       ●   ●   ● ●                       ●
                           ●         ●               ●       ●         ●
                           ●   ●   ● ●           ●   ●       ●         ●
                   ●       ●   ●   ● ●     ●   ● ●           ● ●       ●    ●                                 ●                ●
                   ●                 ●     ●   ● ●           ●         ●                              ●                        ●
                                   ● ●   ● ●   ●     ●                      ●                                                          ●
                                     ●     ●                           ●    ●    ●       ●                    ●
                                         ●     ●     ●                 ●                 ●
                                                                       ●                 ●
      20                                 ●
                                                         ●         ●
                                                                             ●
                                                                             ●           ● ●
                                                                                                       ● ●
                                                                                                       ●
                                                                   ●         ●   ●   ●                   ●   ● ●
                                                     ● ●                   ● ●   ●       ● ●     ●   ●   ●     ●       ●           ●
                                                                                         ● ●         ●   ●
                                                                            ●            ● ●         ● ● ●    ●    ●
                                                                                                       ●                   ●
                                                                                             ●
                           2                   3                            4                    5                     6               7
                                                                                 displ
           qplot(displ,	  hwy,	  data	  =	  mpg,	  geom	  =	  c(""point"",	  ""smooth""))	  
 
                        Histograms	  
        30
                                                                            drv
                                                                                  4
count
        20
                                                                                  f
                                                                                  r
        10
        0
             10         20                     30                  40
                                         hwy
                  qplot(hwy,	  data	  =	  mpg,	  ﬁll	  =	  drv)	  
 
                                                                                              Facets	  
  qplot(displ,	  hwy,	  data	  =	  mpg,	  facets	  =	  .	  ~	  drv)	  
                                                                                                                                                 30
                                4                                        f                                  r
                                                                                                                                                 20
                                                      ●
                                                                                                                                                                                    4
                                                                                                                                                 10
                                                      ●
      40
                                                                                                                                                  0
                                                      ●
                                                      ●
                                                      ●
                                                                                                                                                 30
                                                      ●
                                                                                                                                         count
                                                    ●●
                                                    ●     ●                                                                                      20
                                                      ● ●●                                                                                                                          f
      30                                             ●● ●
hwy
                                                    ●●●●●●       ●
             ●                                        ● ●       ●● ●                                                                             10
             ● ●                                      ●● ●     ●●● ●
           ● ● ●                                      ● ●● ●●● ●●●                                    ●●                     ●   ●
           ●   ● ● ●
               ● ●
                                                              ● ●
                                                         ● ●●● ●
                                                                                 ●                    ●
                                                                                                        ●
                                                                                                                                 ●
                                                                                                                                     ●
                                                                                                                                                  0
               ●             ●                              ●      ●●                                           ●            ●
                 ● ●                                            ● ●                                             ●                                30
                                                                   ●                                            ●
      20        ●            ●                                                                                          ●●
                         ● ● ●
                           ● ●●●
                                 ●●  ●
                                       ●●                                                                                ●                       20
                        ●● ●●● ●● ●●● ●     ●                    ●                                              ●        ● ● ●                                                      r
                                 ●● ●                                                                                    ●
                             ●   ●● ●● ●●
                                     ●    ●
                                                                                                                        ●                        10
                                    ●
                                                                                                                                                  0
            2       3       4           5   6   7     2      3       4       5       6   7   2    3    4            5            6   7                10   20     30      40
                                                                     displ                                                                                      hwy
                                                                                              qplot(hwy,	  data	  =	  mpg,	  facets	  =	  drv	  ~	  .,	  binwidth	  =	  2)	  
 
                  MAACS	  Cohort	  
• Mouse	  Allergen	  and	  Asthma	  Cohort	  Study	  
• Bal:more	  children	  (aged	  5—17)	  
• Persistent	  asthma,	  exacerba:on	  in	  past	  year	  
• Study	  indoor	  environment	  and	  its	  rela:onship	  
  with	  asthma	  morbidity	  
• Recent	  publica:on:	  h@p://goo.gl/WqE9j8	  
 
  Exhaled	  nitric	  
    oxide	  
                                  Example:	  MAACS	  
                       Fine	  par:culate	  
                            ma@er	  
 Sensi:zed	  to	  
mouse	  allergen	  
 
              Histogram	  of	  eNO	  
        40
        30
count   20
        10
        0
               2            3                  4                5   6
                                    log(eno)
                   qplot(log(eno),	  data	  =	  maacs)	  
 
              Histogram	  by	  Group	  
        40
        30
                                                                                  mopos
count
                                                                                    no
        20
                                                                                    yes
        10
        0
               2               3              4              5              6
                                     log(eno)
               qplot(log(eno),	  data	  =	  maacs,	  ﬁll	  =	  mopos)	  
 
                                                        Density	  Smooth	  
          0.4
          0.3                                                                                          0.4
density
                                                                                                                                                                 mopos
                                                                                             density
          0.2                                                                                                                                                         no
                                                                                                                                                                      yes
                                                                                                       0.2
          0.1
          0.0
                                                                                                       0.0
                         2              3              4              5
                                            log(eno)                                                         2           3            4           5
                                                                                                                             log(eno)
                qplot(log(eno),	  data	  =	  maacs,	  geom	  =	  ""density"")	  
                                                                                        qplot(log(eno),	  data	  =	  maacs,	  geom	  =	  ""density"",	  color	  =	  mopos)	  
 
                                                      Sca@erplots:	  eNO	  vs.	  PM2.5	  
                                                                            ●                                                                                        ●                                                                                                      ●
                                                                                 ●
                                                                                                                                                                                                                                                                                ●
                                                             ●             ●                                                                             ●
                                                     ●                                                                                                                                                                                                          ●          ●
                                                                         ●
                                                                         ● ● ● ●●                                                                                                                                                                         ●
           5                                        ●       ●       ●●
                                                                                                                      5
                                                                                                                                                                     ● ●●                                                                                                ●● ● ● ●●
                                                 ●● ● ●             ●●● ●●  ●●    ●                                                                ●
                                                                                                                                                              ●                                                     5                                 ● ●
                                                                                                                                                                                                                                                                     ●
                                                                                                                                                                                                                                                                     ●
                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                      ●● ●● ●
                                                   ●        ●● ● ●● ●● ●                                                                         ●●●                                                                                               ●● ● ●                 ●●
                                             ●● ●            ●● ●    ●
                                                      ●● ●●● ●  ● ●● ●● ●● ●                                                                            ●● ● ● ●                                                                                     ●       ● ● ●● ●● ●
                                                                                                                                                                                                                                                                   ●
                                                               ●        ●                                                                       ●        ●                                                                                      ●● ●           ●● ●
                                                                                                                                                                                                                                                                         ●● ●
                                                    ●●●       ●  ●
                                                                 ●●● ●                                                                        ●
                                                      ● ●●
                                                      ●      ●●●
                                                                ●
                                                                ●
                                                                ●●  ●● ●     ●
                                                                         ●●● ● ●
                                                                                 ● ●                                                                 ● ●
                                                                                                                                                       ●   ●● ●                                                                                         ●● ●●●●   ●●● ●●
                                                                                                                                                                                                                                                                   ●● ●
                                     ●●
                                                 ● ●   ● ●●            ● ●●●  ● ● ●●
                                                                                     ●                                                                  ●
                                                                                                                                                   ●● ● ●●              ● ●                                                                           ●●●       ●●●●        ● ●● ●
                                         ● ● ●           ●● ● ●●●  ●●●
                                                                     ●●
                                                                     ●        ● ●● ●                                                                ● ●●   ●                                                                                            ●
                                                                                                                                                                                                                                                        ● ●● ●●   ●●●●● ●●●
                                 ●
                                     ●
                                             ●     ●●●●●● ● ● ●   ●●●●● ●●       ●         ●                                            ●           ●   ●           ●       ●                                                            ●● ●
                                                                                                                                                                                                                                                  ● ●    ●  ●  ●●●●● ●●●●●   ●
                                                                                                                                                                                                                                                                             ● ●
                                                                                                                                                                                                                                                                                   ●● ●
                                     ●                                                                                                      ●● ●                                                                                                                      ●
           4                                  ●
                                              ●     ●●●      ●●●
                                                             ●   ●●        ●    ● ●                                                    ●                ●                                                                                      ● ●         ●●● ●●●●   ●● ●      ●
log(eno)
                                              ●          ●
                                                         ●
                                                         ●  ●
                                                            ●   ●        ● ● ●●                                                                    ●           ●                ●                                                        ●           ●●●                                ●
                                           ●● ●●   ● ●●●
                                                   ●●●
                                                            ●● ●           ●●      ●                                                                                   ●●                                                              ● ●      ●       ●●●●● ●         ●●     ●
                                                                                                                                                                                                                                                                               ●●
                                     ● ● ● ●● ●        ●●●●●
                                                            ●                       ●                                                         ●         ● ● ●                                                                                   ●      ●      ●    ●
                                                                                                                                                                                                                                                                   ●●    ●
                                                      ●                             ●
                                                                                                                      4                              ●●● ●    ● ● ●                            mopos                4                           ●     ●●●     ●●● ●●●●● ●● ● ●● ●●                      mopos
                                                                                                           log(eno)
                                                                                                                                                                  ●●
                                                                                                                                                                                                         log(eno)
                                               ● ● ●     ●●  ● ● ●●●● ● ● ●                                                                   ●         ● ●                                                                                    ● ● ●       ●●●
                                                                                                                                                                                                                                                             ●●
                                                                                                                                                                                                                                                              ●            ● ●
                                         ●      ● ●●●● ●    ●● ●●●● ●                                                                                                                                                                         ● ●●   ●● ●  ●
                                              ●● ●●● ●  ●●
                                                            ●
                                                          ● ● ● ●  ●●      ●          ●                                                         ● ● ●
                                                                                                                                                      ●                   ●                                                              ●● ● ●     ●●● ●●●
                                                                                                                                                                                                                                                             ●●
                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                             ●    ●
                                                                                                                                                                                                                                                                                  ●
                                                  ●●● ●   ●          ● ●●●                                                                                                                                                                   ● ●●●
                                                                                                                                                                                                                                                            ●●● ●     ●● ● ● ●
                                           ●●         ● ●●  ●●●● ●●             ●
                                                                       ●●● ●● ● ●● ●                                                              ●     ● ●●●                                                                                        ●  ●● ● ●● ●●  ●●
                                                     ●  ●●
                                                         ●●●●●● ● ●
                                                             ●
                                                             ●      ●●●
                                                                       ●
                                                                       ●●          ●● ●                                                          ●●●●●●
                                                                                                                                                              ●● ●
                                                                                                                                                             ●●
                                                                                                                                                                  ●●                           ●   no                                            ●●●●
                                                                                                                                                                                                                                                    ● ●●●
                                                                                                                                                                                                                                                     ●●●
                                                                                                                                                                                                                                                          ●●
                                                                                                                                                                                                                                                          ●●
                                                                                                                                                                                                                                                             ●
                                                                                                                                                                                                                                                             ●
                                                                                                                                                                                                                                                            ●●  ●●  ●●● ●  ●●
                                                                                                                                                                                                                                                                           ●       ●                    ●   no
                 ●
                             ●           ●● ●   ●●   ● ●    ●    ●
                                                                 ●●●   ● ●●●●●                                                                             ● ●                                                                                ●● ● ● ●●      ●●   ● ●● ●● ●    ●
                                                ●● ●  ●           ●
                                                              ● ● ●  ●                                                                              ● ●●● ● ●            ●                                                                                      ●
                                                                                                                                                                                                                                                                ● ●●    ●
                                                                                                                                                                                                                                                                        ●●●      ●●●
                                                       ●
                                                        ●●●     ●●● ● ●
                                                                                 ●●
                                                                                                                                                    ●  ●●                ●●●
                                                                                                                                                                         ●                                                                             ●●● ●   ●
                                                                                                                                                                                                                                                             ●● ●   ●            ●
                                                                                                                                                                                                                                                                                 ●  ●
                                                       ●    ●
                                                            ●●
                                                           ●●  ●
                                                              ● ●●
                                                                  ●●●●●● ●●●●              ●●                                                    ● ● ●● ●●●● ●●
                                                                                                                                                                ●
                                                                                                                                                                                                   yes                                ●      ●● ●  ● ●●●   ●●      ●
                                                                                                                                                                                                                                                                   ●●●●●●●
                                                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                                                        ●  ● ●                          ●   yes
           3                                ●     ●
                                                  ●        ●    ●● ●●●● ●
                                                                            ●● ● ●
                                                                                        ●●
                                                                                           ●                              ●                     ●●        ●      ●                                                      ●                         ●
                                                                                                                                                                                                                                                  ●● ●   ● ●●
                                                                                                                                                                                                                                                             ●        ● ●●●●
                                       ● ●        ●●   ●●
                                                        ●●●●
                                                           ●● ●
                                                              ●●       ●● ●● ● ●                                                                                                                                                                            ●● ●   ●●● ●
                                                                                                                                                                                                                                                                  ●●           ●●
                                                       ●     ●●●● ● ●●●
                                                               ●         ● ●                                                                         ●●
                                                                                                                                                     ●   ●
                                                                                                                                                           ●●
                                                                                                                                                           ●
                                                                                                                                                        ●● ●●  ●
                                                                                                                                                               ● ●
                                                                                                                                                             ● ●● ●    ●        ●●                                                                       ●●
                                                                                                                                                                                                                                                          ●●  ●●  ●●●●●●● ●
                                                                                                                                                                                                                                                                          ●●            ●●
                                            ●
                                                  ●
                                                   ●●●●● ● ●
                                                     ●●
                                                ●● ● ● ●●●  ●
                                                               ● ●●
                                                                ●●
                                                                ● ●●
                                                                     ●
                                                                       ●
                                                                          ●●●●●
                                                                           ●   ●●●            ●                       3                      ●          ●      ● ●
                                                                                                                                                                   ●           ●                                    3                          ● ●  ●
                                                                                                                                                                                                                                                         ●●
                                                                                                                                                                                                                                                             ●●●●
                                                                                                                                                                                                                                                             ●      ●●● ●   ●● ● ● ●
                                                                                                                                                                                                                                                                           ●● ● ●●
                                                       ●● ●●●● ●●● ● ●
                                                                       ●                           ●                                                 ●
                                                                                                                                                     ●●●●●● ●●
                                                                                                                                                          ●     ● ●● ●
                                                                                                                                                                ●                                                                          ● ●      ●●    ●●●
                                                                                                                                                                                                                                                            ●●● ●
                                                                                                                                                                                                                                                                ●●●
                                                                                                                                                                                                                                                               ●●
                                                                                                                                                                                                                                                                    ●●●
                                                                                                                                                                                                                                                                 ●● ●●●
                                                                                                                                                                                                                                                                         ●●
                                                                                                                                                                                                                                                                          ●● ●
                                         ●           ●●
                                                      ●●●●● ●  ● ● ●
                                                                   ●        ●
                                                                                     ●         ●                                                     ●    ●● ●●                                                                                ● ●●
                                                                                                                                                                                                                                                         ●
                                                                                                                                                                                                                                                       ●●●● ●
                                                                                                                                                                                                                                                                 ●
                                                                                                                                                                                                                                                                 ● ●● ● ●  ●●●
                                                                                                                                                                                                                                                                             ●
                                                      ●              ● ●●                                                                    ● ●●   ●● ●  ● ● ●●                   ●                                                                ●●  ●● ●●●    ●● ● ● ●●●               ●
                                          ●    ●     ●●           ●●                                                                                ●● ● ● ●● ● ●●●                                                                               ●●    ●     ● ●● ●●
                                         ●       ● ● ● ●● ●●●         ●             ●                                                             ● ●● ●●  ● ●                                                                                                          ●
                                                            ●●●●●● ●●● ●●●    ● ●                                                                     ●●●● ●● ● ●          ●           ●
                                                                                                                                                                                                                                             ●         ●●●●●●● ● ●● ● ●            ●            ●
                                                            ●                                                                               ●       ●●
                                                                                                                                                     ● ●●    ●     ●                                                                                    ●●●●● ●  ●● ●       ●               ●
           2                                   ●
                                                  ● ●          ●
                                                                               ●
                                                                               ●
                                                                              ●●                                                                ● ●●● ● ●    ●● ●●        ●                                                                  ●
                                                                                                                                                                                                                                             ●   ●     ●●
                                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                                   ● ● ● ●● ●●     ●● ● ●●
                                                                                                                                                                                                                                                                                  ●
                                                                      ●                                                                                                                                                                                               ●●
                                                     ● ●● ●                              ●                                                          ● ●●  ●●●●●     ●●                                                                                       ●●  ●●●●        ●●
                                                                              ●                                                                           ● ●●
                                                                                                                                                          ●    ●● ● ●                                                                                        ●  ●      ●● ●●●
                                                           ●              ●       ●                                   2                           ● ●      ●
                                                                                                                                                                      ●
                                                                                                                                                                      ●                                             2                            ●
                                                                                                                                                                                                                                                    ● ●          ●
                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                             ●●
                                                                                                                                               ●               ●    ●●                                                                                 ●●  ● ● ●                      ●
                                                                                                                                                      ●● ●                    ●
                                                                                                                                                                     ●                                                                                                       ●
                         0                       2                          4                          6                                                         ●      ●                                                                                   ●             ●     ●
                                          log(pm25)
                                                                                                                                 0              2                    4                     6                                      0                   2                     4                       6
                                                                                                                                        log(pm25)                                                                                            log(pm25)
               qplot(log(pm25),	  log(eno),	  data	  =	                                                               qplot(log(pm25),	  log(eno),	  data	  =	                                                      qplot(log(pm25),	  log(eno),	  data	  =	  
               maacs)	                                                                                                   maacs,	  shape	  =	  mopos)	                                                                  maacs,	  color	  =	  mopos)	  
 
                      Sca@erplots:	  eNO	  vs.	  PM2.5	  
                                                                                                                  ●
                                                                                                                         ●
                                                                                               ●                 ●
                                                                                   ●
                                                                                                         ●● ●         ● ●
                                      5                                           ●
                                                                                           ●        ●
                                                                                                   ●●                    ●
                                                                                                                            ●
                                                                                                   ● ●          ●●
                                                                                ● ●●     ●                   ● ●
                                                                                  ●        ●● ● ●●            ●● ●
                                                                             ●          ●     ● ● ●     ●            ●
                                                                          ●
                                                                                    ●●         ● ● ● ● ●●   ●●
                                                                                  ● ● ●●●●●● ●     ●● ● ●                      ●
                                                                                    ●●            ●● ●      ●    ●       ●
                                                                              ●     ●●     ●  ● ● ●
                                                                                                  ● ● ●
                                                                                                      ● ●    ●●● ● ●
                                                                                                                ●             ●● ●
                                                               ●●                     ● ● ● ●●      ● ● ● ● ●●           ●
                                                                     ● ●        ●       ● ● ● ●● ● ●   ●●                ●
                                                              ●                  ●●●                ●     ●        ●    ●             ●
                                                          ●               ●          ●●●                  ● ●           ● ●
                                                              ●           ●        ●●● ●●  ●●●●● ●●● ●● ●
                                                                           ●                     ●             ● ●● ●        ●
                                      4                               ●●
                                                                         ●  ●     ●
                                                                                ●● ●
                                                                                         ●●●
                                                                                         ●  ●
                                                                                            ●
                                                                                            ●
                                                                                                  ●●
                                                                                                     ●
                                                                                                             ●●●            ●
                                                                                                                                                          mopos
                           log(eno)
                                                                                            ● ●                    ●
                                                              ● ●          ●● ●●●●● ●   ● ●●                                 ●●
                                                                    ●
                                                                            ●        ● ●●● ● ● ●● ● ● ●              ●    ●
                                                                             ● ●● ● ● ●     ●● ● ● ●
                                                                            ●●  ●
                                                                                  ●
                                                                               ● ● ●●  ●●
                                                                                          ●
                                                                                          ●
                                                                                          ●
                                                                                           ●   ●
                                                                                               ●    ●●●   ●  ●
                                                                                                              ● ●●
                                                                                                                ●
                                                                                                                                ●                         ●   no
                                                                                 ●●               ●     ●●            ●
                                                                        ●●           ● ● ●●●●           ● ●●● ●● ●
                                                                                    ● ●● ●                ●                 ● ●●
                                                                                                                           ●●
                                                      ●             ● ●      ●● ● ●●●●
                                                                                            ●●●
                                                                                              ●      ●   ●
                                                                                                   ●●●●● ●
                                                                                                   ●
                                                                                                           ●  ●
                                                                                                          ● ● ●
                                                                                                                              ●                           ●   yes
                                          ●                                  ●              ●
                                                                             ●●
                                                                                     ●●
                                                                                      ●        ● ● ●● ●● ●●
                                                                                          ●● ●●●    ● ●
                                                                                       ●
                                                                                      ●● ●●● ● ● ● ●●  ● ●
                                                                                                         ●
                                                                                                         ●   ●●●       ● ●           ●●
                                      3                                 ●       ●
                                                                                ●          ●
                                                                                            ● ● ●●
                                                                                                  ● ●●
                                                                                                                ●●●
                                                                                                                        ●
                                                                                                                         ●
                                                                                                                                ●
                                                                                                                                  ● ●
                                                                                                                                     ●
                                                                ●   ●            ● ●●●
                                                                                 ●       ●●● ●● ● ● ●●● ●● ●
                                                                                       ● ● ●●        ●
                                                                                                     ● ● ●●           ●
                                                                                      ●       ●● ●●          ● ●
                                                                        ●        ●● ● ● ● ● ●●  ● ●●●         ●●●●●                       ●
                                                                              ●●● ●  ● ● ● ●●
                                                                                     ●            ●● ●●●        ●    ●● ●
                                                                                             ●   ●
                                                                                      ● ●● ●●● ● ●● ●     ●                                       ●
                                                                     ●              ●●
                                                                                     ●●● ●● ●● ●     ●           ●
                                                                                                                  ●            ●              ●
                                                                                     ●              ●           ●
                                                                     ●       ●      ●●              ●● ●      ●
                                                                    ●         ●         ● ●                                   ●
                                                                                     ●          ● ● ●●●               ●
                                                                                            ● ● ●●                 ●
                                                                                            ● ● ●      ●● ● ● ●●
                                      2                                         ●     ●          ●
                                                                                                                     ●
                                                                                                                     ●
                                                                            ●                            ●         ● ●
                                                                                    ● ●●       ●                                   ●
                                                                                                                   ●
                                                                                          ●                  ●            ●
                                                0                           2                                     4                                   6
                                                                      log(pm25)
qplot(log(pm25),	  log(eno),	  data	  =	  maacs,	  color	  =	  mopos,	  geom	  =	  c(""point"",	  ""smooth""),	  method	  =	  ""lm"")	  
 
                                 Sca@erplots:	  eNO	  vs.	  PM2.5	  
                                                     no                                                                        yes
                                                                            ●
                                                                                                                                                         ●
                                                               ●                                                                                     ●
                                                                                                                                   ●
                                                                            ●●                                                                   ●
                                                                                                                                                 ●●
                            5                          ●
                                                                   ●
                                                                              ●
                                                                                                                                        ●     ●
                                                                                                                                             ●●● ●        ●
                                                       ●
                                                      ●●                                                                               ●         ● ●●
                                                              ●● ● ●       ●                                                    ●                 ● ●
                                                                                                                                              ●●
                                                                                                                                              ●
                                                   ●●           ●
                                                                  ●●                                                                ● ● ● ●●    ●●
                                                                                                                                                      ●
                                                        ●●● ●●●● ● ●             ● ●
                                                                                                                                 ●
                                                                                                                                  ●     ●●●●
                                                                                                                                           ●●●
                                                                                                                                              ●●
                                                         ●● ● ●● ●                                                                ●
                                                                                                                              ● ●● ●●●●   ●●  ● ●   ●
                                                                                                                                                 ●●● ●● ● ●
                                            ●
                                                ●● ●         ●               ●       ●                              ●                      ● ●● ●● ●     ●
                                           ●                  ●        ●                 ●                                          ● ● ●●●●  ●      ● ●●
                                                       ●                        ● ●                               ● ●      ●     ●
                                                                                                                                 ●●●       ●   ● ●
                                                                                                                                           ●
                                                   ●      ● ●●   ● ● ● ●●                                                   ●   ●●●●   ●●●● ●● ●       ●    ●
                            4                                  ● ●
                                                               ●  ●                                                         ● ●        ●         ●    ●
                 log(eno)
                                                  ●                        ●
                                                                           ●                                             ●     ●    ●
                                                                                                                                    ●●●
                                                                                                                                      ● ●                  ●
                                                    ● ● ●                          ●                                ●● ● ● ●   ●●●●●●●●
                                                                                                                                  ●
                                                                                                                                        ●            ●
                                                                                                                                                            ●
                                                            ●                                                                ●         ●      ● ●  ●   ●  ●
                                                      ●        ● ●●●●●●● ●                                              ●         ● ●
                                                                                                                                    ●
                                                                                                                             ● ● ●● ● ●●
                                                                                                                                            ●
                                                     ●●●●●●               ●●                                                ●●●    ●
                                                                                                                                   ● ●  ●
                                                                                                                                        ●  ●●       ●        ●
                                                                  ● ●                                                          ●● ●            ● ●● ●   ●
                                                          ● ●●●       ● ●         ●                                       ●●            ●      ●●●
                                                         ●  ●●          ●         ●●●
                                                                                  ●                                                ●●●●●● ● ●
                                                                                                                                       ●
                                                                                                                                       ●       ●  ●
                                                                                                                                                           ● ●
                                                    ● ● ● ●●●● ●●
                                                     ●       ●                                                  ●       ● ● ● ● ●● ●       ●● ● ● ●
                                ●                     ●         ●        ●                                                                 ●● ●
                                                                                                                             ● ●   ●       ●●
                                                          ●●●     ●● ●
                                                              ●●●● ●●●
                                                                          ●
                                                                       ●● ●     ●        ●●                                         ● ●● ●●●● ●●
                                                                                                                                                          ●
                            3                     ●
                                                          ●● ●●
                                                              ● ●●
                                                                    ●  ● ●●
                                                                            ●            ●                                     ●
                                                                                                                               ●    ●
                                                                                                                                      ●   ●●
                                                                                                                                          ●
                                                                                                                                            ●●
                                                                                                                                            ●  ●   ●
                                                                                                                                                     ● ● ●●
                                                                                                                                                         ●     ●
                                                           ● ● ● ●●     ●●
                                                                         ● ●●                                         ● ●      ● ● ●● ●
                                                           ●     ●●                                                                    ●●        ●●
                                                  ●    ●●● ● ●   ● ● ●●                     ●                                     ●  ● ● ●●        ●●●
                                                         ●● ● ● ●● ● ●●●                                                     ●●           ● ●
                                                      ● ● ● ●●    ● ●                                                             ● ●
                                                ●        ●  ●●● ● ● ● ●
                                                            ●                       ●            ●                                ●●●●●● ●                         ●
                                                          ●
                                                          ●      ● ●●
                                                                    ●       ●
                                                    ● ●● ● ● ●      ●● ●●          ●                                    ●●    ● ●
                                                          ● ●●   ● ●●
                                                                    ●●●      ●●                                                               ●
                                                                 ●
                                                                 ● ●●● ● ●                                                            ●             ●
                            2                       ●
                                                      ● ●        ●
                                                                              ●
                                                                             ●●
                                                                               ●
                                                           ●● ●        ●               ●                                           ●
                                                                             ●
                                                                          ●      ●                                                     ●
                                    0               2                       4                        6      0                  2                     4                 6
                                                                                                log(pm25)
qplot(log(pm25),	  log(eno),	  data	  =	  maacs,	  geom	  =	  c(""point"",	  ""smooth""),	  method	  =	  ""lm"",	  facets	  =	  .	  ~	  mopos)	  
 
               Summary	  of	  qplot()	  
• The	  qplot()	  func:on	  is	  the	  analog	  to	  plot()	  but	  
  with	  many	  built-­‐in	  features	  
• Syntax	  somewhere	  in	  between	  base/la$ce	  
• Produces	  very	  nice	  graphics,	  essen:ally	  
  publica:on	  ready	  (if	  you	  like	  the	  design)	  
• Diﬃcult	  to	  go	  against	  the	  grain/customize	  (don’t	  
  bother;	  use	  full	  ggplot2	  power	  in	  that	  case)	  
 
                          Resources	  
• The	  ggplot2	  book	  by	  Hadley	  Wickham	  
• The	  R	  Graphics	  Cookbook	  by	  Winston	  Chang	  
  (examples	  in	  base	  plots	  and	  in	  ggplot2)	  
• ggplot2	  web	  site	  (h@p://ggplot2.org)	  
• ggplot2	  mailing	  list	  (h@p://goo.gl/OdW3uB),	  
  primarily	  for	  developers	  
 
                     What	  is	  ggplot2?	  
• An	  implementa:on	  of	  the	  Grammar	  of	  Graphics	  
  by	  Leland	  Wilkinson	  
• Grammar	  of	  graphics	  represents	  and	  abstrac:on	  
  of	  graphics	  ideas/objects	  
• Think	  “verb”,	  “noun”,	  “adjec:ve”	  for	  graphics	  
• Allows	  for	  a	  “theory”	  of	  graphics	  on	  which	  to	  
  build	  new	  graphics	  and	  graphics	  objects	  
 
 Basic	  Components	  of	  a	  ggplot2	  Plot	  
• A	  data	  frame	  
• aesthe/c	  mappings:	  how	  data	  are	  mapped	  to	  color,	  size	  	  
• geoms:	  geometric	  objects	  like	  points,	  lines,	  shapes.	  	  
• facets:	  for	  condi:onal	  plots.	  	  
• stats:	  sta:s:cal	  transforma:ons	  like	  binning,	  quan:les,	  
  smoothing.	  	  
• scales:	  what	  scale	  an	  aesthe:c	  map	  uses	  (example:	  male	  =	  
  red,	  female	  =	  blue).	  	  
• coordinate	  system	  	  
 
          Building	  Plots	  with	  ggplot2	  
• When	  building	  plots	  in	  ggplot2	  (rather	  than	  
  using	  qplot)	  the	  “ar:st’s	  pale@e”	  model	  may	  
  be	  the	  closest	  analogy	  
• Plots	  are	  built	  up	  in	  layers	  
   – Plot	  the	  data	  
   – Overlay	  a	  summary	  
   – Metadata	  and	  annota:on	  
 
     Example:	  BMI,	  PM2.5,	  Asthma	  
• Mouse	  Allergen	  and	  Asthma	  Cohort	  Study	  
• Bal:more	  children	  (age	  5-­‐17)	  
• Persistent	  asthma,	  exacerba:on	  in	  past	  year	  
• Does	  BMI	  (normal	  vs.	  overweight)	  modify	  the	  
  rela:onship	  between	  PM2.5	  and	  asthma	  
  symptoms?	  
 
                                                                                 Basic	  Plot	  
                                                      normal weight                                                                overweight
                                                    ●●                     ●
                                                                           ●                                                           ●    ●    ●●       ●● ● ●
                      10                        ●        ●
     NocturnalSympt
                                                             ●
                                                                      ●         ●●                                                ●        ● ● ●     ●
                                                                      ●     ● ●                                                    ●       ●         ●●
                                                                  ●● ●                                                        ●
                                                                                                                              ●
                       5                        ●     ●               ●                                                        ●       ●        ●●    ●   ●
                                                      ●●         ●●   ●     ●           ●                                     ● ● ● ●● ●                                 ● ●
                                     ●      ● ● ●●
                                                 ● ●●●●●●                        ●●                           ●         ●
                                                                                                                        ●     ●●       ●
                                                                                                                                       ●● ● ● ● ●
                                                                                                                                                ●●            ●●         ●
                                ●     ●●    ●●● ●● ●●            ●●
                                                                  ●●● ●
                                                                      ●●
                                                                       ●● ●
                                                                          ● ●● ●●● ●
                                                                                   ●● ●                                     ●●●
                                                                                                                              ●● ●●
                                                                                                                                  ●        ●● ● ● ● ●          ● ●             ●
                                           ●●       ●●
                                                     ●●●●●●
                                                          ●
                                                          ●●●●
                                                             ●●● ● ● ●●
                                                                      ●              ● ● ●●
                                                                                          ●                             ● ● ● ● ●●
                                                                                                                                 ● ●● ●
                                                                                                                                      ●●●
                                                                                                                                        ●●                ●
                       0   ●
                           ●   ●     ● ●●
                                        ●
                                        ●●
                                         ●●
                                          ●●●
                                            ●●
                                             ●●●●
                                                ●
                                                ●●
                                                 ●●●
                                                   ●
                                                   ●
                                                   ●●
                                                    ●●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●●
                                                       ●●
                                                        ●
                                                        ●●
                                                         ●●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●●
                                                              ●●●
                                                                ●
                                                                ●●●
                                                                  ●
                                                                  ●●
                                                                   ●
                                                                   ●●
                                                                    ●
                                                                    ●●
                                                                     ●●●
                                                                       ●
                                                                       ●
                                                                       ●●
                                                                        ●●
                                                                         ●
                                                                         ●●
                                                                          ●●●●
                                                                             ●●●
                                                                               ●●●
                                                                                 ●●●
                                                                                   ●● ● ●               ●   ● ●   ●● ● ●
                                                                                                                       ●●●
                                                                                                                         ●
                                                                                                                         ●●●
                                                                                                                           ●
                                                                                                                           ●
                                                                                                                           ●●●
                                                                                                                             ●●
                                                                                                                              ●●●
                                                                                                                                ●●
                                                                                                                                 ●
                                                                                                                                 ●●
                                                                                                                                  ●●
                                                                                                                                   ●
                                                                                                                                   ●●●
                                                                                                                                     ●●●●
                                                                                                                                        ●●
                                                                                                                                         ●
                                                                                                                                         ●●●
                                                                                                                                           ●
                                                                                                                                           ●●
                                                                                                                                            ●
                                                                                                                                            ●●●
                                                                                                                                              ●●●
                                                                                                                                                ●
                                                                                                                                                ●●
                                                                                                                                                 ●●●●●●
                                                                                                                                                     ●
                                                                                                                                                     ●● ●
                                                                                                                                                        ●●● ●●
                                                                                                                                                            ●● ●
                               0.5              1.0                       1.5               2.0   2.5       0.5             1.0                  1.5               2.0         2.5
                                                                                                  logpm25
qplot(logpm25, NocturnalSympt, data = maacs, facets = . ~ bmicat, geom =
c(""point"", ""smooth""), method = ""lm”)!
 
                 Building	  Up	  in	  Layers	  
> head(maacs)!
    logpm25        bmicat NocturnalSympt!
2 1.5361795 normal weight              1!
3 1.5905409 normal weight              0!                   Data	  Frame	  
4 1.5217786 normal weight              0!
5 1.4323277 normal weight              0!
6 1.2762320    overweight              8!
                                                    Aesthe:cs	  
8 0.7139103    overweight              0!
!                                                            Ini:al	  call	  to	  
> g <- ggplot(maacs, aes(logpm25, NocturnalSympt))!               ggplot	  
!
> summary(g)!
data: logpm25, bmicat, NocturnalSympt [554x3]!               Summary	  of	  
mapping: x = logpm25, y = NocturnalSympt!                   ggplot	  object	  
faceting: facet_null() !
!
 
                   No	  Plot	  Yet!	  
> g <- ggplot(maacs, aes(logpm25, NocturnalSympt))!
> print(g)!
Error: No layers in plot!
!
> p <- g + geom_point()!             Explicitly	  save	  and	  print	  
> print(p)!                                ggplot	  object	  
!
> g + geom_point()!                    Auto-­‐print	  plot	  object	  
                                          without	  saving	  
 
                       First	  Plot	  with	  Point	  Layer	  
                                                                     ● ●               ●               ●              ●●
                                                                                                                       ●                 ● ●        ●       ●
                 10                                              ●           ●
NocturnalSympt
                                                                                      ●
                                                                             ●                 ●       ●       ● ●              ● ●●
                                                                                 ●                 ●       ●                ●●      ●
                                                                        ●●                      ● ●            ●
                  5                                          ●          ●
                                                                        ●                  ●                   ●● ●
                                                                                                                  ●             ●        ●
                                                                      ● ● ●●           ● ● ●
                                                                                           ●               ●●               ●                        ●                     ●   ●
                                        ●    ●           ●
                                                         ●● ●         ●●
                                                                       ●●            ● ● ●●
                                                                                          ●● ●● ● ●                ●    ●       ●● ●●    ●
                                                                                                                                         ●      ●       ●              ●
                                                                                                                                                                       ●
                                    ●            ●●      ● ● ●●● ●●●
                                                                   ● ● ●
                                                                       ●●●●                    ●●
                                                                                                ●●●● ●●
                                                                                                      ●
                                                                                                      ●●●●●                 ●
                                                                                                                            ●●      ●
                                                                                                                                    ●●   ● ●●       ●● ● ●       ● ●               ●
                                                      ● ●●       ● ● ●●●     ●●
                                                                              ● ●●
                                                                                 ●●●
                                                                                   ●●
                                                                                    ●●● ●
                                                                                        ●●●●
                                                                                           ● ● ●●●●● ●● ●●
                                                                                                         ●                               ● ● ●          ● ●●
                  0    ●●   ●   ●       ●   ●●●
                                              ● ●●●●●●
                                                     ●●
                                                      ●●●●
                                                         ●●●
                                                           ●
                                                           ●●●
                                                             ●●
                                                              ●●●
                                                                ●
                                                                ●●
                                                                 ●●
                                                                  ●
                                                                  ●●
                                                                   ●
                                                                   ●●●
                                                                     ●
                                                                     ●●●
                                                                       ●
                                                                       ●●
                                                                        ●
                                                                        ●●●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●●●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                   ●●
                                                                                    ●●
                                                                                     ●●
                                                                                      ●
                                                                                      ●●
                                                                                       ●
                                                                                       ●●
                                                                                        ●●
                                                                                         ●
                                                                                         ●●
                                                                                          ●●
                                                                                           ●
                                                                                           ●●
                                                                                            ●●
                                                                                             ●
                                                                                             ●●
                                                                                              ●●
                                                                                               ●●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●●
                                                                                                  ●●
                                                                                                   ●●
                                                                                                    ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●●●
                                                                                                        ●●●
                                                                                                          ●●
                                                                                                           ●●
                                                                                                            ●
                                                                                                            ●●
                                                                                                             ●●
                                                                                                             ●●
                                                                                                              ●
                                                                                                              ●
                                                                                                              ●●
                                                                                                               ●●●
                                                                                                                 ●
                                                                                                                 ●●
                                                                                                                  ●●
                                                                                                                   ●●
                                                                                                                    ●●●
                                                                                                                      ●●
                                                                                                                       ●●● ●
                                                                                                                           ●●●●●●●
                                                                                                                                 ●●
                                                                                                                                  ●
                                                                                                                                  ●●● ●●                          ●● ●
                                0.5                               1.0                                                 1.5                                       2.0                2.5
                                                                                           logpm25
                      g <- ggplot(maacs, aes(logpm25, NocturnalSympt))!
                      g + geom_point()!
 
                                 Adding	  More	  Layers:	  Smooth	  
                                                   ●●        ●     ●     ●●
                                                                          ●          ●● ● ●                                                                         ●●        ●     ●     ●●
                                                                                                                                                                                           ●          ●● ● ●
                 10                             ●       ●
                                                             ●                                                                    10                             ●       ●
NocturnalSympt                                                                                                   NocturnalSympt
                                                        ●         ● ● ●●      ● ●●                                                                                            ●
                                                         ●        ● ●       ●● ●                                                                                         ●         ● ● ●●      ● ●●
                                                    ●
                                                    ●             ●● ●                                                                                                    ●        ● ●       ●● ●
                 5                             ●     ●
                                                     ●        ●        ●
                                                                       ●●      ●     ●
                                                                                                                                                                     ●
                                                                                                                                                                     ●             ●● ●
                                                    ●● ●● ● ●●
                                                             ● ●●           ●            ●         ●   ●
                                ●   ●        ●
                                             ●
                                             ●●     ●●
                                                     ●● ●●●
                                                          ●
                                                          ●●●● ●●        ● ● ●● ●● ● ● ●           ●
                                                                                                                                   5                            ●     ●
                                                                                                                                                                      ●        ●        ●
                                                                                                                                                                                        ●●      ●     ●
                                                                                                                                                                     ●● ●● ● ●●
                                                                                                                                                                              ● ●●           ●            ●         ●   ●
                            ●         ●●     ● ●●
                                                ●●●
                                                  ●●
                                                   ● ●●
                                                      ●●● ●●
                                                           ●●
                                                            ●● ●●
                                                                ●●
                                                                 ●●● ●
                                                                     ●● ●● ●●● ●
                                                                               ●● ● ●●                     ●
                                           ●●● ● ●●
                                                  ●● ●●
                                                      ●●●
                                                        ●●
                                                         ●●
                                                          ●●
                                                           ● ●●
                                                              ●
                                                              ●●●●●
                                                                  ●●● ●● ●●
                                                                          ●          ● ●● ●●●                                                    ●   ●        ●
                                                                                                                                                              ●
                                                                                                                                                              ●●     ●●
                                                                                                                                                                      ●● ●●●
                                                                                                                                                                           ●
                                                                                                                                                                           ●●●● ●●        ● ● ●● ●● ● ● ●           ●
                 0    ●
                      ●●   ● ●      ●●● ●●
                                         ●●
                                          ●●
                                           ●●
                                            ●●
                                             ●●
                                              ●
                                              ●●●
                                                ●●
                                                ●●
                                                 ●
                                                 ●●●
                                                   ●
                                                   ●
                                                   ●●
                                                    ●
                                                    ●●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●●
                                                                 ●
                                                                 ●●
                                                                  ●●
                                                                  ●
                                                                  ●●
                                                                   ●
                                                                   ●●
                                                                    ●
                                                                    ●●
                                                                     ●
                                                                     ●●
                                                                      ●
                                                                      ●●
                                                                       ●
                                                                       ●●
                                                                        ●●
                                                                         ●
                                                                         ●●
                                                                          ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●●●●
                                                                                  ●
                                                                                  ●●●●●
                                                                                      ●
                                                                                      ●●●
                                                                                        ●●●
                                                                                          ●
                                                                                          ●●
                                                                                           ●●
                                                                                            ●●● ●● ●                                         ●         ●●     ● ●●
                                                                                                                                                                 ●●●
                                                                                                                                                                   ●●
                                                                                                                                                                    ● ●●
                                                                                                                                                                       ●●● ●●
                                                                                                                                                                            ●●
                                                                                                                                                                             ●● ●●
                                                                                                                                                                                 ●●
                                                                                                                                                                                  ●●● ●
                                                                                                                                                                                      ●● ●● ●●● ●
                                                                                                                                                                                                ●● ● ●●                     ●
                                                                                                                                                            ●●● ● ●●
                                                                                                                                                                   ●● ●●
                                                                                                                                                                       ●●●
                                                                                                                                                                         ●●
                                                                                                                                                                          ●●
                                                                                                                                                                           ●●
                                                                                                                                                                            ● ●●
                                                                                                                                                                               ●
                                                                                                                                                                               ●●●●●
                                                                                                                                                                                   ●●● ●● ●●
                                                                                                                                                                                           ●          ● ●● ●●●
                                                                                                                                   0   ●
                                                                                                                                       ●●   ● ●      ●●● ●●
                                                                                                                                                          ●●
                                                                                                                                                           ●●
                                                                                                                                                            ●●
                                                                                                                                                             ●●
                                                                                                                                                              ●●
                                                                                                                                                               ●
                                                                                                                                                               ●●●
                                                                                                                                                                 ●●
                                                                                                                                                                 ●●
                                                                                                                                                                  ●
                                                                                                                                                                  ●●●
                                                                                                                                                                    ●
                                                                                                                                                                    ●
                                                                                                                                                                    ●●
                                                                                                                                                                     ●
                                                                                                                                                                     ●●
                                                                                                                                                                      ●●
                                                                                                                                                                       ●
                                                                                                                                                                       ●
                                                                                                                                                                       ●●
                                                                                                                                                                        ●
                                                                                                                                                                        ●●
                                                                                                                                                                         ●
                                                                                                                                                                         ●●
                                                                                                                                                                          ●
                                                                                                                                                                          ●●●
                                                                                                                                                                            ●
                                                                                                                                                                            ●
                                                                                                                                                                            ●●
                                                                                                                                                                             ●●
                                                                                                                                                                              ●
                                                                                                                                                                              ●
                                                                                                                                                                              ●●
                                                                                                                                                                               ●●
                                                                                                                                                                                ●
                                                                                                                                                                                ●●
                                                                                                                                                                                 ●
                                                                                                                                                                                 ●●
                                                                                                                                                                                  ●
                                                                                                                                                                                  ●●
                                                                                                                                                                                   ●●
                                                                                                                                                                                   ●
                                                                                                                                                                                   ●●
                                                                                                                                                                                    ●
                                                                                                                                                                                    ●●
                                                                                                                                                                                     ●
                                                                                                                                                                                     ●●
                                                                                                                                                                                      ●
                                                                                                                                                                                      ●●
                                                                                                                                                                                       ●
                                                                                                                                                                                       ●●
                                                                                                                                                                                        ●
                                                                                                                                                                                        ●●
                                                                                                                                                                                         ●●
                                                                                                                                                                                          ●
                                                                                                                                                                                          ●●
                                                                                                                                                                                           ●●
                                                                                                                                                                                            ●
                                                                                                                                                                                            ●●
                                                                                                                                                                                             ●
                                                                                                                                                                                             ●●
                                                                                                                                                                                              ●
                                                                                                                                                                                              ●●
                                                                                                                                                                                               ●●
                                                                                                                                                                                                ●
                                                                                                                                                                                                ●●
                                                                                                                                                                                                 ●
                                                                                                                                                                                                 ●●●●
                                                                                                                                                                                                   ●
                                                                                                                                                                                                   ●●●●●
                                                                                                                                                                                                       ●
                                                                                                                                                                                                       ●●●
                                                                                                                                                                                                         ●●●
                                                                                                                                                                                                           ●
                                                                                                                                                                                                           ●●
                                                                                                                                                                                                            ●●
                                                                                                                                                                                                             ●●● ●● ●
                           0.5                  1.0                      1.5                 2.0           2.5                              0.5                  1.0                      1.5                 2.0           2.5
                                                             logpm25                                                                                                          logpm25
                      g + geom_point() + geom_smooth()!                                                             g + geom_point() + geom_smooth(method = ""lm”)!
 
                       Adding	  More	  Layers:	  Facets	  
 Labels	  from	                                                   normal weight                                                             overweight
facet	  variable	                                                ●●                     ●
                                                                                          ●                                                       ●    ●    ●●       ●● ● ●
                                       10                      ●        ●
                      NocturnalSympt
                                                                            ●
                                                                                     ●         ●●                                             ●       ● ● ●     ●
                                                                                     ●     ● ●                                                ●       ●         ●●
                                                                                 ●● ●                                                     ●
                                                                                                                                          ●
                                        5                      ●    ●                ●                                                    ●       ●        ●●    ●   ●
                                                                    ●●          ●●   ●     ●         ●                                   ● ● ●●● ●                              ● ●
                                                      ●    ● ● ●●
                                                                ● ●●●●●●                        ●●                         ●        ●
                                                                                                                                    ●     ●●      ●
                                                                                                                                                  ●● ● ● ● ●
                                                                                                                                                           ●●            ●●     ●
                                                 ●     ●● ●●●●●●●               ●●●
                                                                                  ●●●●
                                                                                     ●● ●
                                                                                        ● ●● ●●● ●
                                                                                                 ●● ●                                   ●●●
                                                                                                                                          ●
                                                                                                                                          ● ●●
                                                                                                                                             ●        ●●● ● ● ●           ● ●         ●
                                                          ●●       ●●
                                                                    ●●●●●
                                                                        ●●
                                                                         ●●●●
                                                                            ●
                                                                            ●● ● ●●●
                                                                                   ● ● ● ●●
                                                                                          ●                                         ● ● ● ● ●●
                                                                                                                                             ● ●● ●
                                                                                                                                                  ●●●
                                                                                                                                                    ●●               ●
                                        0   ●
                                            ●   ●     ● ●●
                                                         ●
                                                         ●●
                                                          ●●
                                                           ●●●
                                                             ●●
                                                              ●●
                                                               ●●●
                                                                 ●
                                                                 ●
                                                                 ●●●
                                                                   ●
                                                                   ●
                                                                   ●
                                                                   ●●
                                                                    ●
                                                                    ●
                                                                    ●●
                                                                     ●
                                                                     ●
                                                                     ●●●
                                                                       ●
                                                                       ●●
                                                                        ●
                                                                        ●●
                                                                         ●●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●●●
                                                                               ●
                                                                               ●●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                   ●●
                                                                                    ●●
                                                                                     ●
                                                                                     ●
                                                                                     ●●
                                                                                      ●●
                                                                                       ●
                                                                                       ●●
                                                                                        ●
                                                                                        ●●●
                                                                                          ●●●
                                                                                            ●●●
                                                                                              ●●●
                                                                                                ●● ●●                ●   ● ●   ●● ● ●
                                                                                                                                    ●●●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                      ●●●
                                                                                                                                        ●
                                                                                                                                        ●●
                                                                                                                                         ●●●
                                                                                                                                           ●●●
                                                                                                                                             ●●
                                                                                                                                              ●
                                                                                                                                              ●●
                                                                                                                                               ●
                                                                                                                                               ●●●
                                                                                                                                                 ●●
                                                                                                                                                  ●●●
                                                                                                                                                    ●●
                                                                                                                                                     ●
                                                                                                                                                     ●●
                                                                                                                                                      ●●
                                                                                                                                                       ●
                                                                                                                                                       ●●
                                                                                                                                                        ●
                                                                                                                                                        ●●
                                                                                                                                                         ●●●
                                                                                                                                                           ●
                                                                                                                                                           ●●
                                                                                                                                                            ●●●●●
                                                                                                                                                                ●●●
                                                                                                                                                                  ●●
                                                                                                                                                                   ● ●●
                                                                                                                                                                     ●● ●
                                                0.5            1.0                       1.5             2.0   2.5       0.5            1.0                1.5            2.0         2.5
                                                                                                               logpm25
  Add	  facets	                                                                                                                                 Face:ng	  (factor)	  variable	  
         g + geom_point() + facet_grid(. ~ bmicat) + geom_smooth(method = ""lm"")!
 
                              Annota:on	  
• Labels:	  xlab(),	  ylab(),	  labs(),	  gg:tle()	  
• Each	  of	  the	  “geom”	  func:ons	  has	  op:ons	  to	  modify	  	  
• For	  things	  that	  only	  make	  sense	  globally,	  use	  theme()	  	  
   – Example:	  theme(legend.posi:on	  =	  ""none"")	  	  
• Two	  standard	  appearance	  themes	  are	  included	  
   – theme_gray():	  The	  default	  theme	  (gray	  background)	  
   – theme_bw():	  More	  stark/plain	  	  
 
                                        Modifying	  Aesthe:cs	  
                 10
NocturnalSympt
                                                                              10
                                                             NocturnalSympt
                                                                                                                               bmicat
                                                                                                                                  normal weight
                                                                                                                                  overweight
                  5
                                                                               5
                  0                                                            0
                        0.5      1.0       1.5   2.0   2.5                            0.5         1.0       1.5    2.0   2.5
                                       logpm25                                                          logpm25
g + geom_point(color = ""steelblue”,                                                g + geom_point(aes(color = bmicat),
size = 4, alpha = 1/2)!                                                            size = 4, alpha = 1/2)!
                 Constant	  values	                                                       Data	  variable	  
 
                                                  Modifying	  Labels	  
                                                                MAACS Cohort
                                                                     ●●       ●    ●     ●●
                                                                                          ●        ●● ● ●
                  Nocturnal Symptoms
                                       10                         ●       ●
                                                                              ●
                                                                          ●       ● ● ●●    ●●●
                                                                                                                             bmicat
                                                                          ●       ●●        ●● ●
                                                                                                                              ●   normal weight
                                                                      ●
                                                                      ●           ●● ●                                        ●   overweight
                                        5                        ●    ●
                                                                      ●       ●        ●
                                                                                       ●●     ●    ●
                                                                      ●● ●● ●●●
                                                                              ● ●●          ●          ●         ● ●                            labs()	  func:on	  for	  
                                                  ●
                                                   ●   ●
                                                         ●●
                                                               ●
                                                               ● ● ●●
                                                               ●●●
                                                                 ●●●
                                                                   ●
                                                                    ●● ●●●
                                                                    ●
                                                                    ● ●●
                                                                       ●●
                                                                         ●
                                                                         ●●●●●● ● ● ●
                                                                         ● ●●
                                                                            ●●
                                                                             ●● ●●
                                                                                 ●●
                                                                                  ●●
                                                                                    ●●
                                                                                   ●●
                                                                                      ●● ● ●
                                                                                     ●● ●● ●●● ●
                                                                                               ●●● ●●
                                                                                                                 ●
                                                                                                                       ●
                                                                                                                                               modifying	  :tles	  and	  
                                                              ●●
                                                               ● ●●●
                                                                   ●●●
                                                                     ●
                                                                     ●●●
                                                                       ●●
                                                                        ●
                                                                        ●●●●●
                                                                            ●●●●
                                                                               ●●●
                                                                                 ●●● ●●
                                                                                      ●            ●●● ●●
                                                                                                        ●                                         x-­‐,	  y-­‐axis	  labels	  
                                        0   ●
                                            ●●   ● ●   ●
                                                       ●● ●●
                                                           ●
                                                           ●●
                                                            ●●
                                                             ●●●●
                                                                ●●
                                                                 ●●
                                                                 ●
                                                                 ●●●
                                                                   ●
                                                                   ●
                                                                   ●●
                                                                    ●●
                                                                     ●
                                                                     ●●
                                                                      ●●
                                                                       ●
                                                                       ●
                                                                       ●●
                                                                        ●
                                                                        ●●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                          ●●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                   ●
                                                                                   ●●
                                                                                    ●
                                                                                    ●●
                                                                                     ●
                                                                                     ●●
                                                                                      ●●
                                                                                       ●
                                                                                       ●●
                                                                                        ●
                                                                                        ●●
                                                                                         ●
                                                                                         ●●
                                                                                          ●
                                                                                          ●●
                                                                                           ●●
                                                                                            ●
                                                                                            ●●
                                                                                             ●●●●
                                                                                               ●
                                                                                               ●●●●
                                                                                                  ●
                                                                                                  ●●●
                                                                                                    ●
                                                                                                    ●●●
                                                                                                      ●●
                                                                                                      ●●
                                                                                                       ●●
                                                                                                        ● ●● ●
                                                 0.5              1.0                   1.5                2.0         2.5
                                                                          log PM2.5
g + geom_point(aes(color = bmicat)) + labs(title = ""MAACS Cohort"") + labs(x = expression(""log ""
* PM[2.5]), y = ""Nocturnal Symptoms"")!
 
                   Customizing	  the	  Smooth	  
                                10
               NocturnalSympt
                                                                        bmicat
                                                                           normal weight
                                                                           overweight
                                5
Modiﬁed	  
smoother	  
                                0
                                     0.5   1.0        1.5   2.0   2.5
                                                 logpm25
     g + geom_point(aes(color = bmicat), size = 2, alpha = 1/2) +
     geom_smooth(size = 4, linetype = 3, method = ""lm"", se = FALSE)!
 
                                 Changing	  the	  Theme	  
                                                                         ●●         ●           ●         ●●
                                                                                                           ●          ●● ● ●
                        10                                           ●        ●
       NocturnalSympt
                                                                                    ●
                                                                               ●            ●   ● ●●           ● ●●
                                                                                                                                                           bmicat
                                                                               ●            ● ●                ●● ●
                                                                                                                                                            ●   normal weight
                                                                          ●●                ●●      ●                                                       ●   overweight
                         5                                       ●         ●
                                                                           ●            ●           ●
                                                                                                    ●●●        ●      ●
                                                                         ●● ●● ● ● ●
                                                                                   ●                ●●     ●                 ●               ●   ●
                                            ●    ●          ●
                                                            ●
                                                            ●●           ●●
                                                                          ●●       ●●●
                                                                                     ●●
                                                                                      ●●● ●●             ● ● ●● ●● ●      ● ●            ●
                                                                                                                                         ●
                                        ●            ●●     ● ●●
                                                               ●●●
                                                                 ●●● ●●
                                                                      ●●●               ●●
                                                                                         ●●
                                                                                          ●● ●●
                                                                                              ●
                                                                                              ●●●●● ●
                                                                                                    ●● ●● ● ●● ●●● ● ● ●                             ●
                                                          ● ●●   ● ● ●●● ●●
                                                                          ●●●
                                                                            ●●
                                                                             ●●
                                                                              ●
                                                                              ●●●●
                                                                                 ●●
                                                                                  ●●
                                                                                   ●●●●
                                                                                      ●●● ●● ●●
                                                                                              ●                       ● ●●   ●●●
                         0   ●● ●   ●       ●   ●●●
                                                  ● ●●
                                                     ●●●
                                                       ●●
                                                        ●●●
                                                          ●●
                                                           ●●●
                                                             ●
                                                             ●●
                                                              ●●
                                                               ●●●●
                                                                 ●
                                                                 ●●
                                                                  ●
                                                                  ●●
                                                                   ●●
                                                                    ●
                                                                    ●●
                                                                     ●●
                                                                      ●
                                                                      ●
                                                                      ●●
                                                                       ●●
                                                                        ●
                                                                        ●●
                                                                         ●●
                                                                          ●
                                                                          ●●●
                                                                            ●
                                                                            ●●
                                                                             ●●
                                                                              ●
                                                                              ●●
                                                                              ●
                                                                              ●●
                                                                               ●●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●●
                                                                                   ●
                                                                                   ●
                                                                                   ●●
                                                                                    ●
                                                                                    ●●
                                                                                    ●●
                                                                                     ●
                                                                                     ●●
                                                                                      ●
                                                                                      ●●
                                                                                       ●
                                                                                       ●
                                                                                       ●●
                                                                                        ●
                                                                                        ●●
                                                                                         ●
                                                                                         ●●
                                                                                          ●●
                                                                                           ●●
                                                                                            ●●
                                                                                             ●●
                                                                                              ●
                                                                                              ●●
                                                                                               ●●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●●
                                                                                                      ●●
                                                                                                       ●●
                                                                                                        ●●●●
                                                                                                           ●●
                                                                                                           ●●●
                                                                                                             ●●
                                                                                                             ●●●
                                                                                                               ●●●
                                                                                                                 ●●● ●● ●
                                    0.5                              1.0                                 1.5                       2.0               2.5
                                                                                     logpm25
g + geom_point(aes(color = bmicat)) + theme_bw(base_family = ""Times”)!
 
                        A	  Notes	  about	  Axis	  Limits	  
                                                              100
              3
              2
                                                               75
              1
  testdat$y
              0
                                                          y    50
              −1
                                                               25
              −3
                   0   20   40    60    80   100                0
                            testdat$x                               0   25    50     75     100
                                                                               x
testdat <- data.frame(x = 1:100, y = rnorm(100))!
                                                              g <- ggplot(testdat, aes(x = x, y = y))!
testdat[50,2] <- 100 ## Outlier!!
plot(testdat$x, testdat$y, type = ""l"", ylim = c(-3,3))!       g + geom_line()!
 
 Outlier	  missing	  
                                   Axis	  Limits	                       Outlier	  included	  
                                                     3
                                                     2
     2
                                                     1
y    0                                          y    0
                                                    −1
    −2
                                                    −2
                                                    −3
         0        25     50   75    100                  0   25      50       75        100
                         x                                           x
                                          g + geom_line() + coord_cartesian(ylim = c(-3, 3))!
         g + geom_line() + ylim(-3, 3)!
 
             More	  Complex	  Example	  
• How	  does	  the	  rela:onship	  between	  PM2.5	  and	  
  nocturnal	  symptoms	  vary	  by	  BMI	  and	  NO2?	  
• Unlike	  our	  previous	  BMI	  variable,	  NO2	  is	  
  con:nuous	  
• We	  need	  to	  make	  NO2	  categorical	  so	  we	  can	  
  condi:on	  on	  it	  in	  the	  plo$ng	  
   – Use	  the	  cut()	  func:on	  for	  this	  
 
                     Making	  NO2	  Deciles	  
## Calculate the deciles of the data!
> cutpoints <- quantile(maacs$logno2_new, seq(0, 1, length = 11), na.rm = TRUE)!
!
## Cut the data at the deciles and create a new factor variable!
> maacs$no2dec <- cut(maacs$logno2_new, cutpoints)!
!
## See the levels of the newly created factor variable!
> levels(maacs$no2dec)!
  [1] ""(0.378,0.969]"" ""(0.969,1.1]""   ""(1.1,1.17]""   ""(1.17,1.26]"" !
  [5] ""(1.26,1.32]""   ""(1.32,1.38]""   ""(1.38,1.44]""  ""(1.44,1.54]"" !
  [9] ""(1.54,1.69]""   ""(1.69,2.55]"" !
!
 
      Non-­‐default	  font	  
                               Final	  Plot	   Mul:ple	  panels	  
Transparent	  
   points	  
                                                            Smoother	  
Labels/Title	  
 
             Code	  for	  Final	  Plot	  
                                        Add	  points	  
## Setup ggplot with data frame!
g <- ggplot(maacs, aes(logpm25, NocturnalSympt))!
!                                                          Add	  smoother	  
## Add layers!                        Make	  panels	  
g + geom_point(alpha = 1/3) !
  + facet_wrap(bmicat ~ no2dec, nrow = 2, ncol = 4) !
  + geom_smooth(method=""lm"", se=FALSE, col=""steelblue"")                        !
  + theme_bw(base_family = ""Avenir"", base_size = 10) !
  + labs(x = expression(""log "" * PM[2.5]) !
  + labs(y = ""Nocturnal Symptoms”) !                      Change	  theme	  
  + labs(title = ""MAACS Cohort”)!
                                                    Add	  labels	  
 
                             Summary	  
• ggplot2	  is	  very	  powerful	  and	  ﬂexible	  if	  you	  
  learn	  the	  “grammar”	  and	  the	  various	  elements	  
  that	  can	  be	  tuned/modiﬁed	  
• Many	  more	  types	  of	  plots	  can	  be	  made;	  
  explore	  and	  mess	  around	  with	  the	  package	  
  (references	  men:oned	  in	  Part	  1	  are	  useful)	  
"
"./04_ExploratoryAnalysis/GraphicsDevices/GraphicsDevicesinR.pdf","Graphics Devices in R
Roger Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is a Graphics Device?
· A graphics device is something where you can make a plot appear
    - A window on your computer (screen device)
    - A PDF file (file device)
    - A PNG or JPEG file (file device)
    - A scalable vector graphics (SVG) file (file device)
· When you make a plot in R, it has to be ""sent"" to a specific graphics device
· The most common place for a plot to be ""sent"" is the screen device
    - On a Mac the screen device is launched with the quartz()
    - On Windows the screen device is launched with windows()
    - On Unix/Linux the screen device is launched with x11()
                                                                               2/10
 
 What is a Graphic Device?
· When making a plot, you need to consider how the plot will be used to determine what device the
  plot should be sent to.
    - The list of devices is found in ?Devices; there are also devices created by users on CRAN
· For quick visualizations and exploratory analysis, usually you want to use the screen device
    - Functions like plot in base, xyplot in lattice, or qplot in ggplot2 will default to sending a
       plot to the screen device
    - On a given platform (Mac, Windows, Unix/Linux) there is only one screen device
· For plots that may be printed out or be incorporated into a document (e.g. papers/reports, slide
  presentations), usually a file device is more appropriate
    - There are many different file devices to choose from
· NOTE: Not all graphics devices are available on all platforms (i.e. you cannot launch the
  windows() on a Mac)
                                                                                                3/10
 
 How Does a Plot Get Created?
There are two basic approaches to plotting. The first is most common:
  1. Call a plotting function like plot, xyplot, or qplot
  2. The plot appears on the screen device
  3. Annotate plot if necessary
  4. Enjoy
 library(datasets)
 with(faithful, plot(eruptions, waiting))      ## Make plot appear on screen device
 title(main = ""Old Faithful Geyser data"")      ## Annotate with a title
                                                                                    4/10
 
 How Does a Plot Get Created?
The second approach to plotting is most commonly used for file devices:
  1. Explicitly launch a graphics device
  2. Call a plotting function to make a plot (Note: if you are using a file device, no plot will appear on
     the screen)
  3. Annotate plot if necessary
  4. Explicitly close graphics device with dev.off() (this is very important!)
 pdf(file = ""myplot.pdf"") ## Open PDF device; create 'myplot.pdf' in my working directory
 ## Create plot and send to a file (no plot appears on screen)
 with(faithful, plot(eruptions, waiting))
 title(main = ""Old Faithful Geyser data"") ## Annotate plot; still nothing on screen
 dev.off() ## Close the PDF file device
 ## Now you can view the file 'myplot.pdf' on your computer
                                                                                                       5/10
 
 Graphics File Devices
There are two basic types of file devices: vector and bitmap devices
Vector formats:
 · pdf: useful for line-type graphics, resizes well, usually portable, not efficient if a plot has many
   objects/points
 · svg: XML-based scalable vector graphics; supports animation and interactivity, potentially useful
   for web-based plots
 · win.metafile: Windows metafile format (only on Windows)
 · postscript: older format, also resizes well, usually portable, can be used to create
   encapsulated postscript files; Windows systems often don’t have a postscript viewer
                                                                                                    6/10
 
 Graphics File Devices
Bitmap formats
 · png: bitmapped format, good for line drawings or images with solid colors, uses lossless
   compression (like the old GIF format), most web browsers can read this format natively, good for
   plotting many many many points, does not resize well
 · jpeg: good for photographs or natural scenes, uses lossy compression, good for plotting many
   many many points, does not resize well, can be read by almost any computer and any web
   browser, not great for line drawings
 · tiff: Creates bitmap files in the TIFF format; supports lossless compression
 · bmp: a native Windows bitmapped format
                                                                                               7/10
 
 Multiple Open Graphics Devices
· It is possible to open multiple graphics devices (screen, file, or both), for example when viewing
  multiple plots at once
· Plotting can only occur on one graphics device at a time
· The currently active graphics device can be found by calling dev.cur()
· Every open graphics device is assigned an integer ≥ 2.
· You can change the active graphics device with dev.set(<integer>) where <integer> is the
  number associated with the graphics device you want to switch to
                                                                                                 8/10
 
 Copying Plots
Copying a plot to another device can be useful because some plots require a lot of code and it can
be a pain to type all that in again for a different device.
 · dev.copy: copy a plot from one device to another
 · dev.copy2pdf: specifically copy a plot to a PDF file
NOTE: Copying a plot is not an exact operation, so the result may not be identical to the original.
 library(datasets)
 with(faithful, plot(eruptions, waiting)) ## Create plot on screen device
 title(main = ""Old Faithful Geyser data"") ## Add a main title
 dev.copy(png, file = ""geyserplot.png"") ## Copy my plot to a PNG file
 dev.off() ## Don't forget to close the PNG device!
                                                                                                    9/10
 
 Summary
· Plots must be created on a graphics device
· The default graphics device is almost always the screen device, which is most useful for
  exploratory analysis
· File devices are useful for creating plots that can be included in other documents or sent to other
  people
· For file devices, there are vector and bitmap formats
     - Vector formats are good for line drawings and plots with solid colors using a modest number
       of points
     - Bitmap formats are good for plots with a large number of points, natural scenes or web-
       based plots
                                                                                                 10/10
"
"./04_ExploratoryAnalysis/hierarchicalClustering/Hierarchical clustering.pdf","8/28/13                                                                                                  Hierarchical clustering
                            Hierarchical clustering
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                    1/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Can we find things that are close together?
              Clustering organizes things that are close into groups
                 · How do we define close?
                 · How do we group things?
                 · How do we visualize the grouping?
                 · How do we interpret the grouping?
                                                                                                                                 2/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         2/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hugely important/impactful
              http://scholar.google.com/scholar?hl=en&q=cluster+analysis&btnG=&as_sdt=1%2C21&as_sdtp=
                                                                                                                                 3/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         3/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering
                 · An agglomerative approach
                          - Find closest two things
                          - Put them together
                          - Find next closest
                 · Requires
                          - A defined distance
                          - A merging approach
                 · Produces
                          - A tree showing how close things are to each other
                                                                                                                                 4/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         4/21
 
 8/28/13                                                                                                  Hierarchical clustering
              How do we define close?
                 · Most important step
                          - Garbage in -> garbage out
                 · Distance or similarity
                          - Continuous - euclidean distance
                          - Continuous - correlation similarity
                          - Binary - manhattan distance
                 · Pick a distance/similarity that makes sense for your problem
                                                                                                                                 5/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         5/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Example distances - Euclidean
              http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf
                                                                                                                                 6/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         6/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Example distances - Euclidean
              In general:
                                                                   √
                                                                        _
                                                                        (A  1  J
                                                                          _____A___
                                                                                     2
                                                                                         __________
                                                                                         2
                                                                                       ) + (B        1  J     _________________
                                                                                                            B ) + Q + (Z
                                                                                                              2
                                                                                                                  2
                                                                                                                                 1 J  ____
                                                                                                                                     Z )
                                                                                                                                      2
                                                                                                                                         2
              http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf
                                                                                                                                           7/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                                   7/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Example distances - Manhattan
              In general:
                                                                          | A1   J   A2 | + | B1       J   B2 | +     Q + |Z     1 J Z2 |
              http://en.wikipedia.org/wiki/Taxicab_geometry
                                                                                                                                          8/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                                  8/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - example
                 set.seed(1234); par(mar=c(0,0,0,0))
                 x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
                 y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
                 plot(x,y,col=""blue"",pch=19,cex=2)
                 text(x+0.05,y+0.05,labels=as.character(1:12))
                                                                                                                                 9/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                         9/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - dist
                 · Important parameters: x,method
                 dataFrame <- data.frame(x=x,y=y)
                 dist(dataFrame)
                                   1              2               3             4               5               6                7 8 9 10 11
                 2 0.34121
                 3 0.57494 0.24103
                 4 0.26382 0.52579 0.71862
                 5 1.69425 1.35818 1.11953 1.80667
                 6 1.65813 1.31960 1.08339 1.78081 0.08150
                 7 1.49823 1.16621 0.92569 1.60132 0.21110 0.21667
                 8 1.99149 1.69093 1.45649 2.02849 0.61704 0.69792 0.65063
                 9 2.13630 1.83168 1.67836 2.35676 1.18350 1.11500 1.28583 1.76461
                 10 2.06420 1.76999 1.63110 2.29239 1.23848 1.16550 1.32063 1.83518 0.14090
                 11 2.14702 1.85183 1.71074 2.37462 1.28154 1.21077 1.37370 1.86999 0.11624 0.08318
                 12 2.05664 1.74663 1.58659 2.27232 1.07701 1.00777 1.17740 1.66224 0.10849 0.19129 0.20803
                                                                                                                                             10/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                                      10/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - #1
                                                                                                                                 11/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          11/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - #2
                                                                                                                                 12/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          12/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - #3
                                                                                                                                 13/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          13/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Hierarchical clustering - hclust
                 dataFrame <- data.frame(x=x,y=y)
                 distxy <- dist(dataFrame)
                 hClustering <- hclust(distxy)
                 plot(hClustering)
                                                                                                                                 14/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          14/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Prettier dendrograms
                 myplclust <- function( hclust, lab=hclust$labels, lab.col=rep(1,length(hclust$labels)), hang=0.1,...){
                    ## modifiction of plclust for plotting hclust objects *in colour*!
                    ## Copyright Eva KF Chan 2009
                    ## Arguments:
                    ##           hclust:              hclust object
                    ##           lab:                   a character vector of labels of the leaves of the tree
                    ##           lab.col:               colour for the labels; NA=default device foreground colour
                    ##           hang:               as in hclust & plclust
                    ## Side effect:
                    ##           A display of hierarchical cluster with coloured leaf labels.
                    y <- rep(hclust$height,2); x <- as.numeric(hclust$merge)
                    y <- y[which(x<0)]; x <- x[which(x<0)]; x <- abs(x)
                    y <- y[order(x)]; x <- x[order(x)]
                    plot( hclust, labels=FALSE, hang=hang, ... )
                    text( x=x, y=y[hclust$order]-(max(hclust$height)*hang),
                                 labels=lab[hclust$order], col=lab.col[hclust$order],
                                 srt=90, adj=c(1,0.5), xpd=NA, ... )
                 }
                                                                                                                                 15/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          15/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Pretty dendrograms
                 dataFrame <- data.frame(x=x,y=y)
                 distxy <- dist(dataFrame)
                 hClustering <- hclust(distxy)
                 myplclust(hClustering,lab=rep(1:3,each=4),lab.col=rep(1:3,each=4))
                                                                                                                                 16/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          16/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Even Prettier dendrograms
              http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79
                                                                                                                                 17/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          17/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Merging points - complete
                                                                                                                                 18/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          18/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Merging points - average
                                                                                                                                 19/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          19/21
 
 8/28/13                                                                                                  Hierarchical clustering
              heatmap()
                 dataFrame <- data.frame(x=x,y=y)
                 set.seed(143)
                 dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
                 heatmap(dataMatrix)
                                                                                                                                 20/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          20/21
 
 8/28/13                                                                                                  Hierarchical clustering
              Notes and further resources
                 · Gives an idea of the relationships between variables/observations
                 · The picture may be unstable
                          - Change a few points
                          - Have different missing values
                          - Pick a different distance
                          - Change the merging strategy
                          - Change the scale of points for one variable
                 · But it is deterministic
                 · Choosing where to cut isn't always obvious
                 · Should be primarily used for exploration
                 · Rafa's Distances and Clustering Video
                 · Elements of statistical learning
                                                                                                                                 21/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/004hierachicalClustering/index.html#1                          21/21
"
"./04_ExploratoryAnalysis/kmeansClustering/K-meansClustering_169.pdf","K-means Clustering
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Can we find things that are close together?
· How do we define close?
· How do we group things?
· How do we visualize the grouping?
· How do we interpret the grouping?
                                            2/14
 
 How do we define close?
· Most important step
    - Garbage in ⟶ garbage out
· Distance or similarity
    - Continuous - euclidean distance
    - Continous - correlation similarity
    - Binary - manhattan distance
· Pick a distance/similarity that makes sense for your problem
                                                               3/14
 
 K-means clustering
· A partioning approach
    - Fix a number of clusters
    - Get ""centroids"" of each cluster
    - Assign things to closest centroid
    - Reclaculate centroids
· Requires
    - A defined distance metric
    - A number of clusters
    - An initial guess as to cluster centroids
· Produces
    - Final estimate of cluster centroids
    - An assignment of each point to clusters
                                               4/14
 
 K-means clustering - example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = ""blue"", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
                                                           5/14
 
 K-means clustering - starting centroids
                                        6/14
 
 K-means clustering - assign to closest
centroid
                                       7/14
 
 K-means clustering - recalculate centroids
                                           8/14
 
 K-means clustering - reassign values
                                     9/14
 
 K-means clustering - update centroids
                                      10/14
 
 kmeans()
· Important parameters: x, centers, iter.max, nstart
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
## [1] ""cluster""      ""centers""          ""totss""     ""withinss""
## [5] ""tot.withinss"" ""betweenss""        ""size""      ""iter""
## [9] ""ifault""
kmeansObj$cluster
##  [1] 3 3 3 3 1 1 1 1 2 2 2 2
                                                                11/14
 
 kmeans()
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
                                                                12/14
 
 Heatmaps
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = ""n"")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = ""n"")
                                                             13/14
 
 Notes and further resources
· K-means requires a number of clusters
    - Pick by eye/intuition
    - Pick by cross validation/information theory, etc.
    - Determining the number of clusters
· K-means is not deterministic
    - Different # of clusters
    - Different number of iterations
· Rafael Irizarry's Distances and Clustering Video
· Elements of statistical learning
                                                        14/14
"
"./04_ExploratoryAnalysis/kmeansClustering/K-meansClustering_letter.pdf","K-means Clustering
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Can we find things that are close together?
· How do we define close?
· How do we group things?
· How do we visualize the grouping?
· How do we interpret the grouping?
                                            2/14
 
 How do we define close?
· Most important step
    - Garbage in ⟶ garbage out
· Distance or similarity
    - Continuous - euclidean distance
    - Continous - correlation similarity
    - Binary - manhattan distance
· Pick a distance/similarity that makes sense for your problem
                                                               3/14
 
 K-means clustering
· A partioning approach
    - Fix a number of clusters
    - Get ""centroids"" of each cluster
    - Assign things to closest centroid
    - Reclaculate centroids
· Requires
    - A defined distance metric
    - A number of clusters
    - An initial guess as to cluster centroids
· Produces
    - Final estimate of cluster centroids
    - An assignment of each point to clusters
                                               4/14
 
 K-means clustering - example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = ""blue"", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
                                                           5/14
 
 K-means clustering - starting centroids
                                        6/14
 
 K-means clustering - assign to closest
centroid
                                       7/14
 
 K-means clustering - recalculate centroids
                                           8/14
 
 K-means clustering - reassign values
                                     9/14
 
 K-means clustering - update centroids
                                      10/14
 
 kmeans()
· Important parameters: x, centers, iter.max, nstart
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
## [1] ""cluster""      ""centers""          ""totss""     ""withinss""
## [5] ""tot.withinss"" ""betweenss""        ""size""      ""iter""
## [9] ""ifault""
kmeansObj$cluster
##  [1] 3 3 3 3 1 1 1 1 2 2 2 2
                                                                11/14
 
 kmeans()
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
                                                                12/14
 
 Heatmaps
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = ""n"")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = ""n"")
                                                             13/14
 
 Notes and further resources
· K-means requires a number of clusters
    - Pick by eye/intuition
    - Pick by cross validation/information theory, etc.
    - Determining the number of clusters
· K-means is not deterministic
    - Different # of clusters
    - Different number of iterations
· Rafael Irizarry's Distances and Clustering Video
· Elements of statistical learning
                                                        14/14
"
"./04_ExploratoryAnalysis/lectures/clusteringExample.pdf","EDA Case Study - Understanding Human
Activity with Smart Phones
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Samsung Galaxy S3
http://www.samsung.com/global/galaxys3/
                                        2/18
 
 Samsung Data
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
                                                                                    3/18
 
 Slightly processed data
Samsung data file
 load(""data/samsungData.rda"")
 names(samsungData)[1:12]
 ## [1]  ""tBodyAcc-mean()-X"" ""tBodyAcc-mean()-Y"" ""tBodyAcc-mean()-Z""
 ## [4]  ""tBodyAcc-std()-X""  ""tBodyAcc-std()-Y""  ""tBodyAcc-std()-Z""
 ## [7]  ""tBodyAcc-mad()-X""  ""tBodyAcc-mad()-Y""  ""tBodyAcc-mad()-Z""
 ## [10] ""tBodyAcc-max()-X""  ""tBodyAcc-max()-Y""  ""tBodyAcc-max()-Z""
 table(samsungData$activity)
 ##
 ##   laying  sitting standing     walk walkdown   walkup
 ##     1407      1286    1374     1226      986     1073
                                                                     4/18
 
 Plotting average acceleration for first subject
par(mfrow = c(1, 2), mar = c(5, 4, 1, 1))
samsungData <- transform(samsungData, activity = factor(activity))
sub1 <- subset(samsungData, subject == 1)
plot(sub1[, 1], col = sub1$activity, ylab = names(sub1)[1])
plot(sub1[, 2], col = sub1$activity, ylab = names(sub1)[2])
legend(""bottomright"", legend = unique(sub1$activity), col = unique(sub1$activity),
    pch = 1)
                                                                                   5/18
 
 Clustering based just on average acceleration
source(""myplclust.R"")
distanceMatrix <- dist(sub1[, 1:3])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
                                                         6/18
 
 Plotting max acceleration for the first subject
par(mfrow = c(1, 2))
plot(sub1[, 10], pch = 19, col = sub1$activity, ylab = names(sub1)[10])
plot(sub1[, 11], pch = 19, col = sub1$activity, ylab = names(sub1)[11])
                                                                        7/18
 
 Clustering based on maximum acceleration
source(""myplclust.R"")
distanceMatrix <- dist(sub1[, 10:12])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
                                                         8/18
 
 Singular Value Decomposition
svd1 = svd(scale(sub1[, -c(562, 563)]))
par(mfrow = c(1, 2))
plot(svd1$u[, 1], col = sub1$activity, pch = 19)
plot(svd1$u[, 2], col = sub1$activity, pch = 19)
                                                 9/18
 
 Find maximum contributor
plot(svd1$v[, 2], pch = 19)
                            10/18
 
 New clustering with maximum contributer
maxContrib <- which.max(svd1$v[, 2])
distanceMatrix <- dist(sub1[, c(10:12, maxContrib)])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
                                                         11/18
 
 New clustering with maximum contributer
names(samsungData)[maxContrib]
## [1] ""fBodyAcc.meanFreq...Z""
                                        12/18
 
 K-means clustering (nstart=1, first try)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6)
table(kClust$cluster, sub1$activity)
##
##     laying sitting standing walk walkdown walkup
##   1      0       0        0   50        1      0
##   2      0       0        0    0       48      0
##   3     27      37       51    0        0      0
##   4      3       0        0    0        0     53
##   5      0       0        0   45        0      0
##   6     20      10        2    0        0      0
                                                    13/18
 
 K-means clustering (nstart=1, second try)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 1)
table(kClust$cluster, sub1$activity)
##
##     laying sitting standing walk walkdown walkup
##   1      0       0        0    0       49      0
##   2     18      10        2    0        0      0
##   3      0       0        0   95        0      0
##   4     29       0        0    0        0      0
##   5      0      37       51    0        0      0
##   6      3       0        0    0        0     53
                                                                14/18
 
 K-means clustering (nstart=100, first try)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)
##
##     laying sitting standing walk walkdown walkup
##   1     18      10        2    0        0      0
##   2     29       0        0    0        0      0
##   3      0       0        0   95        0      0
##   4      0       0        0    0       49      0
##   5      3       0        0    0        0     53
##   6      0      37       51    0        0      0
                                                                  15/18
 
 K-means clustering (nstart=100, second try)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)
##
##     laying sitting standing walk walkdown walkup
##   1     29       0        0    0        0      0
##   2      3       0        0    0        0     53
##   3      0       0        0    0       49      0
##   4      0       0        0   95        0      0
##   5      0      37       51    0        0      0
##   6     18      10        2    0        0      0
                                                                  16/18
 
 Cluster 1 Variable Centers (Laying)
plot(kClust$center[1, 1:10], pch = 19, ylab = ""Cluster Center"", xlab = """")
                                                                           17/18
 
 Cluster 2 Variable Centers (Walking)
plot(kClust$center[4, 1:10], pch = 19, ylab = ""Cluster Center"", xlab = """")
                                                                           18/18
"
"./04_ExploratoryAnalysis/lectures/dimensionReduction.pdf","Principal Components Analysis and
Singular Value Decomposition
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Matrix data
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
                                                       2/24
 
 Cluster the data
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
                       3/24
 
 What if we add a pattern?
set.seed(678910)
for (i in 1:40) {
    # flip a coin
    coinFlip <- rbinom(1, size = 1, prob = 0.5)
    # if coin is heads add a common pattern to that row
    if (coinFlip) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
    }
}
                                                                    4/24
 
 What if we add a pattern? - the data
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
                                                       5/24
 
 What if we add a pattern? - the clustered data
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
                                              6/24
 
 Patterns in rows and columns
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = ""Row Mean"", ylab = ""Row"", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = ""Column"", ylab = ""Column Mean"", pch = 19)
                                                                                     7/24
 
 Related problems
You have multivariate variables X1 , … , Xn so X1 = (X11 , … , X1m )
 · Find a new set of multivariate variables that are uncorrelated and explain as much variance as
   possible.
 · If you put all the variables together in one matrix, find the best matrix created with fewer variables
   (lower rank) that explains the original data.
The first goal is statistical and the second goal is data compression.
                                                                                                      8/24
 
 Related solutions - PCA/SVD
SVD
If X is a matrix with each variable in a column and each observation in a row then the SVD is a
""matrix decomposition""
                                              X = UDV T
where the columns of U are orthogonal (left singular vectors), the columns of V are orthogonal (right
singular vectors) and D is a diagonal matrix (singular values).
PCA
The principal components are equal to the right singular values if you first scale (subtract the mean,
divide by the standard deviation) the variables.
                                                                                                   9/24
 
 Components of the SVD - u and v
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, , xlab = ""Row"", ylab = ""First left singular vector"",
    pch = 19)
plot(svd1$v[, 1], xlab = ""Column"", ylab = ""First right singular vector"", pch = 19)
                                                                                   10/24
 
 Components of the SVD - Variance explained
par(mfrow = c(1, 2))
plot(svd1$d, xlab = ""Column"", ylab = ""Singular value"", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = ""Column"", ylab = ""Prop. of variance explained"",
    pch = 19)
                                                                                    11/24
 
 Relationship to principal components
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = ""Principal Component 1"",
    ylab = ""Right Singular Vector 1"")
abline(c(0, 1))
                                                                                12/24
 
 Components of the SVD - variance explained
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1),each=5)}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d,xlab=""Column"",ylab=""Singular value"",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab=""Column"",ylab=""Prop. of variance explained"",pch=19)
                                                                                     13/24
 
 What if we add a second pattern?
set.seed(678910)
for (i in 1:40) {
    # flip a coin
    coinFlip1 <- rbinom(1, size = 1, prob = 0.5)
    coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
    # if coin is heads add a common pattern to that row
    if (coinFlip1) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), each = 5)
    }
    if (coinFlip2) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), 5)
    }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
                                                                    14/24
 
 Singular value decomposition - true patterns
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = ""Column"", ylab = ""Pattern 1"")
plot(rep(c(0, 1), 5), pch = 19, xlab = ""Column"", ylab = ""Pattern 2"")
                                                                            15/24
 
 v and patterns of variance in rows
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = ""Column"", ylab = ""First right singular vector"")
plot(svd2$v[, 2], pch = 19, xlab = ""Column"", ylab = ""Second right singular vector"")
                                                                                    16/24
 
 d and variance explained
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = ""Column"", ylab = ""Singular value"", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = ""Column"", ylab = ""Percent of variance explained"",
    pch = 19)
                                                                                      17/24
 
 Missing values
dataMatrix2 <- dataMatrixOrdered
## Randomly insert some missing data
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
svd1 <- svd(scale(dataMatrix2)) ## Doesn't work!
## Error: infinite or missing values in 'x'
                                                             18/24
 
 Imputing {impute}
 library(impute) ## Available from http://bioconductor.org
 dataMatrix2 <- dataMatrixOrdered
 dataMatrix2[sample(1:100,size=40,replace=FALSE)] <- NA
 dataMatrix2 <- impute.knn(dataMatrix2)$data
 svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
 par(mfrow=c(1,2)); plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19)
                                                                        19/24
 
 Face example
load(""data/face.rda"")
image(t(faceData)[, nrow(faceData):1])
                                       20/24
 
 Face example - variance explained
svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = ""Singular vector"", ylab = ""Variance explained"")
                                                                                         21/24
 
 Face example - create approximations
svd1 <- svd(scale(faceData))
## Note that %*% is matrix multiplication
# Here svd1$d[1] is a constant
approx1 <- svd1$u[, 1] %*% t(svd1$v[, 1]) * svd1$d[1]
# In these examples we need to make the diagonal matrix out of d
approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5])
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[, 1:10])
                                                                        22/24
 
 Face example - plot approximations
par(mfrow = c(1, 4))
image(t(approx1)[, nrow(approx1):1], main = ""(a)"")
image(t(approx5)[, nrow(approx5):1], main = ""(b)"")
image(t(approx10)[, nrow(approx10):1], main = ""(c)"")
image(t(faceData)[, nrow(faceData):1], main = ""(d)"") ## Original data
                                                                      23/24
 
 Notes and further resources
· Scale matters
· PC's/SV's may mix real patterns
· Can be computationally intensive
· Advanced data analysis from an elementary point of view
· Elements of statistical learning
· Alternatives
    - Factor analysis
    - Independent components analysis
    - Latent semantic analysis
                                                          24/24
"
"./04_ExploratoryAnalysis/lectures/exploratoryGraphs.pdf","Exploratory Graphs
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Why do we use graphs in data analysis?
· To understand data properties
· To find patterns in data
· To suggest modeling strategies
· To ""debug"" analyses
· To communicate results
                                       2/23
 
 Exploratory graphs
· To understand data properties
· To find patterns in data
· To suggest modeling strategies
· To ""debug"" analyses
· To communicate results
                                 3/23
 
 Characteristics of exploratory graphs
· They are made quickly
· A large number are made
· The goal is for personal understanding
· Axes/legends are generally cleaned up (later)
· Color/size are primarily used for information
                                                4/23
 
 Air Pollution in the United States
· The U.S. Environmental Protection Agency (EPA) sets national ambient air quality standards for
  outdoor air pollution
    - U.S. National Ambient Air Quality Standards
· For fine particle pollution (PM2.5), the ""annual mean, averaged over 3 years"" cannot exceed
  12 μg/m3 .
· Data on daily PM2.5 are available from the U.S. EPA web site
    - EPA Air Quality System
· Question: Are there any counties in the U.S. that exceed that national standard for fine particle
  pollution?
                                                                                               5/23
 
 Data
Annual average PM2.5 averaged over the period 2008 through 2010
 pollution <- read.csv(""data/avgpm25.csv"", colClasses = c(""numeric"", ""character"",
     ""factor"", ""numeric"", ""numeric""))
 head(pollution)
 ##     pm25  fips region longitude latitude
 ## 1  9.771 01003   east     -87.75    30.59
 ## 2  9.994 01027   east     -85.84    33.27
 ## 3 10.689 01033   east     -87.73    34.73
 ## 4 11.337 01049   east     -85.80    34.46
 ## 5 12.120 01055   east     -86.03    34.02
 ## 6 10.828 01069   east     -85.35    31.19
Do any counties exceed the standard of 12 μg/m3 ?
                                                                                  6/23
 
 Simple Summaries of Data
One dimension
 · Five-number summary
 · Boxplots
 · Histograms
 · Density plot
 · Barplot
                         7/23
 
 Five Number Summary
summary(pollution$pm25)
##    Min. 1st Qu.  Median Mean 3rd Qu.  Max.
##    3.38    8.55   10.00 9.84   11.40 18.40
                                              8/23
 
 Boxplot
boxplot(pollution$pm25, col = ""blue"")
                                      9/23
 
 Histogram
hist(pollution$pm25, col = ""green"")
                                    10/23
 
 Histogram
hist(pollution$pm25, col = ""green"")
rug(pollution$pm25)
                                    11/23
 
 Histogram
hist(pollution$pm25, col = ""green"", breaks = 100)
rug(pollution$pm25)
                                                  12/23
 
 Overlaying Features
boxplot(pollution$pm25, col = ""blue"")
abline(h = 12)
                                      13/23
 
 Overlaying Features
hist(pollution$pm25, col = ""green"")
abline(v = 12, lwd = 2)
abline(v = median(pollution$pm25), col = ""magenta"", lwd = 4)
                                                             14/23
 
 Barplot
barplot(table(pollution$region), col = ""wheat"", main = ""Number of Counties in Each Region"")
                                                                                         15/23
 
 Simple Summaries of Data
Two dimensions
 · Multiple/overlayed 1-D plots (Lattice/ggplot2)
 · Scatterplots
 · Smooth scatterplots
> 2 dimensions
 · Overlayed/multiple 2-D plots; coplots
 · Use color, size, shape to add dimensions
 · Spinning plots
 · Actual 3-D plots (not that useful)
                                                  16/23
 
 Multiple Boxplots
boxplot(pm25 ~ region, data = pollution, col = ""red"")
                                                      17/23
 
 Multiple Histograms
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
hist(subset(pollution, region == ""east"")$pm25, col = ""green"")
hist(subset(pollution, region == ""west"")$pm25, col = ""green"")
                                                              18/23
 
 Scatterplot
with(pollution, plot(latitude, pm25))
abline(h = 12, lwd = 2, lty = 2)
                                      19/23
 
 Scatterplot - Using Color
with(pollution, plot(latitude, pm25, col = region))
abline(h = 12, lwd = 2, lty = 2)
                                                    20/23
 
 Multiple Scatterplots
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))
with(subset(pollution, region == ""west""), plot(latitude, pm25, main = ""West""))
with(subset(pollution, region == ""east""), plot(latitude, pm25, main = ""East""))
                                                                               21/23
 
 Summary
· Exploratory plots are ""quick and dirty""
· Let you summarize the data (usually graphically) and highlight any broad features
· Explore basic questions and hypotheses (and perhaps rule them out)
· Suggest modeling strategies for the ""next step""
                                                                                    22/23
 
 Further resources
· R Graph Gallery
· R Bloggers
                  23/23
"
"./04_ExploratoryAnalysis/lectures/GraphicsDevices.pdf","Graphics Devices in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is a Graphics Device?
· A graphics device is something where you can make a plot appear
    - A window on your computer (screen device)
    - A PDF file (file device)
    - A PNG or JPEG file (file device)
    - A scalable vector graphics (SVG) file (file device)
· When you make a plot in R, it has to be ""sent"" to a specific graphics device
· The most common place for a plot to be ""sent"" is the screen device
    - On a Mac the screen device is launched with the quartz()
    - On Windows the screen device is launched with windows()
    - On Unix/Linux the screen device is launched with x11()
                                                                               2/10
 
 What is a Graphic Device?
· When making a plot, you need to consider how the plot will be used to determine what device the
  plot should be sent to.
    - The list of devices is found in ?Devices; there are also devices created by users on CRAN
· For quick visualizations and exploratory analysis, usually you want to use the screen device
    - Functions like plot in base, xyplot in lattice, or qplot in ggplot2 will default to sending a
       plot to the screen device
    - On a given platform (Mac, Windows, Unix/Linux) there is only one screen device
· For plots that may be printed out or be incorporated into a document (e.g. papers/reports, slide
  presentations), usually a file device is more appropriate
    - There are many different file devices to choose from
· NOTE: Not all graphics devices are available on all platforms (i.e. you cannot launch the
  windows() on a Mac)
                                                                                                3/10
 
 How Does a Plot Get Created?
There are two basic approaches to plotting. The first is most common:
  1. Call a plotting function like plot, xyplot, or qplot
  2. The plot appears on the screen device
  3. Annotate plot if necessary
  4. Enjoy
 library(datasets)
 with(faithful, plot(eruptions, waiting))      ## Make plot appear on screen device
 title(main = ""Old Faithful Geyser data"")      ## Annotate with a title
                                                                                    4/10
 
 How Does a Plot Get Created?
The second approach to plotting is most commonly used for file devices:
  1. Explicitly launch a graphics device
  2. Call a plotting function to make a plot (Note: if you are using a file device, no plot will appear on
     the screen)
  3. Annotate plot if necessary
  4. Explicitly close graphics device with dev.off() (this is very important!)
 pdf(file = ""myplot.pdf"") ## Open PDF device; create 'myplot.pdf' in my working directory
 ## Create plot and send to a file (no plot appears on screen)
 with(faithful, plot(eruptions, waiting))
 title(main = ""Old Faithful Geyser data"") ## Annotate plot; still nothing on screen
 dev.off() ## Close the PDF file device
 ## Now you can view the file 'myplot.pdf' on your computer
                                                                                                       5/10
 
 Graphics File Devices
There are two basic types of file devices: vector and bitmap devices
Vector formats:
 · pdf: useful for line-type graphics, resizes well, usually portable, not efficient if a plot has many
   objects/points
 · svg: XML-based scalable vector graphics; supports animation and interactivity, potentially useful
   for web-based plots
 · win.metafile: Windows metafile format (only on Windows)
 · postscript: older format, also resizes well, usually portable, can be used to create
   encapsulated postscript files; Windows systems often don’t have a postscript viewer
                                                                                                    6/10
 
 Graphics File Devices
Bitmap formats
 · png: bitmapped format, good for line drawings or images with solid colors, uses lossless
   compression (like the old GIF format), most web browsers can read this format natively, good for
   plotting many many many points, does not resize well
 · jpeg: good for photographs or natural scenes, uses lossy compression, good for plotting many
   many many points, does not resize well, can be read by almost any computer and any web
   browser, not great for line drawings
 · tiff: Creates bitmap files in the TIFF format; supports lossless compression
 · bmp: a native Windows bitmapped format
                                                                                               7/10
 
 Multiple Open Graphics Devices
· It is possible to open multiple graphics devices (screen, file, or both), for example when viewing
  multiple plots at once
· Plotting can only occur on one graphics device at a time
· The currently active graphics device can be found by calling dev.cur()
· Every open graphics device is assigned an integer ≥ 2.
· You can change the active graphics device with dev.set(<integer>) where <integer> is the
  number associated with the graphics device you want to switch to
                                                                                                 8/10
 
 Copying Plots
Copying a plot to another device can be useful because some plots require a lot of code and it can
be a pain to type all that in again for a different device.
 · dev.copy: copy a plot from one device to another
 · dev.copy2pdf: specifically copy a plot to a PDF file
NOTE: Copying a plot is not an exact operation, so the result may not be identical to the original.
 library(datasets)
 with(faithful, plot(eruptions, waiting)) ## Create plot on screen device
 title(main = ""Old Faithful Geyser data"") ## Add a main title
 dev.copy(png, file = ""geyserplot.png"") ## Copy my plot to a PNG file
 dev.off() ## Don't forget to close the PNG device!
                                                                                                    9/10
 
 Summary
· Plots must be created on a graphics device
· The default graphics device is almost always the screen device, which is most useful for
  exploratory analysis
· File devices are useful for creating plots that can be included in other documents or sent to other
  people
· For file devices, there are vector and bitmap formats
     - Vector formats are good for line drawings and plots with solid colors using a modest number
       of points
     - Bitmap formats are good for plots with a large number of points, natural scenes or web-
       based plots
                                                                                                 10/10
"
"./04_ExploratoryAnalysis/lectures/hierachicalClustering.pdf","Hierarchical Clustering
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Can we find things that are close together?
Clustering organizes things that are close into groups
 · How do we define close?
 · How do we group things?
 · How do we visualize the grouping?
 · How do we interpret the grouping?
                                                       2/21
 
 Hugely important/impactful
http://scholar.google.com/scholar?hl=en&q=cluster+analysis&btnG=&as_sdt=1%2C21&as_sdtp=
                                                                                        3/21
 
 Hierarchical clustering
· An agglomerative approach
    - Find closest two things
    - Put them together
    - Find next closest
· Requires
    - A defined distance
    - A merging approach
· Produces
    - A tree showing how close things are to each other
                                                        4/21
 
 How do we define close?
· Most important step
    - Garbage in -> garbage out
· Distance or similarity
    - Continuous - euclidean distance
    - Continuous - correlation similarity
    - Binary - manhattan distance
· Pick a distance/similarity that makes sense for your problem
                                                               5/21
 
 Example distances - Euclidean
http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf
                                                         6/21
 
 Example distances - Euclidean
In general:
                                ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾    2‾
                             √(A 1 − A 2 ) + (B 1 − B 2 ) + … + (Z1 − Z2 )
                                          2              2
http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf
                                                                              7/21
 
 Example distances - Manhattan
In general:
                                  |A 1 − A 2 | + |B 1 − B 2 | + … + |Z1 − Z2 |
http://en.wikipedia.org/wiki/Taxicab_geometry
                                                                               8/21
 
 Hierarchical clustering - example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = ""blue"", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
                                                           9/21
 
 Hierarchical clustering - dist
· Important parameters: x,method
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)
##          1       2       3       4       5       6       7       8       9
## 2 0.34121
## 3 0.57494  0.24103
## 4 0.26382  0.52579 0.71862
## 5 1.69425  1.35818 1.11953 1.80667
## 6 1.65813  1.31960 1.08339 1.78081 0.08150
## 7 1.49823  1.16621 0.92569 1.60132 0.21110 0.21667
## 8 1.99149  1.69093 1.45649 2.02849 0.61704 0.69792 0.65063
## 9 2.13630  1.83168 1.67836 2.35676 1.18350 1.11500 1.28583 1.76461
## 10 2.06420 1.76999 1.63110 2.29239 1.23848 1.16550 1.32063 1.83518 0.14090
## 11 2.14702 1.85183 1.71074 2.37462 1.28154 1.21077 1.37370 1.86999 0.11624
## 12 2.05664 1.74663 1.58659 2.27232 1.07701 1.00777 1.17740 1.66224 0.10849
##         10      11
## 2
## 3
## 4
## 5                                                                          10/21
 
 Hierarchical clustering - #1
                             11/21
 
 Hierarchical clustering - #2
                             12/21
 
 Hierarchical clustering - #3
                             13/21
 
 Hierarchical clustering - hclust
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
                                      14/21
 
 Prettier dendrograms
myplclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)),
    hang = 0.1, ...) {
    ## modifiction of plclust for plotting hclust objects *in colour*! Copyright
    ## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector
    ## of labels of the leaves of the tree lab.col: colour for the labels;
    ## NA=default device foreground colour hang: as in hclust & plclust Side
    ## effect: A display of hierarchical cluster with coloured leaf labels.
    y <- rep(hclust$height, 2)
    x <- as.numeric(hclust$merge)
    y <- y[which(x < 0)]
    x <- x[which(x < 0)]
    x <- abs(x)
    y <- y[order(x)]
    x <- x[order(x)]
    plot(hclust, labels = FALSE, hang = hang, ...)
    text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
        col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)
}
                                                                                         15/21
 
 Pretty dendrograms
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
                                                                               16/21
 
 Even Prettier dendrograms
http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79
                                                            17/21
 
 Merging points - complete
                          18/21
 
 Merging points - average
                         19/21
 
 heatmap()
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)
                                                   20/21
 
 Notes and further resources
· Gives an idea of the relationships between variables/observations
· The picture may be unstable
    - Change a few points
    - Have different missing values
    - Pick a different distance
    - Change the merging strategy
    - Change the scale of points for one variable
· But it is deterministic
· Choosing where to cut isn't always obvious
· Should be primarily used for exploration
· Rafa's Distances and Clustering Video
· Elements of statistical learning
                                                                    21/21
"
"./04_ExploratoryAnalysis/lectures/kmeansClustering.pdf","K-means Clustering
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Can we find things that are close together?
· How do we define close?
· How do we group things?
· How do we visualize the grouping?
· How do we interpret the grouping?
                                            2/14
 
 How do we define close?
· Most important step
    - Garbage in ⟶ garbage out
· Distance or similarity
    - Continuous - euclidean distance
    - Continous - correlation similarity
    - Binary - manhattan distance
· Pick a distance/similarity that makes sense for your problem
                                                               3/14
 
 K-means clustering
· A partioning approach
    - Fix a number of clusters
    - Get ""centroids"" of each cluster
    - Assign things to closest centroid
    - Reclaculate centroids
· Requires
    - A defined distance metric
    - A number of clusters
    - An initial guess as to cluster centroids
· Produces
    - Final estimate of cluster centroids
    - An assignment of each point to clusters
                                               4/14
 
 K-means clustering - example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = ""blue"", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
                                                           5/14
 
 K-means clustering - starting centroids
                                        6/14
 
 K-means clustering - assign to closest
centroid
                                       7/14
 
 K-means clustering - recalculate centroids
                                           8/14
 
 K-means clustering - reassign values
                                     9/14
 
 K-means clustering - update centroids
                                      10/14
 
 kmeans()
· Important parameters: x, centers, iter.max, nstart
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
## [1] ""cluster""      ""centers""          ""totss""     ""withinss""
## [5] ""tot.withinss"" ""betweenss""        ""size""      ""iter""
## [9] ""ifault""
kmeansObj$cluster
##  [1] 3 3 3 3 1 1 1 1 2 2 2 2
                                                                11/14
 
 kmeans()
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
                                                                12/14
 
 Heatmaps
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = ""n"")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = ""n"")
                                                             13/14
 
 Notes and further resources
· K-means requires a number of clusters
    - Pick by eye/intuition
    - Pick by cross validation/information theory, etc.
    - Determining the number of clusters
· K-means is not deterministic
    - Different # of clusters
    - Different number of iterations
· Rafael Irizarry's Distances and Clustering Video
· Elements of statistical learning
                                                        14/14
"
"./04_ExploratoryAnalysis/lectures/PlottingBase.pdf","The Base Plotting System in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Plotting System
The core plotting and graphics engine in R is encapsulated in the following packages:
 · graphics: contains plotting functions for the ""base"" graphing systems, including plot, hist,
   boxplot and many others.
 · grDevices: contains all the code implementing the various graphics devices, including X11, PDF,
   PostScript, PNG, etc.
The lattice plotting system is implemented using the following packages:
 · lattice: contains code for producing Trellis graphics, which are independent of the “base” graphics
   system; includes functions like xyplot, bwplot, levelplot
 · grid: implements a different graphing system independent of the “base” system; the lattice
   package builds on top of grid; we seldom call functions from the grid package directly
                                                                                                  2/20
 
 The Process of Making a Plot
When making a plot one must first make a few considerations (not necessarily in this order):
· Where will the plot be made? On the screen? In a file?
· How will the plot be used?
     - Is the plot for viewing temporarily on the screen?
     - Will it be presented in a web browser?
     - Will it eventually end up in a paper that might be printed?
     - Are you using it in a presentation?
· Is there a large amount of data going into the plot? Or is it just a few points?
· Do you need to be able to dynamically resize the graphic?
                                                                                             3/20
 
 The Process of Making a Plot
· What graphics system will you use: base, lattice, or ggplot2? These generally cannot be mixed.
· Base graphics are usually constructed piecemeal, with each aspect of the plot handled
  separately through a series of function calls; this is often conceptually simpler and allows plotting
  to mirror the thought process
· Lattice graphics are usually created in a single function call, so all of the graphics parameters
  have to specified at once; specifying everything at once allows R to automatically calculate the
  necessary spacings and font sizes.
· ggplot2 combines concepts from both base and lattice graphics but uses an independent
  implementation
We focus on using the base plotting system to create graphics on the screen device.
                                                                                                   4/20
 
 Base Graphics
Base graphics are used most commonly and are a very powerful system for creating 2-D graphics.
 · There are two phases to creating a base plot
      - Initializing a new plot
      - Annotating (adding to) an existing plot
 · Calling plot(x, y) or hist(x) will launch a graphics device (if one is not already open) and
   draw a new plot on the device
 · If the arguments to plot are not of some special class, then the default method for plot is
   called; this function has many arguments, letting you set the title, x axis label, y axis label, etc.
 · The base graphics system has many parameters that can set and tweaked; these parameters are
   documented in ?par; it wouldn’t hurt to try to memorize this help page!
                                                                                                         5/20
 
 Simple Base Graphics: Histogram
library(datasets)
hist(airquality$Ozone) ## Draw a new plot
                                          6/20
 
 Simple Base Graphics: Scatterplot
library(datasets)
with(airquality, plot(Wind, Ozone))
                                    7/20
 
 Simple Base Graphics: Boxplot
library(datasets)
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality, xlab = ""Month"", ylab = ""Ozone (ppb)"")
                                                                         8/20
 
 Some Important Base Graphics Parameters
Many base plotting functions share a set of parameters. Here are a few key ones:
· pch: the plotting symbol (default is open circle)
· lty: the line type (default is solid line), can be dashed, dotted, etc.
· lwd: the line width, specified as an integer multiple
· col: the plotting color, specified as a number, string, or hex code; the colors() function gives
  you a vector of colors by name
· xlab: character string for the x-axis label
· ylab: character string for the y-axis label
                                                                                               9/20
 
 Some Important Base Graphics Parameters
The par() function is used to specify global graphics parameters that affect all plots in an R
session. These parameters can be overridden when specified as arguments to specific plotting
functions.
 · las: the orientation of the axis labels on the plot
 · bg: the background color
 · mar: the margin size
 · oma: the outer margin size (default is 0 for all sides)
 · mfrow: number of plots per row, column (plots are filled row-wise)
 · mfcol: number of plots per row, column (plots are filled column-wise)
                                                                                           10/20
 
 Some Important Base Graphics Parameters
Default values for global graphics parameters
 par(""lty"")
 ## [1] ""solid""
 par(""col"")
 ## [1] ""black""
 par(""pch"")
 ## [1] 1
                                              11/20
 
 Some Important Base Graphics Parameters
Default values for global graphics parameters
 par(""bg"")
 ## [1] ""transparent""
 par(""mar"")
 ## [1] 5.1 4.1 4.1 2.1
 par(""mfrow"")
 ## [1] 1 1
                                              12/20
 
 Base Plotting Functions
· plot: make a scatterplot, or other type of plot depending on the class of the object being plotted
· lines: add lines to a plot, given a vector x values and a corresponding vector of y values (or a 2-
  column matrix); this function just connects the dots
· points: add points to a plot
· text: add text labels to a plot using specified x, y coordinates
· title: add annotations to x, y axis labels, title, subtitle, outer margin
· mtext: add arbitrary text to the margins (inner or outer) of the plot
· axis: adding axis ticks/labels
                                                                                                 13/20
 
 Base Plot with Annotation
library(datasets)
with(airquality, plot(Wind, Ozone))
title(main = ""Ozone and Wind in New York City"") ## Add a title
                                                               14/20
 
 Base Plot with Annotation
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City""))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = ""blue""))
                                                                              15/20
 
 Base Plot with Annotation
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City"",
    type = ""n""))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = ""blue""))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = ""red""))
legend(""topright"", pch = 1, col = c(""blue"", ""red""), legend = c(""May"", ""Other Months""))
                                                                                       16/20
 
 Base Plot with Regression Line
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City"",
    pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)
                                                                             17/20
 
 Multiple Base Plots
par(mfrow = c(1, 2))
with(airquality, {
    plot(Wind, Ozone, main = ""Ozone and Wind"")
    plot(Solar.R, Ozone, main = ""Ozone and Solar Radiation"")
})
                                                             18/20
 
 Multiple Base Plots
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
with(airquality, {
    plot(Wind, Ozone, main = ""Ozone and Wind"")
    plot(Solar.R, Ozone, main = ""Ozone and Solar Radiation"")
    plot(Temp, Ozone, main = ""Ozone and Temperature"")
    mtext(""Ozone and Weather in New York City"", outer = TRUE)
})
                                                               19/20
 
 Summary
· Plots in the base plotting system are created by calling successive R functions to ""build up"" a plot
· Plotting occurs in two stages:
    - Creation of a plot
    - Annotation of a plot (adding lines, points, text, legends)
· The base plotting system is very flexible and offers a high degree of control over plotting
                                                                                                  20/20
"
"./04_ExploratoryAnalysis/lectures/PlottingLattice.pdf","The Lattice Plotting System in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 The Lattice Plotting System
The lattice plotting system is implemented using the following packages:
 · lattice: contains code for producing Trellis graphics, which are independent of the “base” graphics
   system; includes functions like xyplot, bwplot, levelplot
 · grid: implements a different graphing system independent of the “base” system; the lattice
   package builds on top of grid
      - We seldom call functions from the grid package directly
 · The lattice plotting system does not have a ""two-phase"" aspect with separate plotting and
   annotation like in base plotting
 · All plotting/annotation is done at once with a single function call
                                                                                                  2/15
 
 Lattice Functions
· xyplot: this is the main function for creating scatterplots
· bwplot: box-and-whiskers plots (“boxplots”)
· histogram: histograms
· stripplot: like a boxplot but with actual points
· dotplot: plot dots on ""violin strings""
· splom: scatterplot matrix; like pairs in base plotting system
· levelplot, contourplot: for plotting ""image"" data
                                                                3/15
 
 Lattice Functions
Lattice functions generally take a formula for their first argument, usually of the form
 xyplot(y ~ x | f * g, data)
 · We use the formula notation here, hence the ~.
 · On the left of the ~ is the y-axis variable, on the right is the x-axis variable
 · f and g are conditioning variables — they are optional
      - the * indicates an interaction between two variables
 · The second argument is the data frame or list from which the variables in the formula should be
   looked up
      - If no data frame or list is passed, then the parent frame is used.
 · If no other arguments are passed, there are defaults that can be used.
                                                                                               4/15
 
 Simple Lattice Plot
library(lattice)
library(datasets)
## Simple scatterplot
xyplot(Ozone ~ Wind, data = airquality)
                                        5/15
 
 Simple Lattice Plot
library(datasets)
library(lattice)
## Convert 'Month' to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
                                                                  6/15
 
 Lattice Behavior
Lattice functions behave differently from base graphics functions in one critical way.
 · Base graphics functions plot data directly to the graphics device (screen, PDF file, etc.)
 · Lattice graphics functions return an object of class trellis
 · The print methods for lattice functions actually do the work of plotting the data on the graphics
   device.
 · Lattice functions return ""plot objects"" that can, in principle, be stored (but it’s usually better to just
   save the code + data).
 · On the command line, trellis objects are auto-printed so that it appears the function is plotting the
   data
                                                                                                         7/15
 
 Lattice Behavior
p <- xyplot(Ozone ~ Wind, data = airquality)  ## Nothing happens!
print(p) ## Plot appears
xyplot(Ozone ~ Wind, data = airquality)  ## Auto-printing
                                                                  8/15
 
 Lattice Panel Functions
· Lattice functions have a panel function which controls what happens inside each panel of the
  plot.
· The lattice package comes with default panel functions, but you can supply your own if you want
  to customize what happens in each panel
· Panel functions receive the x/y coordinates of the data points in their panel (along with any
  optional arguments)
                                                                                              9/15
 
 Lattice Panel Functions
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c(""Group 1"", ""Group 2""))
xyplot(y ~ x | f, layout = c(2, 1)) ## Plot with 2 panels
                                                          10/15
 
 Lattice Panel Functions
## Custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
    panel.xyplot(x, y, ...) ## First call the default panel function for 'xyplot'
    panel.abline(h = median(y), lty = 2) ## Add a horizontal line at the median
})
                                                                                  11/15
 
 Lattice Panel Functions: Regression line
## Custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
    panel.xyplot(x, y, ...) ## First call default panel function
    panel.lmline(x, y, col = 2) ## Overlay a simple linear regression line
})
                                                                           12/15
 
 Many Panel Lattice Plot: Example from
MAACS
 · Study: Mouse Allergen and Asthma Cohort Study (MAACS)
 · Study subjects: Children with asthma living in Baltimore City, many allergic to mouse allergen
 · Design: Observational study, baseline home visit + every 3 months for a year.
 · Question: How does indoor airborne mouse allergen vary over time and across subjects?
Ahluwalia et al., Journal of Allergy and Clinical Immunology, 2013
                                                                                                  13/15
 
 Many Panel Lattice Plot
                        14/15
 
 Summary
· Lattice plots are constructed with a single function call to a core lattice function (e.g. xyplot)
· Aspects like margins and spacing are automatically handled and defaults are usually sufficient
· The lattice system is ideal for creating conditioning plots where you examine the same kind of
  plot under many different conditions
· Panel functions can be specified/customized to modify what is plotted in each of the plot panels
                                                                                                     15/15
"
"./04_ExploratoryAnalysis/lectures/PlottingMath.pdf"," 
  
  
  
 "
"./04_ExploratoryAnalysis/lectures/PlottingSystems.pdf","Plotting Systems in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 The Base Plotting System
· ""Artist's palette"" model
· Start with blank canvas and build up from there
· Start with plot function (or similar)
· Use annotation functions to add/modify (text, lines, points, axis)
                                                                     2/11
 
 The Base Plotting System
· Convenient, mirrors how we think of building plots and analyzing data
· Can’t go back once plot has started (i.e. to adjust margins); need to plan in advance
· Difficult to ""translate"" to others once a new plot has been created (no graphical ""language"")
· Plot is just a series of R commands
                                                                                                3/11
 
 Base Plot
library(datasets)
data(cars)
with(cars, plot(speed, dist))
                              4/11
 
 The Lattice System
· Plots are created with a single function call (xyplot, bwplot, etc.)
· Most useful for conditioning types of plots: Looking at how y changes with x across levels of z
· Things like margins/spacing set automatically because entire plot is specified at once
· Good for puttng many many plots on a screen
                                                                                                  5/11
 
 The Lattice System
· Sometimes awkward to specify an entire plot in a single function call
· Annotation in plot is not especially intuitive
· Use of panel functions and subscripts difficult to wield and requires intense preparation
· Cannot ""add"" to the plot once it is created
                                                                                            6/11
 
 Lattice Plot
library(lattice)
state <- data.frame(state.x77, region = state.region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))
                                                                   7/11
 
 The ggplot2 System
· Splits the difference between base and lattice in a number of ways
· Automatically deals with spacings, text, titles but also allows you to annotate by ""adding"" to a plot
· Superficial similarity to lattice but generally easier/more intuitive to use
· Default mode makes many choices for you (but you can still customize to your heart's desire)
                                                                                                    8/11
 
 ggplot2 Plot
library(ggplot2)
data(mpg)
qplot(displ, hwy, data = mpg)
                              9/11
 
 Summary
· Base: ""artist's palette"" model
· Lattice: Entire plot specified by one function; conditioning
· ggplot2: Mixes elements of Base and Lattice
                                                               10/11
 
 References
Paul Murrell (2011). R Graphics, CRC Press.
Hadley Wickham (2009). ggplot2, Springer.
                                            11/11
"
"./04_ExploratoryAnalysis/lectures/Principles.pdf","Principles of Analytic Graphics
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
                                                                                    2/16
 
 Show Comparisons
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   3/16
 
 Show Comparisons
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   4/16
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
· Principle 2: Show causality, mechanism, explanation, systematic structure
    - What is your causal framework for thinking about a question?
                                                                                    5/16
 
 Show causality, mechanism
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   6/16
 
 Show causality, mechanism
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   7/16
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
· Principle 2: Show causality, mechanism, explanation, systematic structure
    - What is your causal framework for thinking about a question?
· Principle 3: Show multivariate data
    - Multivariate = more than 2 variables
    - The real world is multivariate
    - Need to ""escape flatland""
                                                                                    8/16
 
 Show Multivariate Data
                       9/16
 
 Show Multivariate Data
                       10/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
                                                                       11/16
 
 Integrate Different Modes of Evidence
                                      12/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
· Principle 5: Describe and document the evidence with appropriate labels, scales, sources, etc.
    - A data graphic should tell a complete story that is credible
                                                                                               13/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
· Principle 5: Describe and document the evidence with appropriate labels, scales, sources, etc.
    - A data graphic should tell a complete story that is credible
· Principle 6: Content is king
    - Analytical presentations ultimately stand or fall depending on the quality, relevance, and
      integrity of their content
                                                                                               14/16
 
 Summary
· Principle 1: Show comparisons
· Principle 2: Show causality, mechanism, explanation
· Principle 3: Show multivariate data
· Principle 4: Integrate multiple modes of evidence
· Principle 5: Describe and document the evidence
· Principle 6: Content is king
                                                      15/16
 
 References
Edward Tufte (2006). Beautiful Evidence, Graphics Press LLC. www.edwardtufte.com
                                                                                 16/16
"
"./04_ExploratoryAnalysis/lectures/RColors.pdf","Plotting and Color in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Plotting and Color
· The default color schemes for most plots in R are horrendous
    - I don’t have good taste and even I know that
· Recently there have been developments to improve the handling/specifica1on of colors in
  plots/graphs/etc.
· There are functions in R and in external packages that are very handy
                                                                                      2/18
 
 Colors 1, 2, and 3
                   3/18
 
 Default Image Plots in R
                         4/18
 
 Color U1li1es in R
· The grDevices package has two functions
    - colorRamp
    - colorRampPalette
· These functions take palettes of colors and help to interpolate between the colors
· The function colors() lists the names of colors you can use in any plotting function
                                                                                       5/18
 
 Color Palette Utilities in R
· colorRamp: Take a palette of colors and return a function that takes valeus between 0 and 1,
  indicating the extremes of the color palette (e.g. see the 'gray' function)
· colorRampPalette: Take a palette of colors and return a function that takes integer arguments
  and returns a vector of colors interpolating the palette (like heat.colors or topo.colors)
                                                                                             6/18
 
 colorRamp
[,1] [,2] [,3] corresponds to [Red] [Blue] [Green]
 > pal <- colorRamp(c(""red"", ""blue""))
 > pal(0)
      [,1] [,2] [,3]
 [1,] 255     0    0
 > pal(1)
      [,1] [,2] [,3]
 [1,]    0    0 255
 > pal(0.5)
       [,1] [,2] [,3]
 [1,] 127.5    0 127.5
                                                   7/18
 
 colorRamp
> pal(seq(0, 1, len = 10))
                  [,1] [,2]      [,3]
        [1,] 255.00000    0         0
        [2,] 226.66667    0  28.33333
        [3,] 198.33333    0  56.66667
        [4,] 170.00000    0  85.00000
        [5,] 141.66667    0 113.33333
        [6,] 113.33333    0 141.66667
        [7,] 85.00000     0 170.00000
        [8,] 56.66667     0 198.33333
        [9,] 28.33333     0 226.66667
        [10,] 0.00000     0 255.00000
                                      8/18
 
 colorRampPalette
> pal <- colorRampPalette(c(""red"", ""yellow""))
> pal(2)
[1] ""#FF0000"" ""#FFFF00""
> pal(10)
 [1] ""#FF0000"" ""#FF1C00"" ""#FF3800"" ""#FF5500"" ""#FF7100""
 [6] ""#FF8D00"" ""#FFAA00"" ""#FFC600"" ""#FFE200"" ""#FFFF00”
                                                       9/18
 
 RColorBrewer Package
· One package on CRAN that contains interes1ng/useful color palettes
· There are 3 types of palettes
    - Sequential
    - Diverging
    - Qualitative
· Palette  informa1on     can   be used    in  conjunction    with   the colorRamp()  and
  colorRampPalette()
                                                                                     10/18
 
 11/18 
 RColorBrewer and colorRampPalette
> library(RColorBrewer)
> cols <- brewer.pal(3, ""BuGn"")
> cols
[1] ""#E5F5F9"" ""#99D8C9"" ""#2CA25F""
> pal <- colorRampPalette(cols)
> image(volcano, col = pal(20))
                                  12/18
 
 RColorBrewer and colorRampPalette
                                  13/18
 
 The smoothScatter function
                           14/18
 
 Some other plotting notes
· The rgb function can be used to produce any color via red, green, blue proportions
· Color transparency can be added via the alpha parameter to rgb
· The colorspace package can be used for a different control over colors
                                                                                     15/18
 
 Scatterplot with no transparency
                                 16/18
 
 Scatterplot with transparency
                              17/18
 
 Summary
· Careful use of colors in plots/maps/etc. can make it easier for the reader to get what you're trying
  to say (why make it harder?)
· The RColorBrewer package is an R package that provides color palettes for sequential,
  categorical, and diverging data
· The colorRamp and colorRampPalette functions can be used in conjunction with color
  palettes to connect data to colors
· Transparency can sometimes be used to clarify plots with many points
                                                                                                  18/18
"
"./04_ExploratoryAnalysis/old/001exploratoryGraphs1/Exploratory graphs.pdf","8/28/13                                                                                                  Exploratory graphs
                            Exploratory graphs
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                  1/20
 
 8/28/13                                                                                                  Exploratory graphs
              Why do we use graphs in data analysis?
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                            2/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       2/20
 
 8/28/13                                                                                                  Exploratory graphs
              Exploratory graphs
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                            3/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       3/20
 
 8/28/13                                                                                                  Exploratory graphs
              Characteristics of exploratory graphs
                 · They are made quickly
                 · A large number are made
                 · The goal is for personal understanding
                 · Axes/legends are generally cleaned up
                 · Color/size are primarily used for information
                                                                                                                            4/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       4/20
 
 8/28/13                                                                                                  Exploratory graphs
              Background - perceptual tasks
              Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical
              Models
                                                                                                                            5/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       5/20
 
 8/28/13                                                                                                  Exploratory graphs
              Position versus length
              Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical
              Models
                                                                                                                            6/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       6/20
 
 8/28/13                                                                                                  Exploratory graphs
              Position versus length - results
              Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical
              Models
                                                                                                                            7/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       7/20
 
 8/28/13                                                                                                  Exploratory graphs
              Position versus angle
              Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical
              Models
                                                                                                                            8/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       8/20
 
 8/28/13                                                                                                  Exploratory graphs
              Position versus angle - results
              Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical
              Models
                                                                                                                            9/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                       9/20
 
 8/28/13                                                                                                  Exploratory graphs
              More experimental results
              Graphical Perception and Graphical Methods for Analyzing Scientific Data
                                                                                                                            10/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        10/20
 
 8/28/13                                                                                                  Exploratory graphs
              Summary
                 · Use common scales when possible
                 · When possible use position comparisons
                 · Angle comparisons are frequently hard to interpret (no piecharts!)
                 · No 3-D barcharts
                                                                                                                            11/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        11/20
 
 8/28/13                                                                                                  Exploratory graphs
              Housing data
                 pData <- read.csv(""./data/ss06pid.csv"")
                                                                                                                            12/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        12/20
 
 8/28/13                                                                                                  Exploratory graphs
              Boxplots
                 · Important parameters: col,varwidth,names,horizontal
                 boxplot(pData$AGEP,col=""blue"")
                                                                                                                            13/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        13/20
 
 8/28/13                                                                                                  Exploratory graphs
              Boxplots
                 boxplot(pData$AGEP ~ as.factor(pData$DDRS),col=""blue"")
                                                                                                                            14/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        14/20
 
 8/28/13                                                                                                  Exploratory graphs
              Boxplots
                 boxplot(pData$AGEP ~ as.factor(pData$DDRS),col=c(""blue"",""orange""),names=c(""yes"",""no""),varwidth=TRUE)
                                                                                                                            15/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        15/20
 
 8/28/13                                                                                                  Exploratory graphs
              Barplots
                 barplot(table(pData$CIT),col=""blue"")
                                                                                                                            16/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        16/20
 
 8/28/13                                                                                                  Exploratory graphs
              Histograms
                 · Important parameters: breaks,freq,col,xlab,ylab, xlim, _ylim ,main
                 hist(pData$AGEP,col=""blue"")
                                                                                                                            17/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        17/20
 
 8/28/13                                                                                                  Exploratory graphs
              Histograms
                 hist(pData$AGEP,col=""blue"",breaks=100,main=""Age"")
                                                                                                                            18/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        18/20
 
 8/28/13                                                                                                  Exploratory graphs
              Density plots
              Important parameters (to plot): col,lwd,xlab,ylab,xlim,ylim
                 dens <- density(pData$AGEP)
                 plot(dens,lwd=3,col=""blue"")
                                                                                                                            19/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        19/20
 
 8/28/13                                                                                                  Exploratory graphs
              Density plots - multiple distributions
                 dens <- density(pData$AGEP)
                 densMales <- density(pData$AGEP[which(pData$SEX==1)])
                 plot(dens,lwd=3,col=""blue"")
                 lines(densMales,lwd=3,col=""orange"")
                                                                                                                            20/20
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/001exploratoryGraphs1/index.html#1                        20/20
"
"./04_ExploratoryAnalysis/old/002exploratoryGraphs2/Exploratory graphs.pdf","8/28/13                                                                                                  Exploratory graphs
                            Exploratory graphs
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                  1/23
 
 8/28/13                                                                                                  Exploratory graphs
              Why do we use graphs in data analysis?
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                            2/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       2/23
 
 8/28/13                                                                                                  Exploratory graphs
              Exploratory graphs
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                            3/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       3/23
 
 8/28/13                                                                                                  Exploratory graphs
              Characteristics of exploratory graphs
                 · They are made quickly
                 · A large number are made
                 · The goal is for personal understanding
                 · Axes/legends are generally not cleaned up
                 · Color/size are primarily used for information
                                                                                                                            4/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       4/23
 
 8/28/13                                                                                                  Exploratory graphs
              Housing data
                 pData <- read.csv(""./data/ss06pid.csv"")
                                                                                                                            5/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       5/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots
                 · Important paramters: x,y,type,xlab,ylab,xlim,ylim,cex,col,bg
                 · See ?par for more
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"")
                                                                                                                            6/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       6/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots - size matters
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=0.5)
                                                                                                                            7/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       7/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots - using color
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=pData$SEX,cex=0.5)
                                                                                                                            8/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       8/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots - using size
                 percentMaxAge <- pData$AGEP/max(pData$AGEP)
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=percentMaxAge*0.5)
                                                                                                                            9/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                       9/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots - overlaying lines/points
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=0.5)
                 lines(rep(100,dim(pData)[1]),pData$WAGP,col=""grey"",lwd=5)
                 points(seq(0,200,length=100),seq(0,20e5,length=100),col=""red"",pch=19)
                                                                                                                            10/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        10/23
 
 8/28/13                                                                                                  Exploratory graphs
              Scatterplots - numeric variables as factors
                 library(Hmisc)
                 ageGroups <- cut2(pData$AGEP,g=5)
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=ageGroups,cex=0.5)
                                                                                                                            11/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        11/23
 
 8/28/13                                                                                                  Exploratory graphs
              If you have a lot of points
                 x <- rnorm(1e5)
                 y <- rnorm(1e5)
                 plot(x,y,pch=19)
                                                                                                                            12/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        12/23
 
 8/28/13                                                                                                  Exploratory graphs
              If you have a lot of points - sampling
                 x <- rnorm(1e5)
                 y <- rnorm(1e5)
                 sampledValues <- sample(1:1e5,size=1000,replace=FALSE)
                 plot(x[sampledValues],y[sampledValues],pch=19)
                                                                                                                            13/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        13/23
 
 8/28/13                                                                                                  Exploratory graphs
              If you have a lot of points - smoothScatter
                 x <- rnorm(1e5)
                 y <- rnorm(1e5)
                 smoothScatter(x,y)
                                                                                                                            14/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        14/23
 
 8/28/13                                                                                                  Exploratory graphs
              If you have a lot of points - hexbin {hexbin}
                 library(hexbin)
                 x <- rnorm(1e5)
                 y <- rnorm(1e5)
                 hbo <- hexbin(x,y)
                 plot(hbo)
                                                                                                                            15/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        15/23
 
 8/28/13                                                                                                  Exploratory graphs
              QQ-plots
                 · Important parameters: x,y
                 x <- rnorm(20); y <- rnorm(20)
                 qqplot(x,y)
                 abline(c(0,1))
                                                                                                                            16/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        16/23
 
 8/28/13                                                                                                  Exploratory graphs
              Matplot and spaghetti
                 · Important paramters: x, y, lty,lwd,pch,col
                 X <- matrix(rnorm(20*5),nrow=20)
                 matplot(X,type=""b"")
                                                                                                                            17/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        17/23
 
 8/28/13                                                                                                  Exploratory graphs
              Heatmaps
                 · Important paramters: x,y,z,col
                 image(1:10,161:236,as.matrix(pData[1:10,161:236]))
                                                                                                                            18/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        18/23
 
 8/28/13                                                                                                  Exploratory graphs
              Heatmaps - matching intuition
                 newMatrix <- as.matrix(pData[1:10,161:236])
                 newMatrix <- t(newMatrix)[,nrow(newMatrix):1]
                 image(161:236, 1:10, newMatrix)
                                                                                                                            19/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        19/23
 
 8/28/13                                                                                                  Exploratory graphs
              Maps - very basics
              You make need to run install.packages(""maps"")if you don't have the mapspackage installed.
                 library(maps)
                 map(""world"")
                 lat <- runif(40,-180,180); lon <- runif(40,-90,90)
                 points(lat,lon,col=""blue"",pch=19)
                                                                                                                            20/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        20/23
 
 8/28/13                                                                                                  Exploratory graphs
              Missing values and plots
                 x <- c(NA,NA,NA,4,5,6,7,8,9,10)
                 y <- 1:10
                 plot(x,y,pch=19,xlim=c(0,11),ylim=c(0,11))
                                                                                                                            21/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        21/23
 
 8/28/13                                                                                                  Exploratory graphs
              Missing values and plots
                 x <- rnorm(100)
                 y <- rnorm(100)
                 y[x < 0] <- NA
                 boxplot(x ~ is.na(y))
                                                                                                                            22/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        22/23
 
 8/28/13                                                                                                  Exploratory graphs
              Further resources
                 · R Graph Gallery
                 · ggplot2,ggplot2 basic introduction
                 · lattice package,lattice introduction
                 · R bloggers
                                                                                                                            23/23
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/002exploratoryGraphs2/index.html#1                        23/23
"
"./04_ExploratoryAnalysis/old/003expositoryGraphs/Expository graphs.pdf","8/28/13                                                                                                  Expository graphs
                            Expository graphs
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                   1/22
 
 8/28/13                                                                                                  Expository graphs
              Why do we use graphs in data analysis?
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                           2/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        2/22
 
 8/28/13                                                                                                  Expository graphs
              Expository graphs
                 · To understand data properties
                 · To find patterns in data
                 · To suggest modeling strategies
                 · To ""debug"" analyses
                 · To communicate results
                                                                                                                           3/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        3/22
 
 8/28/13                                                                                                  Expository graphs
              Characteristics of expository graphs
                 · The goal is to communicate information
                 · Information density is generally good
                 · Color/size are used both for aesthetics and communication
                 · Expository figures have understandable axes, titles, and legends
                                                                                                                           4/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        4/22
 
 8/28/13                                                                                                  Expository graphs
              Housing data
                 pData <- read.csv(""./data/ss06pid.csv"")
                                                                                                                           5/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        5/22
 
 8/28/13                                                                                                  Expository graphs
              Axes
              Important parameters: xlab,ylab,cex.lab,cex.axis
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=0.5,
                            xlab=""Travel time (min)"",ylab=""Last 12 month wages (dollars)"")
                                                                                                                           6/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        6/22
 
 8/28/13                                                                                                  Expository graphs
              Axes
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=0.5,
                            xlab=""Travel time (min)"",ylab=""Last 12 month wages (dollars)"",cex.lab=2,cex.axis=1.5)
                                                                                                                           7/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        7/22
 
 8/28/13                                                                                                  Expository graphs
              Legends
                 · Important paramters: x,y,legend, other plotting parameters
                 plot(pData$JWMNP,pData$WAGP,pch=19,col=""blue"",cex=0.5,xlab=""TT (min)"",ylab=""Wages (dollars)"")
                 legend(100,200000,legend=""All surveyed"",col=""blue"",pch=19,cex=0.5)
                                                                                                                           8/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        8/22
 
 8/28/13                                                                                                  Expository graphs
              Legends
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""TT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                                                                                                                           9/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                        9/22
 
 8/28/13                                                                                                  Expository graphs
              Titles
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",
                            ylab=""Wages (dollars)"",col=pData$SEX,main=""Wages earned versus commute time"")
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                                                                                                                           10/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         10/22
 
 8/28/13                                                                                                  Expository graphs
              Multiple panels
                 par(mfrow=c(1,2))
                 hist(pData$JWMNP,xlab=""CT (min)"",col=""blue"",breaks=100,main="""")
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                                                                                                                           11/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         11/22
 
 8/28/13                                                                                                  Expository graphs
              Adding text
                 par(mfrow=c(1,2))
                 hist(pData$JWMNP,xlab=""CT (min)"",col=""blue"",breaks=100,main="""")
                 mtext(text=""(a)"",side=3,line=1)
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                 mtext(text=""(b)"",side=3,line=1)
                                                                                                                           12/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         12/22
 
 8/28/13                                                                                                  Expository graphs
              Figure captions
              Figure 1. Distribution of commute time and relationship to wage earned by sex (a) Commute
              times in the American Community Survey (ACS) are right skewed. (b) Commute times do not appear
              to be strongly correlated with wage for either sex.
                                                                                                                           13/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         13/22
 
 8/28/13                                                                                                  Expository graphs
              Colorblindness
              http://www.vischeck.com/
                                                                                                                           14/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         14/22
 
 8/28/13                                                                                                  Expository graphs
              Graphical workflow
                 · Start with a rough plot
                 · Tweak it to make it expository
                 · Save the file
                 · Include it in presentations
              Saving files in R is done with graphics devices. Use the command ?Devices to see a list. Here we will
              go over the most popular devices.
                                                                                                                           15/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         15/22
 
 8/28/13                                                                                                  Expository graphs
              pdf
                 · Important parameters: file, height,width
                 pdf(file=""twoPanel.pdf"",height=4,width=8)
                 par(mfrow=c(1,2))
                 hist(pData$JWMNP,xlab=""CT (min)"",col=""blue"",breaks=100,main="""")
                 mtext(text=""(a)"",side=3,line=1)
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                 mtext(text=""(b)"",side=3,line=1)
                 dev.off()
                                                                                                                           16/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         16/22
 
 8/28/13                                                                                                  Expository graphs
              png
                 · Important parameters: file, height,width
                 png(file=""twoPanel.png"",height=480,width=(2*480))
                 par(mfrow=c(1,2))
                 hist(pData$JWMNP,xlab=""CT (min)"",col=""blue"",breaks=100,main="""")
                 mtext(text=""(a)"",side=3,line=1)
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                 legend(100,200000,legend=c(""men"",""women""),col=c(""black"",""red""),pch=c(19,19),cex=c(0.5,0.5))
                 mtext(text=""(b)"",side=3,line=1)
                 dev.off()
                 RStudioGD
                                 2
                                                                                                                           17/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         17/22
 
 8/28/13                                                                                                  Expository graphs
              dev.copy2pdf
                 par(mfrow=c(1,2))
                 hist(pData$JWMNP,xlab=""CT (min)"",col=""blue"",breaks=100,main="""")
                 plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab=""CT (min)"",ylab=""Wages (dollars)"",col=pData$SEX)
                                                                                                                           18/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         18/22
 
 8/28/13                                                                                                  Expository graphs
              dev.copy2pdf
                 dev.copy2pdf(file=""twoPanelv2.pdf"")
                 RStudioGD
                                 2
                                                                                                                           19/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         19/22
 
 8/28/13                                                                                                  Expository graphs
              Something to avoid
              http://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/
                                                                                                                           20/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         20/22
 
 8/28/13                                                                                                  Expository graphs
              Something to aspire to
              http://www.facebook.com/notes/facebook-engineering/visualizing-friendships/469716398919
                                                                                                                           21/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         21/22
 
 8/28/13                                                                                                  Expository graphs
              Further resources
                 · How to display data badly
                 · The visual display of quantitative information
                 · Creating more effective graphs
                 · R Graphics Cookbook
                 · ggplot2: Elegant Graphics for Data Analysis
                 · Flowing Data
                                                                                                                           22/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week3/003expositoryGraphs/index.html#1                         22/22
"
"./04_ExploratoryAnalysis/old/003expositoryGraphs/twoPanel.pdf","                              (a)                                                                                               (b)
            1000
                                                                         ●
                                                                         ●●●
                                                                           ●● ●●●●● ● ● ●               ●   ●           ●                                             ●
                                                                                                                                          ●   men
                                             Wages (dollars)
                                                               150000
                                                                                                                                          ●   women
Frequency
            600
                                                                             ● ●      ● ●                           ●
                                                                                  ●      ●                                                                  ●
                                                                               ●                  ●
                                                                           ● ● ●            ● ●
                                                                                            ●  ●                            ●
                                                                           ●          ●                                                         ●
                                                                          ●● ● ● ●    ● ● ●                                                     ●
                                                                         ● ●●●● ●     ● ● ●
                                                                                                        ●                                       ●
                                                                           ● ●●● ● ●              ● ●       ●
                                                                           ● ●
                                                                           ●   ●      ● ● ● ● ● ●
                                                                           ●
                                                                           ●●●
                                                                         ●●●●
                                                                               ●●●● ● ●
                                                                                      ●
                                                                                         ● ●● ●
                                                                                            ●
                                                                                               ●    ●           ●                 ●                                   ●
                                                                           ●●● ●●●● ● ●  ●  ●  ●
                                                                                                  ● ●
                                                                                                    ●
                                                                                                            ●
                                                                                                            ●               ● ●                 ●
                                                                               ●  ●   ● ● ●       ● ●                                                                 ●
                                                                           ●●●    ●●●       ●     ●     ●
                                                                         ●●●
                                                                           ●●●
                                                                             ● ●
                                                                               ●●●●
                                                                                      ● ●
                                                                                      ●  ●
                                                                                         ●  ●     ●
                                                                                                  ● ●   ●   ●
                                                                                                            ●
                                                                                                                                  ●                                   ●
                                                                           ●
                                                                           ●●  ●  ● ● ●           ●     ●               ●         ●
                                                               0 50000
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                             ●  ●●
                                                                               ●●     ● ●● ●●     ● ●
                                                                                                  ● ●
                                                                                                    ●   ●                         ●   ●
                                                                         ●
                                                                         ●●●
                                                                           ● ● ●●●● ● ●
                                                                                      ● ●●  ●
                                                                                         ● ●●● ● ●●     ●   ●                                                         ●
                                                                         ●●●●●
                                                                           ●
                                                                           ●      ● ●   ●● ●        ●     ● ●
                                                                                                            ●           ●         ●             ●
                                                                         ●●●
                                                                           ●●
                                                                            ●●●●●●
                                                                               ●●
                                                                                  ●
                                                                                  ●●● ●
                                                                                      ● ●●●●
                                                                                            ●●●● ● ●    ● ● ●
                                                                                                            ●   ●                ●●             ●                     ●
            200
                                                                         ● ● ●
                                                                             ●              ●  ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●●●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                             ●
                                                                             ●●
                                                                             ●
                                                                             ● ●
                                                                               ● ●●
                                                                                  ●
                                                                                  ● ●●
                                                                                   ●● ● ●
                                                                                      ●  ●
                                                                                         ● ●
                                                                                            ●
                                                                                            ●
                                                                                            ●●●
                                                                                               ●
                                                                                               ●
                                                                                               ● ●● ●
                                                                                                    ●
                                                                                                    ●   ●   ●
                                                                                                            ●
                                                                                                        ● ● ●
                                                                                                                ● ● ●
                                                                                                                    ●        ● ●                                      ●
                                                                         ●
                                                                         ●●●
                                                                           ●
                                                                           ●●
                                                                            ●●
                                                                             ●
                                                                             ●●●
                                                                               ●●
                                                                                ● ●
                                                                                  ●●  ●
                                                                                      ●  ●
                                                                                         ● ●        ●
                                                                                                    ●   ●   ●       ●
                                                                                                                    ● ●    ●
                                                                                                                           ●   ● ● ●            ●                     ●
                                                                         ●
                                                                         ●
                                                                         ●●●
                                                                           ●
                                                                           ●●●
                                                                             ● ●●●●
                                                                                  ● ● ● ●
                                                                                      ●     ● ●● ●  ●   ● ● ●
                                                                                                            ●              ●                    ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●●
                                                                             ●
                                                                             ●
                                                                             ●●●●
                                                                                ●●● ●
                                                                                  ●●●
                                                                                  ●   ●
                                                                                      ●●
                                                                                       ●●●  ●  ●
                                                                                         ● ● ● ●  ● ●
                                                                                                  ● ●
                                                                                                    ●
                                                                                                        ●
                                                                                                        ● ● ●
                                                                                                            ●
                                                                                                            ●
                                                                                                                ●   ●      ●                    ●                     ●
                                                                                                                                                                      ●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●●
                                                                             ●
                                                                             ● ●
                                                                               ●●
                                                                               ●●
                                                                                ●●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ● ●●●●●●
                                                                                         ● ●●  ●
                                                                                               ● ●● ●
                                                                                                    ●   ●
                                                                                                        ● ●
                                                                                                          ● ●     ● ●          ●
                                                                                                                               ●
                                                                                                                                                ●
                                                                                                                                                ●                     ●
                                                                                                                                                                      ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                 ●
                                                                                 ●●●
                                                                                  ● ● ●
                                                                                      ●
                                                                                      ●● ●
                                                                                         ●
                                                                                         ●  ●
                                                                                            ●● ●
                                                                                               ●
                                                                                               ● ●●
                                                                                                  ● ●
                                                                                                    ●   ● ● ●
                                                                                                        ●   ●
                                                                                                            ●   ●
                                                                                                                    ● ●
                                                                                                                  ● ●
                                                                                                                    ● ● ● ●
                                                                                                                           ●
                                                                                                                           ●   ●
                                                                                                                               ●
                                                                                                                                                ●
                                                                                                                                                ●
                                                                                                                                                                      ●
                                                                                                                                                                      ●
                                                                                                                                                                      ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                               ●●
                                                                               ●
                                                                               ●● ●●
                                                                                 ●●
                                                                                  ● ●●
                                                                                    ● ●●
                                                                                      ● ●
                                                                                        ●●
                                                                                         ●  ●
                                                                                            ●  ● ●●
                                                                                                  ● ●   ● ● ●
                                                                                                            ●       ●      ●                                          ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                             ●●●
                                                                             ●●●
                                                                             ●
                                                                             ●
                                                                             ●  ● ● ●
                                                                                  ●●●
                                                                                 ●●
                                                                                ●●●
                                                                                      ●●
                                                                                       ●●
                                                                                      ●● ●
                                                                                         ●●●●
                                                                                            ● ●●●●● ●
                                                                                                    ●
                                                                                                    ●
                                                                                                        ●
                                                                                                        ● ●
                                                                                                        ● ● ●
                                                                                                          ●●●
                                                                                                            ●
                                                                                                            ●
                                                                                                                    ●      ●
                                                                                                                           ●
                                                                                                                           ●   ●
                                                                                                                                                ●
                                                                         ●
                                                                         ●●●
                                                                           ● ●
                                                                             ● ●●   ●    ●● ●  ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●●●
                                                                             ●●
                                                                             ●
                                                                             ●●
                                                                               ●
                                                                               ●●●
                                                                                ●●
                                                                               ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●●
                                                                                  ●●●●
                                                                                      ●
                                                                                      ● ●
                                                                                      ●
                                                                                      ● ●● ●
                                                                                         ●
                                                                                            ●
                                                                                            ●
                                                                                            ● ●●
                                                                                                ●
                                                                                               ● ●● ●
                                                                                                  ● ●
                                                                                                    ●   ●
                                                                                                        ●
                                                                                                          ●
                                                                                                        ●●● ●
                                                                                                            ●
                                                                                                            ●
                                                                                                                    ● ●
                                                                                                                    ●
                                                                                                                           ●
                                                                                                                           ●
                                                                                                                           ●
                                                                                                                           ●
                                                                                                                               ●
                                                                                                                                                ●
                                                                                                                                                    ●
                                                                         ●
                                                                         ●●●
                                                                           ●
                                                                           ●
                                                                           ●●●
                                                                             ●
                                                                             ● ● ●●
                                                                                  ●
                                                                                  ●●  ●  ●  ●     ● ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                  ●●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ● ●
                                                                                      ●●
                                                                                      ●
                                                                                      ●  ●
                                                                                         ●
                                                                                            ● ●
                                                                                            ●
                                                                                            ●● ●  ●●●
                                                                                                  ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                        ●
                                                                                                        ● ●●
                                                                                                        ●   ●
                                                                                                            ●
                                                                                                                      ●●   ●
                                                                                                                           ●
                                                                                                                         ● ●
                                                                                                                           ●
                                                                                                                                                ●                     ●
                                                                         ●● ●● ●●      ● ● ●●     ●
                                                                                                  ●●●   ●         ● ●                                                 ●
            0                                                            ●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●●
                                                                             ●●
                                                                             ●  ●
                                                                               ●●●●●
                                                                                 ●● ●
                                                                                    ●●●
                                                                                      ●●●
                                                                                      ●
                                                                                      ●  ●  ● ●● ●  ●   ●● ●●   ●●         ●   ●                ●
                                                                                                                                                ●       ●             ●
                   0   50      100     150                               0                         50                             100                           150
                            CT (min)                                                                                CT (min)
"
"./04_ExploratoryAnalysis/old/003expositoryGraphs/twoPanelv2.pdf",""
"./04_ExploratoryAnalysis/PlottingBase/PlottingBase.pdf","The Base Plotting System in R
Roger Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Plotting System
The core plotting and graphics engine in R is encapsulated in the following packages:
 · graphics: contains plotting functions for the ""base"" graphing systems, including plot, hist,
   boxplot and many others.
 · grDevices: contains all the code implementing the various graphics devices, including X11, PDF,
   PostScript, PNG, etc.
The lattice plotting system is implemented using the following packages:
 · lattice: contains code for producing Trellis graphics, which are independent of the “base” graphics
   system; includes functions like xyplot, bwplot, levelplot
 · grid: implements a different graphing system independent of the “base” system; the lattice
   package builds on top of grid; we seldom call functions from the grid package directly
                                                                                                  2/20
 
 The Process of Making a Plot
When making a plot one must first make a few considerations (not necessarily in this order):
· Where will the plot be made? On the screen? In a file?
· How will the plot be used?
     - Is the plot for viewing temporarily on the screen?
     - Will it be presented in a web browser?
     - Will it eventually end up in a paper that might be printed?
     - Are you using it in a presentation?
· Is there a large amount of data going into the plot? Or is it just a few points?
· Do you need to be able to dynamically resize the graphic?
                                                                                             3/20
 
 The Process of Making a Plot
· What graphics system will you use: base, lattice, or ggplot2? These generally cannot be mixed.
· Base graphics are usually constructed piecemeal, with each aspect of the plot handled
  separately through a series of function calls; this is often conceptually simpler and allows plotting
  to mirror the thought process
· Lattice graphics are usually created in a single function call, so all of the graphics parameters
  have to specified at once; specifying everything at once allows R to automatically calculate the
  necessary spacings and font sizes.
· ggplot2 combines concepts from both base and lattice graphics but uses an independent
  implementation
We focus on using the base plotting system to create graphics on the screen device.
                                                                                                   4/20
 
 Base Graphics
Base graphics are used most commonly and are a very powerful system for creating 2-D graphics.
 · There are two phases to creating a base plot
      - Initializing a new plot
      - Annotating (adding to) an existing plot
 · Calling plot(x, y) or hist(x) will launch a graphics device (if one is not already open) and
   draw a new plot on the device
 · If the arguments to plot are not of some special class, then the default method for plot is
   called; this function has many arguments, letting you set the title, x axis label, y axis label, etc.
 · The base graphics system has many parameters that can set and tweaked; these parameters are
   documented in ?par; it wouldn’t hurt to try to memorize this help page!
                                                                                                         5/20
 
 Simple Base Graphics: Histogram
library(datasets)
hist(airquality$Ozone) ## Draw a new plot
                                          6/20
 
 Simple Base Graphics: Scatterplot
library(datasets)
with(airquality, plot(Wind, Ozone))
                                    7/20
 
 Simple Base Graphics: Boxplot
library(datasets)
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality, xlab = ""Month"", ylab = ""Ozone (ppb)"")
                                                                         8/20
 
 Some Important Base Graphics Parameters
Many base plotting functions share a set of parameters. Here are a few key ones:
· pch: the plotting symbol (default is open circle)
· lty: the line type (default is solid line), can be dashed, dotted, etc.
· lwd: the line width, specified as an integer multiple
· col: the plotting color, specified as a number, string, or hex code; the colors() function gives
  you a vector of colors by name
· xlab: character string for the x-axis label
· ylab: character string for the y-axis label
                                                                                               9/20
 
 Some Important Base Graphics Parameters
The par() function is used to specify global graphics parameters that affect all plots in an R
session. These parameters can be overridden when specified as arguments to specific plotting
functions.
 · las: the orientation of the axis labels on the plot
 · bg: the background color
 · mar: the margin size
 · oma: the outer margin size (default is 0 for all sides)
 · mfrow: number of plots per row, column (plots are filled row-wise)
 · mfcol: number of plots per row, column (plots are filled column-wise)
                                                                                           10/20
 
 Some Important Base Graphics Parameters
Default values for global graphics parameters
 par(""lty"")
 ## [1] ""solid""
 par(""col"")
 ## [1] ""black""
 par(""pch"")
 ## [1] 1
                                              11/20
 
 Some Important Base Graphics Parameters
Default values for global graphics parameters
 par(""bg"")
 ## [1] ""transparent""
 par(""mar"")
 ## [1] 5.1 4.1 4.1 2.1
 par(""mfrow"")
 ## [1] 1 1
                                              12/20
 
 Base Plotting Functions
· plot: make a scatterplot, or other type of plot depending on the class of the object being plotted
· lines: add lines to a plot, given a vector x values and a corresponding vector of y values (or a 2-
  column matrix); this function just connects the dots
· points: add points to a plot
· text: add text labels to a plot using specified x, y coordinates
· title: add annotations to x, y axis labels, title, subtitle, outer margin
· mtext: add arbitrary text to the margins (inner or outer) of the plot
· axis: adding axis ticks/labels
                                                                                                 13/20
 
 Base Plot with Annotation
library(datasets)
with(airquality, plot(Wind, Ozone))
title(main = ""Ozone and Wind in New York City"") ## Add a title
                                                               14/20
 
 Base Plot with Annotation
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City""))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = ""blue""))
                                                                              15/20
 
 Base Plot with Annotation
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City"",
    type = ""n""))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = ""blue""))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = ""red""))
legend(""topright"", pch = 1, col = c(""blue"", ""red""), legend = c(""May"", ""Other Months""))
                                                                                       16/20
 
 Base Plot with Regression Line
with(airquality, plot(Wind, Ozone, main = ""Ozone and Wind in New York City"",
    pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)
                                                                             17/20
 
 Multiple Base Plots
par(mfrow = c(1, 2))
with(airquality, {
    plot(Wind, Ozone, main = ""Ozone and Wind"")
    plot(Solar.R, Ozone, main = ""Ozone and Solar Radiation"")
})
                                                             18/20
 
 Multiple Base Plots
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
with(airquality, {
    plot(Wind, Ozone, main = ""Ozone and Wind"")
    plot(Solar.R, Ozone, main = ""Ozone and Solar Radiation"")
    plot(Temp, Ozone, main = ""Ozone and Temperature"")
    mtext(""Ozone and Weather in New York City"", outer = TRUE)
})
                                                               19/20
 
 Summary
· Plots in the base plotting system are created by calling successive R functions to ""build up"" a plot
· Plotting occurs in two stages:
    - Creation of a plot
    - Annotation of a plot (adding lines, points, text, legends)
· The base plotting system is very flexible and offers a high degree of control over plotting
                                                                                                  20/20
"
"./04_ExploratoryAnalysis/PlottingLattice/PlottingLattice.pdf","The Lattice Plotting System in R
Computing for Data Analysis
Roger Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 The Lattice Plotting System
The lattice plotting system is implemented using the following packages:
 · lattice: contains code for producing Trellis graphics, which are independent of the “base” graphics
   system; includes functions like xyplot, bwplot, levelplot
 · grid: implements a different graphing system independent of the “base” system; the lattice
   package builds on top of grid
      - We seldom call functions from the grid package directly
 · The lattice plotting system does not have a ""two-phase"" aspect with separate plotting and
   annotation like in base plotting
 · All plotting/annotation is done at once with a single function call
                                                                                                  2/14
 
 Lattice Functions
· xyplot: this is the main function for creating scatterplots
· bwplot: box-and-whiskers plots (“boxplots”)
· histogram: histograms
· stripplot: like a boxplot but with actual points
· dotplot: plot dots on ""violin strings""
· splom: scatterplot matrix; like pairs in base plotting system
· levelplot, contourplot: for plotting ""image"" data
                                                                3/14
 
 Lattice Functions
Lattice functions generally take a formula for their first argument, usually of the form
 xyplot(y ~ x | f * g, data)
 · We use the formula notation here, hence the ~.
 · On the left of the ~ is the y-axis variable, on the right is the x-axis variable
 · f and g are conditioning variables — they are optional
      - the * indicates an interaction between two variables
 · The second argument is the data frame or list from which the variables in the formula should be
   looked up
      - If no data frame or list is passed, then the parent frame is used.
 · If no other arguments are passed, there are defaults that can be used.
                                                                                               4/14
 
 Simple Lattice Plot
library(lattice)
library(datasets)
## Simple scatterplot
xyplot(Ozone ~ Wind, data = airquality)
                                        5/14
 
 Simple Lattice Plot
library(datasets)
library(lattice)
## Convert 'Month' to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
                                                                  6/14
 
 Lattice Behavior
Lattice functions behave differently from base graphics functions in one critical way.
 · Base graphics functions plot data directly to the graphics device (screen, PDF file, etc.)
 · Lattice graphics functions return an object of class trellis
 · The print methods for lattice functions actually do the work of plotting the data on the graphics
   device.
 · Lattice functions return ""plot objects"" that can, in principle, be stored (but it’s usually better to just
   save the code + data).
 · On the command line, trellis objects are auto-printed so that it appears the function is plotting the
   data
                                                                                                         7/14
 
 Lattice Behavior
p <- xyplot(Ozone ~ Wind, data = airquality)  ## Nothing happens!
print(p) ## Plot appears
xyplot(Ozone ~ Wind, data = airquality)  ## Auto-printing
                                                                  8/14
 
 Lattice Panel Functions
· Lattice functions have a panel function which controls what happens inside each panel of the
  plot.
· The lattice package comes with default panel functions, but you can supply your own if you want
  to customize what happens in each panel
· Panel functions receive the x/y coordinates of the data points in their panel (along with any
  optional arguments)
                                                                                              9/14
 
 Lattice Panel Functions
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c(""Group 1"", ""Group 2""))
xyplot(y ~ x | f, layout = c(2, 1)) ## Plot with 2 panels
                                                          10/14
 
 Lattice Panel Functions
## Custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
    panel.xyplot(x, y, ...) ## First call the default panel function for 'xyplot'
    panel.abline(h = median(y), lty = 2) ## Add a horizontal line at the median
})
                                                                                  11/14
 
 Lattice Panel Functions: Regression line
## Custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
    panel.xyplot(x, y, ...) ## First call default panel function
    panel.lmline(x, y, col = 2) ## Overlay a simple linear regression line
})
                                                                           12/14
 
 Many Panel Lattice Plot
                        13/14
 
 Summary
· Lattice plots are constructed with a single function call to a core lattice function (e.g. xyplot)
· Aspects like margins and spacing are automatically handled and defaults are usually sufficient
· The lattice system is ideal for creating conditioning plots where you examine the same kind of
  plot under many different conditions
· Panel functions can be specified/customized to modify what is plotted in each of the plot panels
                                                                                                     14/14
"
"./04_ExploratoryAnalysis/PlottingMath/Plotting - Math Functions.pdf","8/11/13                                                                               Plotting - Math Functions
                          Plotting - Math Functions
                          Computing for Data Analysis
                          Roger Peng, Associate Professor
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/roger/Plotting Math/index.html#1                           1/5
 
 8/11/13                                                                               Plotting - Math Functions
              Mathematical Annotation
              R can produce LaTeX-like symbols on a plot for mathematical annotation. This is very handy and is
              useful for making fun of people who use other statistical packages.
                 · Math symbols are “expressions” in R and need to be wrapped in the expressionfunction
                 · There is a set list of allowed symbols and this is documented in ?plotmath
                 · Plotting functions that take arguments for text generally allow expressions for math symbols
                                                                                                                2/5
file://localhost/Users/sean/Developer/GitHub/modules/roger/Plotting Math/index.html#1                               2/5
 
 8/11/13                                                                               Plotting - Math Functions
              Mathematical Annotation
              Some examples.
                 plot(0, 0, main = expression(theta == 0),
                          ylab = expression(hat(gamma) == 0),
                          xlab = expression(sum(x[i] * y[i], i==1, n)))
              Pasting strings together.
                 x <- rnorm(100)
                 hist(x,
                          xlab=expression(""The mean ("" * bar(x) * "") is "" *
                                                        sum(x[i]/n,i==1,n)))
                                                                                                                3/5
file://localhost/Users/sean/Developer/GitHub/modules/roger/Plotting Math/index.html#1                               3/5
 
 8/11/13                                                                               Plotting - Math Functions
              Substituting
              What if you want to use a computed value in the annotation?
                 x <- rnorm(100)
                 y <- x + rnorm(100, sd = 0.5)
                 plot(x, y,
                          xlab=substitute(bar(x) == k, list(k=mean(x))),
                          ylab=substitute(bar(y) == k, list(k=mean(y)))
                          )
              Or in a loop of plots
                 par(mfrow = c(2, 2))
                 for(i in 1:4) {
                               x <- rnorm(100)
                               hist(x, main=substitute(theta==num,list(num=i)))
                 }
                                                                                                                4/5
file://localhost/Users/sean/Developer/GitHub/modules/roger/Plotting Math/index.html#1                               4/5
 
 8/11/13                                                                               Plotting - Math Functions
              Summary of Important Help Pages
                 · ?par
                 · ?plot
                 · ?xyplot
                 · ?plotmath
                 · ?axis
                                                                                                                5/5
file://localhost/Users/sean/Developer/GitHub/modules/roger/Plotting Math/index.html#1                               5/5
"
"./04_ExploratoryAnalysis/PlottingSystems/Plotting Systems in R.pdf","Plotting Systems in R
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 The Base Plotting System
· ""Artist's palette"" model
· Start with blank canvas and build up from there
· Start with plot function (or similar)
· Use annotation functions to add/modify (text, lines, points, axis)
                                                                     2/11
 
 The Base Plotting System
· Convenient, mirrors how we think of building plots and analyzing data
· Can’t go back once plot has started (i.e. to adjust margins); need to plan in advance
· Difficult to ""translate"" to others once a new plot has been created (no graphical ""language"")
· Plot is just a series of R commands
                                                                                                3/11
 
 Base Plot
library(datasets)
data(cars)
with(cars, plot(speed, dist))
                              4/11
 
 The Lattice System
· Plots are created with a single function call (xyplot, bwplot, etc.)
· Most useful for conditioning types of plots: Looking at how y changes with x across levels of z
· Things like margins/spacing set automatically because entire plot is specified at once
· Good for puttng many many plots on a screen
                                                                                                  5/11
 
 The Lattice System
· Sometimes awkward to specify an entire plot in a single function call
· Annotation in plot is not especially intuitive
· Use of panel functions and subscripts difficult to wield and requires intense preparation
· Cannot ""add"" to the plot once it is created
                                                                                            6/11
 
 Lattice Plot
library(lattice)
state <- data.frame(state.x77, region = state.region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))
                                                                   7/11
 
 The ggplot2 System
· Splits the difference between base and lattice in a number of ways
· Automatically deals with spacings, text, titles but also allows you to annotate by ""adding"" to a plot
· Superficial similarity to lattice but generally easier/more intuitive to use
· Default mode makes many choices for you (but you can still customize to your heart's desire)
                                                                                                    8/11
 
 ggplot2 Plot
library(ggplot2)
data(mpg)
qplot(displ, hwy, data = mpg)
                              9/11
 
 Summary
· Base: ""artist's palette"" model
· Lattice: Entire plot specified by one function; conditioning
· ggplot2: Mixes elements of Base and Lattice
                                                               10/11
 
 References
Paul Murrell (2011). R Graphics, CRC Press.
Hadley Wickham (2009). ggplot2, Springer.
                                            11/11
"
"./04_ExploratoryAnalysis/Principles/PrinciplesofAnalyticGraphics.pdf","Principles of Analytic Graphics
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
                                                                                    2/16
 
 Show Comparisons
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   3/16
 
 Show Comparisons
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   4/16
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
· Principle 2: Show causality, mechanism, explanation, systematic structure
    - What is your causal framework for thinking about a question?
                                                                                    5/16
 
 Show causality, mechanism
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   6/16
 
 Show causality, mechanism
Reference: Butz AM, et al., JAMA Pediatrics, 2011.
                                                   7/16
 
 Principles of Analytic Graphics
· Principle 1: Show comparisons
    - Evidence for a hypothesis is always relative to another competing hypothesis.
    - Always ask ""Compared to What?""
· Principle 2: Show causality, mechanism, explanation, systematic structure
    - What is your causal framework for thinking about a question?
· Principle 3: Show multivariate data
    - Multivariate = more than 2 variables
    - The real world is multivariate
    - Need to ""escape flatland""
                                                                                    8/16
 
 Show Multivariate Data
                       9/16
 
 Show Multivariate Data
                       10/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
                                                                       11/16
 
 Integrate Different Modes of Evidence
                                      12/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
· Principle 5: Describe and document the evidence with appropriate labels, scales, sources, etc.
    - A data graphic should tell a complete story that is credible
                                                                                               13/16
 
 Principles of Analytic Graphics
· Principle 4: Integration of evidence
    - Completely integrate words, numbers, images, diagrams
    - Data graphics should make use of many modes of data presentation
    - Don't let the tool drive the analysis
· Principle 5: Describe and document the evidence with appropriate labels, scales, sources, etc.
    - A data graphic should tell a complete story that is credible
· Principle 6: Content is king
    - Analytical presentations ultimately stand or fall depending on the quality, relevance, and
      integrity of their content
                                                                                               14/16
 
 Summary
· Principle 1: Show comparisons
· Principle 2: Show causality, mechanism, explanation
· Principle 3: Show multivariate data
· Principle 4: Integrate multiple modes of evidence
· Principle 5: Describe and document the evidence
· Principle 6: Content is king
                                                      15/16
 
 References
Edward Tufte (2006). Beautiful Evidence, Graphics Press LLC. www.edwardtufte.com
                                                                                 16/16
"
"./05_ReproducibleResearch/caching/pengslides.pdf","Computa(onal	  and	  Policy	  Tools	  
   for	  Reproducible	  Research	  
                Roger	  D.	  Peng,	  PhD	  
              Department	  of	  Biosta/s/cs	  
 Johns	  Hopkins	  Bloomberg	  School	  of	  Public	  Health	  
                                	  
                          July	  2011	  
                        Vancouver,	  BC	  
 
                         Replica(on	  
• The	  ul(mate	  standard	  for	  strengthening	  scien(ﬁc	  
  evidence	  is	  replica(on	  of	  ﬁndings	  and	  conduc(ng	  
  studies	  with	  independent	  
   – Inves(gators 	  	  
   – Data	  
   – Analy(cal	  methods	  
   – Laboratories	  
   – Instruments	  
• Replica(on	  is	  par(cularly	  important	  in	  studies	  
  that	  can	  impact	  broad	  policy	  or	  regulatory	  
  decisions	  
 
                      Why	  Do	  We	  Need	  
             Reproducible	  Research?	  
• Some	  studies	  cannot	  be	  replicated	  
   – No	  (me,	  opportunis(c	  
   – No	  money	  
   – Unique	  
• New	  technologies	  increasing	  data	  collec(on	  
  throughput;	  data	  are	  more	  complex	  and	  extremely	  
  high	  dimensional	  
• Exis(ng	  databases	  can	  be	  merged	  into	  new	  
  “megadatabases”	  
• Compu(ng	  power	  is	  greatly	  increased,	  allowing	  more	  
  sophis(cated	  analyses	  
• For	  every	  ﬁeld	  “X”	  there	  is	  a	  ﬁeld	  “Computa(onal	  X”	  
 
    How	  Can	  We	  Bridge	  the	  Gap?	  
                       ?	  
Replicate   	                     Nothing       	  
 
 Research	  Pipeline	  
                         Ar(cle	  
                           Reader	  
 
                                  Research	  Pipeline	  
Author	  
                                                       Presenta(on	  code	  
          Processing	  code	     Analy(c	  code	                             Figures	  
 Measured	               Analy(c	            Computa(onal	  
                                                                                 Tables	    Ar(cle	  
 Data	                   Data	               Results	  
                                                                               Numerical	  
                                                                               Summaries	    Text	  
                                                                                               Reader	  
 
  
             Reproducible	  Air	  Pollu(on	  
                  and	  Health	  Research	  
• Es(ma(ng	  small	  (but	  important)	  health	  eﬀects	  
  in	  the	  presence	  of	  much	  stronger	  signals	  
• Results	  inform	  substan(al	  policy	  decisions,	  
  aﬀect	  many	  stakeholders	  
   – EPA	  regula(ons	  can	  cost	  billions	  of	  dollars	  
• Complex	  sta(s(cal	  methods	  are	  needed	  and	  
  subjected	  to	  intense	  scru(ny	  
 
     Internet-­‐based	  Health	  and	  Air	  
Pollu(on	  Surveillance	  System	  (iHAPSS)	  
                                   h_p://www.ihapss.jhsph.edu	  
 
   What	  is	  Reproducible	  Research?	  
• Analy(c	  data	  are	  available	  
• Analy(c	  code	  are	  available	  
• Documenta(on	  of	  code	  and	  data	  
• Standard	  means	  of	  distribu(on	  
 
                Who	  are	  the	  Players?	  
• Authors	  
   – Want	  to	  make	  their	  research	  reproducible	  
   – Want	  tools	  for	  RR	  to	  make	  their	  lives	  easier	  (or	  at	  
     least	  not	  much	  harder)	  
• Readers	  
   – Want	  to	  reproduce	  (and	  perhaps	  expand	  upon)	  
     interes(ng	  ﬁndings	  
   – Want	  tools	  for	  RR	  to	  make	  their	  lives	  easier	  
 
                        Challenges	  
• Authors	  must	  undertake	  considerable	  eﬀort	  to	  
  put	  data/results	  on	  the	  web	  (may	  not	  have	  
  resources	  like	  a	  web	  server)	  
• Readers	  must	  download	  data/results	  
  individually	  and	  piece	  together	  which	  data	  go	  
  with	  which	  code	  sec(ons,	  etc.	  
• Readers	  may	  not	  have	  the	  same	  resources	  as	  
  authors	  
 
                             In	  Reality…	  
• Authors	  
   – Just	  put	  stuﬀ	  on	  the	  web	  
   – Journal	  supplementary	  materials	  
   – There	  are	  some	  central	  databases	  for	  various	  
     ﬁelds	  (e.g.	  biology,	  ICPSR)	  
• Readers	  
   – Just	  download	  the	  data	  and	  (try	  to)	  ﬁgure	  it	  out	  
   – Piece	  together	  the	  socware	  and	  run	  it	  
 
  Literate	  (Sta(s(cal)	  Programming	  
• An	  ar(cle	  is	  a	  stream	  of	  text	  and	  code	  
• Analysis	  code	  is	  divided	  into	  text	  and	  code	  
  “chunks”	  
• Each	  code	  chunk	  loads	  data	  and	  computes	  results	  
• Presenta(on	  code	  formats	  results	  (tables,	  ﬁgures,	  
  etc.)	  
• Ar(cle	  text	  explains	  what	  is	  going	  on	  
• Literate	  programs	  can	  be	  weaved	  to	  produce	  
  human-­‐readable	  documents	  and	  tangled	  to	  
  produce	  machine-­‐readable	  documents	  
 
   Literate	  (Sta(s(cal)	  Programming	  
• Literate	  programming	  is	  a	  general	  concept	  that	  
   requires	  
    1. A	  documenta(on	  language	  (human	  readable)	  
    2. A	  programming	  language	  (machine	  readable)	  
•    Sweave	  uses	  LATEX	  and	  R	  as	  the	  documenta(on	  and	  
     programming	  languages	  
•    Sweave	  was	  developed	  by	  Friedrich	  Leisch	  (member	  
     of	  the	  R	  Core)	  and	  is	  maintained	  by	  R	  core	  
•    Main	  web	  site:	  http://www.statistik.lmu.de/
     ̃leisch/Sweave
•    Alterna(ves	  to	  LATEX/R	  exist,	  suchas	  HTML/R	  
     (package	  R2HTML)	  and	  ODF/R	  (package	  odfWeave).	  
 
                                  Research	  Pipeline	  
Author	  
                                                       Presenta(on	  code	  
          Processing	  code	     Analy(c	  code	                             Figures	  
 Measured	               Analy(c	            Computa(onal	  
                                                                                 Tables	    Ar(cle	  
 Data	                   Data	               Results	  
                                                                               Numerical	  
                                                                               Summaries	    Text	  
                                                                                               Reader	  
 
                                  Research	  Pipeline	  
Author	  
                                                       Presenta(on	  code	  
          Processing	  code	     Analy(c	  code	                             Figures	  
 Measured	               Analy(c	            Computa(onal	  
                                                                                 Tables	    Ar(cle	  
 Data	                   Data	               Results	  
                                      Database	  
                                                                               Numerical	  
                                                                               Summaries	    Text	  
                                                                                               Reader	  
 
             Caching	  Computa(ons	  
LaTeX/R	                                            PDF	  
                          Local/Remote	  
Magnum	  Opus	                                Magnum	  Opus	  
                          Cached	  
                          computa(ons	  
                                                             Figure	  1	  
   Code	  chunk	  1	       Database	  1	  
   Code	  chunk	  2	       Database	  2	  
                                                     Table	  1	  
 
          The	  cacher	  package	  for	  R	  
• Add-­‐on	  package	  for	  R	  
• Evaluates	  code	  wri_en	  in	  ﬁles	  and	  stores	  
  intermediate	  results	  in	  a	  key-­‐value	  database	  
• R	  expressions	  are	  given	  SHA-­‐1	  hash	  values	  so	  that	  
  changes	  can	  be	  tracked	  and	  code	  reevaluated	  if	  
  necessary	  
• “Cacher	  packages”	  can	  be	  built	  for	  distribu(on	  
• Others	  can	  “clone”	  an	  analysis	  and	  evaluate	  
  subsets	  of	  code	  or	  inspect	  data	  objects	  
                                         Journal	  of	  Sta/s/cal	  So;ware,	  26	  (7),	  1—24	  
 
       Conceptual	  Model	  
                     Dataset
                                    Dataset
     Dataset
                                            Code
                    Source
                      File
Code
                                            Code
             Result  Result  Result
 
       Using	  cacher	  as	  an	  Author	  
1. Parse	  the	  R	  source	  ﬁle;	  Create	  the	  necessary	  cache	  
   directories	  and	  subdirectories	  
2. Cycle	  through	  each	  expression	  in	  the	  source	  ﬁle:	  
   – If	  an	  expression	  has	  never	  been	  evaluated,	  evaluate	  it	  
       and	  store	  any	  resul(ng	  R	  objects	  in	  the	  cache	  database,	  	  
   – If	  a	  cached	  result	  exists,	  lazy-­‐load	  the	  results	  from	  the	  
       cache	  database	  and	  move	  to	  the	  next	  expression,	  	  
   – If	  an	  expression	  does	  not	  create	  any	  R	  objects	  (i.e.,	  there	  
       is	  nothing	  to	  cache),	  add	  the	  expression	  to	  the	  list	  of	  
       expressions	  where	  evalua(on	  needs	  to	  be	  forced	  
   – Write	  out	  metadata	  for	  this	  expression	  to	  the	  metadata	  
       ﬁle.	  	  
 
      Using	  cacher	  as	  an	  Author	  
• The	  cachepackage	  func(on	  creates	  a	  
  cacher	  package	  storing	  
   – Source	  ﬁle	  
   – Cached	  data	  objects	  
   – Metadata	  
• Package	  ﬁle	  is	  zipped	  and	  can	  be	  distributed	  
• Readers	  can	  unzip	  the	  ﬁle	  and	  immediately	  
  inves(gate	  its	  contents	  via	  cacher	  package	  
 
           Example:	  Simple	  Analysis	  
library(datasets)!
                          Nothing	  created	  (packages	  a_ached)	  
library(stats)!
!
## Load the dataset!
data(airquality)!         “airquality”	  object	  loaded	  into	  workspace	  
!
## Fit a linear model!
fit <- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality)!
summary(fit)!
!                                                “ﬁt”	  object	  created	  in	  workspace	  
## Plot some diagnostics!               Side	  eﬀect	  (prin(ng	  to	  console)	  
par(mfrow = c(2, 2))!
plot(fit)!
                              Side	  eﬀect	  (plojng	  to	  graphics	  device)	  
 
                    Using	  cacher	  as	  a	  Reader	  
  A	  journal	  ar(cle	  says...	  
         “…the	  code	  and	  data	  for	  this	  analysis	  can	  be	  found	  in	  the	  cacher	  package	  
         092dcc7dda4b93e42f23e038a60e1d44dbec7b3f.”	  
> library(cacher)!
> clonecache(id = ""092dcc7dda4b93e42f23e038a60e1d44dbec7b3f”)!
> clonecache(id = “092d”) ## Same as above!
created cache directory '.cache'!
!
> showfiles()!
[1] ""top20.R""!
> sourcefile(""top20.R"") !
 
                Cloning	  an	  Analysis	  
• Local	  directories	  created	  
• Source	  code	  ﬁles	  and	  metadata	  are	  
  downloaded	  
• Data	  objects	  are	  not	  downloaded	  by	  default	  
• References	  to	  data	  objects	  are	  loaded	  and	  
  corresponding	  data	  can	  be	  lazy-­‐loaded	  on	  
  demand	  
 
           Examining	  Code	  
> code()!
source file: top20.R!
1 cities <- readLines(""citylist.txt"")!
2 classes <- readLines(""colClasses.txt"")!
3 vars <- c(""date"", ""dow"", ""death"", !
4 data <- lapply(cities, function(city) {!
5 names(data) <- cities!
6 estimates <- sapply(data, function(city) {!
7 effect <- weighted.mean(estimates[1, !
8 stderr <- sqrt(1/sum(1/estimates[2, !
!
> graphcode()!
 
 Analysis	  Code	  Graphs	  
cities       classes               vars
               data            quasipoisson
                     estimates
              effect              stderr
 
            Tracing	  Code	  Backwards	  
> objectcode(“data”)!
source file: top20.R!
1 cities <- readLines(""citylist.txt"")!
2 classes <- readLines(""colClasses.txt"")!
3 vars <- c(""date"", ""dow"", ""death"", ""tmpd"", ""rmtmpd"", ""dptp"",!
             ""rmdptp"", ""l1pm10tmean"")!
4 data <- lapply(cities, function(city) {!
           filename <- file.path(""data"", paste(city, ""csv"",!
                                 sep = "".""))!
           d0 <- read.csv(filename, colClasses = classes,!
                          nrow = 5200)!
           d0[, vars]!
   })!
5 names(data) <- cities!
!
 
                      Running	  Code	  
• The	  runcode	  func(on	  executes	  code	  in	  the	  
  source	  ﬁle	  
• By	  default,	  expressions	  that	  results	  in	  an	  
  object	  being	  created	  are	  not	  run	  and	  the	  
  resul(ng	  objects	  is	  lazy-­‐loaded	  into	  the	  
  workspace	  
• Expressions	  not	  resul(ng	  in	  objects	  are	  
  evaluated	  
 
          Checking	  Code	  and	  Objects	  
• The	  checkcode	  func(on	  evaluates	  all	  
  expressions	  from	  scratch	  (no	  lazy-­‐loading)	  
• Results	  of	  evalua(on	  are	  checked	  against	  
  stored	  results	  to	  see	  if	  the	  results	  are	  the	  
  same	  as	  what	  the	  author	  calculated	  
   – Sejng	  RNG	  seeds	  is	  cri(cal	  for	  this	  to	  work	  
• The	  integrity	  of	  data	  objects	  can	  be	  veriﬁed	  
  with	  the	  checkobjects	  func(on	  to	  check	  
  for	  possible	  corrup(on	  of	  data	  (i.e.	  in	  transit)	  
 
       Inspec(ng	  Data	  Objects	  
> loadcache()!
!
> ls()!
[1] ""cities""      ""classes""   ""data""      ""effect""  !
[5] ""estimates"" ""stderr""      ""vars"" !
!
> cities!
/ transferring cache db file b8fd490bcf1d48cd06...!
  [1] ""la""   ""ny""    ""chic"" ""dlft"" ""hous"" ""phoe""!
  [7] ""staa"" ""sand"" ""miam"" ""det"" ""seat"" ""sanb""!
[13] ""sanj"" ""minn"" ""rive"" ""phil"" ""atla"" ""oakl""!
[19] ""denv"" ""clev""!
!
 
             Inspec(ng	  Data	  Objects	  
> effect!
/ transferring cache db file 584115c69e5e2a4ae5...!
[1] 0.0002313219!
!
> stderr!
/ transferring cache db file 81b6dc23736f3d72c6...!
[1] 0.000052457!
!
  A	  10	  unit	  increase	  in	  PM10	  is	  associated	  with	  a	  0.23%	  increase	  in	  daily	  
  mortality	  
 
                 cacher	  Summary	  
• The	  cacher	  package	  can	  be	  used	  by	  authors	  
  to	  create	  cache	  packages	  from	  data	  analyses	  
  for	  distribu(on	  
• Readers	  can	  use	  the	  cacher	  package	  to	  
  inspect	  others’	  data	  analyses	  by	  examining	  
  cached	  computa(ons	  
• cacher	  is	  mindful	  of	  readers’	  resources	  and	  
  eﬃciently	  loads	  only	  those	  data	  objects	  that	  
  are	  needed	  
 
 A	  Central	  Archive	  for	  Reproducible	  Data	  Analyses	  
                                             h_p://penguin.biostat.jhsph.edu/	  
 
   Reproducible	  Research	  and	  Journals	  
• What	  policies	  can	  journals	  implement	  to	  make	  
  published	  research	  reproducible?	  
• Carrot	  or	  s(ck?	  
 
                                                                                                                                                           ownloaded from biostatistics.oxfordjournals.org by guest on January 5, 2011
Roger will be assuming the role of Associate Editor for reproducibility as set out in his piece.
    While we consider reproducibility to be a desirable goal, we wish to emphasise that our policy is to
encourage our authors to consider this as an opportunity that they may wish to take, rather than as a re-
quirement that we impose upon them. All submissions to the journal will continue to be reviewed using
our established system; the issue of reproducibility will be considered only when a paper had been
              RR	  Policy	  at	  Biosta/s/cs	  
accepted for publication on the basis of its scientific merit as judged by our peer-review process.
                                                                                               P ETER J. D IGGLE , S COTT L. Z EGER
                              Reproducible research and Biostatistics
                                                             ROGER D. PENG
                                               1. I NTRODUCTION AND MOTIVATION
The replication of scientific findings using independent investigators, methods, data, equipment, and pro-
tocols has long been, and will continue to be, the standard by which scientific claims are evaluated.
However, in many fields of study there are examples of scientific investigations that cannot be fully repli-
cated because of a lack of time or resources. In such a situation, there is a need for a minimum standard that
can fill the void between full replication and nothing. One candidate for this minimum standard is
“reproducible research”, which requires that data sets and computer code be made available to others
for verifying published results and conducting alternative analyses.
    The need for publishing reproducible research is increasing for a number of reasons. Investigators are
more frequently examining weak associations and complex interactions for which the data contain a low
signal-to-noise ratio. New technologies allow scientists in all areas to compile complex high-dimensional
databases. The ubiquity of powerful statistical and computing capabilities allows investigators to explore
those databases and identify associations of potential interest. However, with the increase in data and com-
puting power comes a greater potential for identifying spurious associations. In addition to these develop-
ments, recent reports of fraudulent research being published in the biomedical literature have highlighted
the need for reproducibility in biomedical studies and have invited the attention of the major medical jour-
nals (Laine and others, 2007). Even without the presence of deliberate fraud, it should be noted that as
analyses become more complicated, the possibility of inadvertant errors resulting in misleading findings
looms large. In the examples of Baggerly and others (2005) and Coombes and others (2007), the errors
discovered were not necessarily simple or obvious and the examination of the problem itself required
                                                                                           Biosta/s/cs	  (2009),	  10,	  3,	  pp.	  405–408	  
c The Author 2009. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oxfordjournals.org.
⃝
 
     Dimensions	  of	  Reproducibility	  
• Data	  (“D”):	  The	  analy(c	  data	  from	  which	  the	  principal	  results	  
  were	  derived	  are	  made	  available	  on	  the	  journal’s	  Web	  site.	  
  The	  authors	  are	  responsible	  for	  ensuring	  that	  necessary	  
  permissions	  are	  obtained	  before	  the	  data	  are	  distributed.	  
 
     Dimensions	  of	  Reproducibility	  
• Data	  (“D”):	  The	  analy(c	  data	  from	  which	  the	  principal	  results	  
  were	  derived	  are	  made	  available	  on	  the	  journal’s	  Web	  site.	  
  The	  authors	  are	  responsible	  for	  ensuring	  that	  necessary	  
  permissions	  are	  obtained	  before	  the	  data	  are	  distributed.	  
• Code	  (“C”):	  Any	  computer	  code,	  socware,	  or	  other	  computer	  
  instruc(ons	  that	  were	  used	  to	  compute	  published	  results	  are	  
  provided.	  For	  socware	  that	  is	  widely	  available	  from	  central	  
  repositories	  (e.g.	  CRAN,	  Statlib),	  a	  reference	  to	  where	  they	  
  can	  be	  obtained	  will	  suﬃce.	  
 
     Dimensions	  of	  Reproducibility	  
• Data	  (“D”):	  The	  analy(c	  data	  from	  which	  the	  principal	  results	  
  were	  derived	  are	  made	  available	  on	  the	  journal’s	  Web	  site.	  
  The	  authors	  are	  responsible	  for	  ensuring	  that	  necessary	  
  permissions	  are	  obtained	  before	  the	  data	  are	  distributed.	  
• Code	  (“C”):	  Any	  computer	  code,	  socware,	  or	  other	  computer	  
  instruc(ons	  that	  were	  used	  to	  compute	  published	  results	  are	  
  provided.	  For	  socware	  that	  is	  widely	  available	  from	  central	  
  repositories	  (e.g.	  CRAN,	  Statlib),	  a	  reference	  to	  where	  they	  
  can	  be	  obtained	  will	  suﬃce.	  
• Reproducible	  (“R”):	  An	  ar(cle	  is	  designated	  as	  reproducible	  if	  
  the	  AER	  succeeds	  in	  execu(ng	  the	  code	  on	  the	  data	  provided	  
  and	  produces	  results	  matching	  those	  that	  the	  authors	  claim	  
  are	  reproducible.	  In	  reproducing	  these	  results,	  reasonable	  
  bounds	  for	  numerical	  tolerance	  will	  be	  considered.	  
 
                                                          Kite	  Marking	  
Biostatistics (2009), 10, 4, pp. 756–772
doi:10.1093/biostatistics/kxp029
Advance Access publication on July 27, 2009
    Second-order estimating equations for the analysis of
               clustered current status data
                                 RICHARD J. COOK∗ , DAVID TOLUSSO
                 Department of Statistics and Actuarial Science, University of Waterloo,
                                                                                                                       Downloaded from biostatistics.oxfordjournals.org by guest on January 5, 2011
                                   Waterloo, ON, Canada N2L 3G1
                                          rjcook@uwaterloo.ca
                                                   S UMMARY Biostatistics (2009), 10, 3, pp. 409–423
                                                                  doi:10.1093/biostatistics/kxp010
With clustered event time data, interest most often lies in marginal    features such as quantiles or proba-
                                                                  Advance
bilities from the marginal event time distribution or covariate effects  onAccess publication
                                                                            marginal   hazard on  April 17, Cop-
                                                                                               functions.   2009
ula models offer a convenient framework for modeling. We present methods of estimating the baseline
marginal distributions, covariate effects, and association parameters for clustered current status data based
on second-order generalized estimating equations. We examine the efficiency gains realized from using
second-order estimating equations compared with first-order equations, issues of copula misspecification,
and apply the methods to motivating studies including one on the incidence of joint damage in patients
with psoriatic arthritis.                                              Air pollution and health in Scotland: a multicity study
                                                                                                        DUNCAN LEE∗ , CLAIRE FERGUSON
Keywords: Current status data; Generalized estimating equations; Piecewise constant hazards; Relative efficiency.
                                                                                    Department of Statistics, University of Glasgow, Glasgow, G12 8QQ UK
                                              1. I NTRODUCTION                                               duncan@stats.gla.ac.uk
                                                                                                                                                                                                      Downloaded from biosta
Current status data, also referred to as type I interval censored data, arise when the status of a subject  RICHARD MITCHELL
is only known at a single point in time, and hence, the underlying failure      time  is either  left or right
                                                                             Public Health and Health Policy, University of Glasgow, Glasgow, G12 8QQ UK
censored. Ayer and others (1955) show how to obtain the nonparametric maximum likelihood estimate
of the distribution function with current status data using techniques from isotonic regression (Barlow
and others, 1972). Methods for fitting multiplicative semiparametric models are described by Xu                     S UMMARY
and others (2004) via sieve estimation, and generalized additiveThis
                                                                  models
                                                                     paperare  fitted an
                                                                            presents  using  isotonic regres-
                                                                                         epidemiological   study investigating the effects of long-term air pollution exposure
sion in Shiboski (1998). Sun (2006) provides a comprehensive summary of modern statistical methods for
 
 What	  is	  Reproducible?	  
                  Lee,	  Ferguson	  &	  Mitchell,	  Biosta/s/cs,	  2009	  
 
 Supplementary	  Data	  (not	  ideal)	   
                       Some	  Sparse	  Data	  
Data	  so	  far	  (a	  li_le	  old…)	  
• 4	  papers	  have	  requested	  and	  received	  the	  “R”	  
  kite	  mark	  
• 4	  papers	  received	  a	  “C”	  
• 2	  papers	  received	  a	  “D”	  
• 1	  paper	  with	  “DC”	  
 
                    Further	  Work	  
• Need	  a	  be_er	  system	  at	  journal	  for	  tracking	  
  and	  highligh(ng	  papers	  with	  kite-­‐marks	  
• Infrastructure	  for	  hos(ng	  data	  is	  limited	  
• Infrastructure	  for	  reproducing	  results	  is	  
  limited	  
• Need	  be_er	  adver/sing	  of	  this	  policy	  
 
                             Summary	  
• Reproducible	  research	  is	  important	  as	  a	  
  minimum	  standard,	  par(cularly	  for	  studies	  that	  
  are	  diﬃcult	  to	  replicate	  
• Infrastructure	  is	  needed	  for	  crea8ng	  and	  
  distribu8ng	  reproducible	  documents,	  beyond	  
  what	  is	  currently	  available	  
• The	  cacher	  package	  caches	  intermediate	  
  computa(ons	  for	  future	  inspec(on	  
• Scien(ﬁc	  culture	  needs	  to	  evolve	  to	  encourage	  
  greater	  sharing	  of	  datasets	  and	  methods	  
• Journals	  can	  play	  a	  key	  role	  by	  providing	  both	  
  carrots	  and	  s(cks	  to	  authors	  
 
                 Acknowledgments	  
• Joint	  work	  with	  	  
   – Duncan	  Temple	  Lang	  (UC	  Davis)	  
   – Deb	  Nolan	  (Berkeley)	  
   – Sandy	  Eckel	  (USC)	  
• Funded	  by	  
   – Na(onal	  Ins(tute	  of	  Environmental	  Health	  Science	  
   – Na(onal	  Ins(tute	  on	  Aging	  
   – Johns	  Hopkins	  Faculty	  Innova(on	  Fund	  
   – Health	  Eﬀects	  Ins(tute	  
"
"./05_ReproducibleResearch/CaseStudy_AP/nickel-beta-lm-no-ny.pdf","                          2            ●
                                           ●
                                      ●
                                      ●        ●
                          1                       ●
                                     ●● ●         ●
                                     ● ●                                       ●●           ●
                                  ●
% Increase in Mortality
                                  ●      ●      ●
                                 ●          ●
                                   ●
                                 ● ● ●●
                                       ● ●● ●                          ●
                                      ●● ● ●
                                    ●●●
                                      ●●                           ●
                          0      ● ●         ●●                ●
                                            ●
                                     ●● ●                 ●
                                 ●
                                 ●     ●
                                 ●
                                 ●● ●   ●●
                                 ● ●  ●
                                 ●
                                                   ●
                          −1
                                      ●●
                                      ●
                          −2
                                  ●
                               0.000                   0.005           0.010        0.015
                                                   Long−term Averge Nickel Concentration
"
"./05_ReproducibleResearch/CaseStudy_AP/nickel-beta-lm.pdf","                          2            ●
                                           ●
                                      ●
                                      ●        ●
                          1                       ●
                                     ●● ●         ●
                                     ● ●                                       ●●           ●
                                  ●
% Increase in Mortality
                                  ●      ●      ●
                                 ●          ●
                                   ●
                                 ● ● ●●
                                       ● ●● ●                          ●
                                      ●● ● ●
                                    ●●●
                                      ●●                           ●
                          0      ● ●         ●●                ●
                                            ●
                                     ●● ●                 ●
                                 ●
                                 ●     ●
                                 ●
                                 ●● ●   ●●
                                 ● ●  ●
                                 ●
                                                   ●
                          −1
                                      ●●
                                      ●
                          −2
                                  ●
                               0.000                   0.005           0.010        0.015
                                                   Long−term Averge Nickel Concentration
"
"./05_ReproducibleResearch/CaseStudy_AP/nickel-beta.pdf","                          2            ●
                                           ●
                                      ●
                                      ●        ●
                          1                       ●
                                     ●● ●         ●
                                     ● ●                                       ●●           ●
                                  ●
% Increase in Mortality
                                  ●      ●      ●
                                 ●          ●
                                   ●
                                 ● ● ●●
                                       ● ●● ●                          ●
                                      ●● ● ●
                                    ●●●
                                      ●●                           ●
                          0      ● ●         ●●                ●
                                            ●
                                     ●● ●                 ●
                                 ●
                                 ●     ●
                                 ●
                                 ●● ●   ●●
                                 ● ●  ●
                                 ●
                                                   ●
                          −1
                                      ●●
                                      ●
                          −2
                                  ●
                               0.000                   0.005           0.010        0.015
                                                   Long−term Averge Nickel Concentration
"
"./05_ReproducibleResearch/CaseStudy_Baggerly/cancerbioinformatics2010_baggerly_irrh_01.pdf","The Importance of Reproducibility in
  High-Throughput Biology: Case
 Studies in Forensic Bioinformatics
                 Keith A. Baggerly
     Bioinformatics and Computational Biology
         UT M. D. Anderson Cancer Center
            kabagg@mdanderson.org
           Cambridge, 4 September 2010
 
 G ENOMIC S IGNATURES                                          1
                 Why is RR So Important in H-TB?
Our intuition about what “makes sense” is very poor in high
dimensions. To use “genomic signatures” as biomarkers, we
need to know they’ve been assembled correctly.
Without documentation, we may need to employ forensic
bioinformatics to infer what was done to obtain the results.
Let’s examine some case studies involving an important
clinical problem: can we predict how a given patient will
respond to available chemotherapeutics?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          2
             Using the NCI60 to Predict Sensitivity
          Potti et al (2006), Nature Medicine, 12:1294-1300.
The main conclusion is that we can use microarray data from
cell lines (the NCI60) to define drug response “signatures”,
which can be used to predict whether patients will respond.
They provide examples using 7 commonly used agents.
This got people at MDA very excited.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          3
                                       Fit Training Data
We want the test data to split like this...
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          4
                                        Fit Testing Data
But it doesn’t. Did we do something wrong?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES               5
                     5-FU Heatmaps
     Nat Med Paper
 
 G ENOMIC S IGNATURES                5
                     5-FU Heatmaps
     Nat Med Paper      Our t-tests
 
 G ENOMIC S IGNATURES                                                         5
                                         5-FU Heatmaps
     Nat Med Paper                                Our t-tests Reported Genes
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          6
                                   Their List and Ours
> temp <- cbind(
          sort(rownames(pottiUpdated)[fuRows]),
          sort(rownames(pottiUpdated)[
                       fuTQNorm@p.values <= fuCut]);
> colnames(temp) <- c(""Theirs"", ""Ours"");
> temp
             Theirs                               Ours
...
[3,] ""1881_at""                                    ""1882_g_at""
[4,] ""31321_at""                                   ""31322_at""
[5,] ""31725_s_at"" ""31726_at""
[6,] ""32307_r_at"" ""32308_r_at""
...
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                                                                                                                                                                 7
                             Offset P-Values: Other Drugs
                                           Topotecan+1                                                   Etoposide+1                                                        Adriamycin+1
                                 1.0                                                         1.0                                                                  1.0
                                 0.8                                                         0.8                                                                  0.8
                                                                                                                                                                                                                 ●
                                 0.6                                                         0.6                                                                  0.6
                       P Value                                                     P Value                                                              P Value                                              ●
                                                                                                                                                                                                             ●
                                 0.4                                                         0.4                                                                  0.4
                                 0.2                                                         0.2                                                                  0.2
                                                                                                                                                                                                             ●
                                 0.0                                                         0.0                                                                  0.0
                                                                                                                                                                                                             ●
                                       ●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●●
                                           ●
                                           ●
                                           ●●
                                            ●
                                            ●
                                            ●●
                                             ●
                                             ●
                                             ●●
                                              ●
                                              ●
                                              ●●
                                               ●
                                               ●
                                               ●●
                                                ●
                                                ●
                                                ●●
                                                 ●
                                                 ●
                                                 ●●
                                                  ●
                                                  ●
                                                  ●●
                                                   ●
                                                   ●
                                                   ●●
                                                    ●
                                                    ●
                                                    ●●
                                                     ●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●●
                                                                 ●
                                                                 ●
                                                                 ●●
                                                                  ●
                                                                  ●
                                                                  ●●
                                                                   ●
                                                                   ●
                                                                   ●●
                                                                    ●
                                                                    ●
                                                                    ●●
                                                                     ●
                                                                     ●
                                                                     ●●
                                                                      ●
                                                                      ●
                                                                      ●●
                                                                       ●
                                                                       ●
                                                                       ●●
                                                                        ●
                                                                        ●
                                                                        ●●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●                      ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●                   ●
                                                                                                                                                                        ●●
                                                                                                                                                                         ●●
                                                                                                                                                                          ●●
                                                                                                                                                                           ●●
                                                                                                                                                                            ●●
                                                                                                                                                                             ●●
                                                                                                                                                                              ●●
                                                                                                                                                                               ●●
                                                                                                                                                                                ●●
                                                                                                                                                                                 ●●
                                                                                                                                                                                  ●●
                                                                                                                                                                                   ●●
                                                                                                                                                                                    ●●
                                                                                                                                                                                     ●●
                                                                                                                                                                                      ●●
                                                                                                                                                                                       ●●
                                                                                                                                                                                        ●●
                                                                                                                                                                                         ●●
                                                                                                                                                                                          ●●
                                                                                                                                                                                           ●●
                                                                                                                                                                                            ●●
                                                                                                                                                                                             ●●
                                                                                                                                                                                              ●●
                                                                                                                                                                                               ●●
                                                                                                                                                                                                ●●
                                                                                                                                                                                                 ●●
                                                                                                                                                                                                  ●●
                                                                                                                                                                                                   ●●
                                                                                                                                                                                                    ●●
                                                                                                                                                                                                     ●●
                                                                                                                                                                                                      ●●
                                                                                                                                                                                                       ●●
                                                                                                                                                                                                        ●●
                                                                                                                                                                                                         ●●
                                                                                                                                                                                                          ●●
                                                                                                                                                                                                           ●●
                                                                                                                                                                                                            ●●
                                       0         50         100             150                    0      10 20 30 40 50                                                0      20        40        60        80
                                                     Index                                                           Index                                                            Index
                                            Paclitaxel+1                                                 Docetaxel+1                                                           Cytoxan+1
                                 1.0                                           ●             1.0                                                                  1.0                                      ●●
                                                                                                                                                                                                             ●
                                                                              ●                                                                   ●                                                       ●
                                                                            ●                                                                                                                            ●
                                                                                                                                                  ●                                                    ●●
                                 0.8                                                         0.8                                                                  0.8                                 ●
                                                                                                                                                                                                     ●
                                                                                                                                               ●
                                                                                                                                               ●
                                 0.6                                                         0.6                                                                  0.6
                       P Value                                                     P Value
                                                                                                                                             ●●
                                                                                                                                                        P Value
                                                                                                                                           ●●●                                                      ●
                                                                                                                                          ●●                                                       ●
                                                                          ●
                                 0.4                                                         0.4                                                                  0.4                          ●
                                                                                                                                       ●                                                      ●
                                                                        ●                                                                                                                   ●●
                                                                                                                                                                                         ●●●
                                                                      ●                                                                                                                ●●
                                 0.2                                                         0.2                                      ●
                                                                                                                                                                  0.2               ●●●
                                                                                                                                                                                 ●●●
                                                                                                                                     ●●                                         ●
                                                                    ●                                                              ●●                                         ●●
                                 0.0   ●●●●●●●●●●●●●●●●●●●●●●●●●●●●                          0.0   ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●                              0.0   ●●●●●●
                                       0 5           15        25             35                   0      10 20 30 40 50                                                0 5           15         25          35
                                                     Index                                                           Index                                                            Index
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                              8
                                  Using Their Software
Their software requires two input files:
1. a quantification matrix, genes by samples, with a header
    giving classifications (0 = Resistant, 1 = Sensitive, 2 = Test)
2. a list of probeset ids in the same order as the quantification
    matrix. This list must not have a header row.
What do we get?
 c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                           9
        Heatmaps Match Exactly for Most Drugs!
From the paper:
From the software:
 
 G ENOMIC S IGNATURES                                          9
        Heatmaps Match Exactly for Most Drugs!
From the paper:
From the software:
We match heatmaps but not gene lists? We’ll come back to
this, because their software also gives predictions.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          10
                  Predicting Docetaxel (Chang 03)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          11
             Predicting Adriamycin (Holleman 04)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                            12
                          There Were Other Genes...
The 50-gene list for docetaxel has 19 “outliers”.
The initial paper on the test data (Chang et al) gave a list of
92 genes that separated responders from nonresponders.
Entries 7-20 in Chang et al’s list comprise 14/19 outliers.
The others: ERCC1, ERCC4, ERBB2, BCL2L11, TUBA3.
These are the genes named to explain the biology.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          13
            RR Theme: Don’t Take My Word For It!
Read the paper! Coombes, Wang & Baggerly, Nat Med, Nov
6, 2007, 13:1276-7, author reply 1277-8.
Try it yourselves! All of the raw data, documentation*, and
code* is available from our web site (*and from Nat Med):
http://bioinformatics.mdanderson.org/
Supplements/ReproRsch-Chemo.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          14
          Potti/Nevins Reply (Nat Med 13:1277-8)
Labels for Adria are correct – details on their web page.
They’ve gotten the approach to work again. (Twice!)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                            15
        Adriamycin 0.9999+ Correlations (Reply)
 
 G ENOMIC S IGNATURES                                          15
        Adriamycin 0.9999+ Correlations (Reply)
Redone in Aug 08, “using only the 95 unique samples”
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                         16
                     The First 20 Files Now Named
Sample ID Response
   1 GSM44303                       RES                       11 GSM9694 RES
   2 GSM44304                       RES                       12 GSM9695 RES
   3 GSM9653                        RES                       13 GSM9696 RES
   4 GSM9653                        RES                       14 GSM9698 RES
   5 GSM9654                        RES                       15 GSM9699 SEN
   6 GSM9655                        RES                       16 GSM9701 RES
   7 GSM9656                        RES                       17 GSM9708 RES
   8 GSM9657                        RES                       18 GSM9708 SEN
   9 GSM9658                        SEN                       19 GSM9709 RES
10 GSM9658                          SEN                       20 GSM9711 RES
15 duplicates; 6 inconsistent. (61R, 13S, 6B) vs (22,48,10).
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          17
                                Validation 1: Hsu et al
J Clin Oncol, Oct 1, 2007, 25:4350-7.
Same approach, using Cisplatin and Pemetrexed.
For cisplatin, U133A arrays were used for training. ERCC1,
ERCC4 and DNA repair genes are identified as “important”.
With some work, we matched the heatmaps. (Gene lists?)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          18
                      The 4 We Can’t Match (Reply)
203719          at, ERCC1,
210158          at, ERCC4,
228131          at, ERCC1, and
231971          at, FANCM (DNA Repair).
The last two probesets are special.
These probesets aren’t on the U133A arrays that were used.
They’re on the U133B.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                19
                     Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
 
 G ENOMIC S IGNATURES                                        19
                     Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
May/June 2009: we learn clinical trials had begun.
2007: pemetrexed vs cisplatin, pem vs vinorelbine.
2008: docetaxel vs doxorubicin, topotecan vs dox (Moffitt).
 
 G ENOMIC S IGNATURES                                           19
                                 Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
May/June 2009: we learn clinical trials had begun.
2007: pemetrexed vs cisplatin, pem vs vinorelbine.
2008: docetaxel vs doxorubicin, topotecan vs dox (Moffitt).
Sep 1. Paper submitted to Annals of Applied Statistics.
Sep 14. Paper online at Annals of Applied Statistics.
Sep-Oct: Story covered by The Cancer Letter, Duke starts
internal investigation, suspends trials.
So, what happened next?
 c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                             20
                                            Jan 29, 2010
Their investigation’s results “strengthen ... confidence in this
evolving approach to personalized cancer treatment.”
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                         21
                     Why We’re Unhappy...
“While the reviewers approved of our sharing the report with
the NCI, we consider it a confidential document” (Duke). A
future paper will explain the methods.
 
 G ENOMIC S IGNATURES                                          21
                                 Why We’re Unhappy...
“While the reviewers approved of our sharing the report with
the NCI, we consider it a confidential document” (Duke). A
future paper will explain the methods.
                          oh, there’s just one more thing...
In mid-Nov (mid-investigation), the Duke team posted new
data for cisplatin and pemetrexed (in trials since ’07).
These included quantifications for 59 ovarian cancer test
samples (from GSE3149) used for predictor validation.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          22
                   We Tried Matching The Samples
We correlated the 59 vectors with all samples in GSE3149.
43 samples are mislabeled; 16 don’t match at all.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                         23
                          FOI(L)A!
April 7: Paul Goldberg of the Cancer Letter requests “access
to and copies of the report (and attendant data)” from the NCI
under the Freedom of Information Act (FOIA).
May 3: redacted report supplied.
 
 G ENOMIC S IGNATURES                                          23
                                                 FOI(L)A!
April 7: Paul Goldberg of the Cancer Letter requests “access
to and copies of the report (and attendant data)” from the NCI
under the Freedom of Information Act (FOIA).
May 3: redacted report supplied.
“we were unable to identify a place where the statistical
methods were described in sufficient detail to independently
replicate the findings of the papers.” – review panel
The report makes no mention of the problems with
cisplatin/pemetrexed that arose during the investigation.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          24
                                            May 14, 2010
“We have asked [CALGB] to remove the Lung Metagene
Score from the trial, because we were unable to confirm the
score’s utility” – Jeff Abrams, CTEP director
(The NCI doesn’t directly sponsor the resumed trials.)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          25
                                           July 16, 2010
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          26
                                           July 19, 2010
“Duke administrators accomplished something monumental:
they triggered a public expression of outrage from
biostatisticians.”
A Baron, K Bandeen-Roche, D Berry, J Bryan,
V Carey, K Chaloner, M Delorenzi, B Efron,
R Elston, D Ghosh, J Goldberg, S Goodman,
F Harrell, S Hilsenbeck, W Huber, R Irizarry,
C Kendziorski, M Kosorok, T Louis, JS Marron,
M Newton, M Ochs, G Parmigiani*, J Quackenbush,
G Rosner, I Ruczinski, Y Shyr*, S Skates,
TP Speed, JD Storey, Z Szallasi, R Tibshirani,
S Zeger
Req to Varmus, DoD, ORI, Duke: suspend trials.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                   27
                 Subsequent Events, and a Caveat
Duke announces trials resuspended
NPR blog, Science blog, Nature blog, NYT blog, article
Lancet Oncology issues Expression of Concern
Varmus & Duke request IOM Involvement
Questions raised about NEJM paper
JCO launches investigation
More awards found to be wrong, COI claims
http://groups.google.com/group/reproducible-research
Correspondence to Nature
 
 G ENOMIC S IGNATURES                                          27
                 Subsequent Events, and a Caveat
Duke announces trials resuspended
NPR blog, Science blog, Nature blog, NYT blog, article
Lancet Oncology issues Expression of Concern
Varmus & Duke request IOM Involvement
Questions raised about NEJM paper
JCO launches investigation
More awards found to be wrong, COI claims
http://groups.google.com/group/reproducible-research
Correspondence to Nature
We’ve seen problems like these before. CAMDA 2002.
Proteomics 2003-5. TCGA current. Others at MDA.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          28
                                   Some Observations
The most common mistakes are simple.
Confounding in the Experimental Design
Mixing up the sample labels
Mixing up the gene labels
Mixing up the group labels
(Most mixups involve simple switches or offsets)
This simplicity is often hidden.
Incomplete documentation
Unfortunately, we suspect
The most simple mistakes are common.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          29
                        What Should the Norm Be?
For papers?
Things we look for:
1. Data (often mentioned, given MIAME)
2. Provenance
3. Code
4. Descriptions of Nonscriptable Steps
5. Descriptions of Planned Design, if Used.
For clinical trials?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          30
                                         Some Lessons
Is our own work reproducible?
Literate Programming. For the past two years, we have
required reports to be prepared in Sweave.
Reusing Templates.
Report Structure.
Executive Summaries.
Appendices. Some things we want to know all the time:
SessionInfo, Saves, and File Location.
The buzz phrase is reproducible research.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          31
                           Some Acknowledgements
Kevin Coombes
Shannon Neeley, Jing Wang
David Ransohoff, Gordon Mills
Jane Fridlyand, Lajos Pusztai, Zoltan Szallasi
MDACC Ovarian SPORE, Lung SPORE, Breast SPORE
Now in the Annals of Applied Statistics! Baggerly and
Coombes (2009), 3(4):1309-34.
http://bioinformatics.mdanderson.org/
Supplements/ReproRsch-All
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          32
                         Validation 2: Bonnefoi et al
Lancet Oncology, Dec 2007, 8:1071-8. (early access Nov 14)
Similar approach, using signatures for Fluorouracil, Epirubicin
Cyclophosphamide, and Taxotere to predict response to
combination therapies: FEC and TET.
Potentially improves ER- response from 44% to 70%.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                          33
             We Might Expect Some Differences...
   High Sample Correlations                                   Array Run Dates
     after Centering by Gene
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                             34
                       How Are Results Combined?
Potti et al predict response to TFAC, Bonnefoi et al to TET
and FEC. Let P() indicate prob sensitive. The rules used are
as follows.
P (T F AC) = P (T )+P (F )+P (A)+P (C)−P (T )P (F )P (A)P (C).
                                 P (ET ) = max[P (E), P (T )].
                                           5                   1
                   P (F EC) = [P (F ) + P (E) + P (C)] − .
                                           8                   4
Each rule is different.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          35
       Predictions for Individual Drugs? (Reply)
Does cytoxan make sense?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                        36
                  What About Blinded Validation?
“Data was made available to us, blinded. All we got was the
gene expression data. We ran the predictions and sent it
back to the EORTC investigators” – Joe Nevins, Oct 2.
 
 G ENOMIC S IGNATURES                                          36
                   What About Blinded Validation?
“Data was made available to us, blinded. All we got was the
gene expression data. We ran the predictions and sent it
back to the EORTC investigators” – Joe Nevins, Oct 2.
Sample info supplied:
Arm, Composite label
A, npCR Ep P- T3 N1 HB01 ...
A, pCR Ep Pp T2 N1 HB04
The data weren’t blinded.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                       37
                     Temozolomide Heatmaps
Augustine et al., 2009, Clin
Can Res, 15:502-10, Fig 4A.
Temozolomide, NCI-60.
 
 G ENOMIC S IGNATURES                                                                      37
                            Temozolomide Heatmaps
Augustine et al., 2009, Clin                                  Hsu et al., 2007, J Clin
Can Res, 15:502-10, Fig 4A.                                   Oncol, 25:4350-7, Fig 1A.
Temozolomide, NCI-60.                                         Cisplatin, Gyorffy cell lines.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          38
                                                     Index
Title
Cell Line Story
1. Trying it Ourselves
2. Matching Features
3. Using Software/Making Predictions
4. The Reply
5. Adriamycin Followup
6. Hsu et al (Cisplatin)
7. Bonnefoi et al (Combination Therapy)
8. More Recent (Temozolomide)
9. Timeline, Trials, Cancer Letter
10. Trial Restart and Objections
11. FOIA
12. Final Lessons
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
"
"./05_ReproducibleResearch/Checklist/Reproducible Research Checklist.pdf","Reproducible Research Checklist
What to Do and What Not to Do
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 DO: Start With Good Science
· Garbage in, garbage out
· Coherent, focused question simplifies many problems
· Working with good collaborators reinforces good practices
· Something that's interesting to you will (hopefully) motivate good habits
                                                                            2/13
 
 DON'T: Do Things By Hand
 · Editing spreadsheets of data to ""clean it up""
     - Removing outliers
     - QA / QC
     - Validating
 · Editing tables or figures (e.g. rounding, formatting)
 · Downloading data from a web site (clicking links in a web browser)
 · Moving data around your computer; splitting / reformatting data files
 · ""We're just going to do this once....""
Things done by hand need to be precisely documented (this is harder than it sounds)
                                                                                    3/13
 
 DON'T: Point And Click
· Many data processing / statistical analysis packages have graphical user interfaces (GUIs)
· GUIs are convenient / intuitive but the actions you take with a GUI can be difficult for others to
  reproduce
· Some GUIs produce a log file or script which includes equivalent commands; these can be saved
  for later examination
· In general, be careful with data analysis software that is highly interactive; ease of use can
  sometimes lead to non-reproducible analyses
· Other interactive software, such as text editors, are usually fine
                                                                                                4/13
 
 DO: Teach a Computer
 · If something needs to be done as part of your analysis / investigation, try to teach your computer
   to do it (even if you only need to do it once)
 · In order to give your computer instructions, you need to write down exactly what you mean to do
   and how it should be done
 · Teaching a computer almost guarantees reproducibilty
For example, by hand, you can
  1. Go to the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/
  2. Download the Bike Sharing Dataset by clicking on the link to the Data Folder, then clicking on
      the link to the zip file of dataset, and choosing ""Save Linked File As..."" and then saving it to a
      folder on your computer
                                                                                                     5/13
 
 DO: Teach a Computer
Or You can teach your computer to do the same thing using R:
 download.file(""http://archive.ics.uci.edu/ml/machine-learning-databases/00275/
                 Bike-Sharing-Dataset.zip"", ""ProjectData/Bike-Sharing-Dataset.zip"")
Notice here that
 · The full URL to the dataset file is specified (no clicking through a series of links)
 · The name of the file saved to your local computer is specified
 · The directory in which the file was saved is specified (""ProjectData"")
 · Code can always be executed in R (as long as link is available)
                                                                                         6/13
 
 DO: Use Some Version Control
· Slow things down
· Add changes in small chunks (don't just do one massive commit)
· Track / tag snapshots; revert to old versions
· Software like GitHub / BitBucket / SourceForge make it easy to publish results
                                                                                 7/13
 
 DO: Keep Track of Your Software Environment
· If you work on a complex project involving many tools / datasets, the software and computing
  environment can be critical for reproducing your analysis
· Computer architecture: CPU (Intel, AMD, ARM), GPUs,
· Operating system: Windows, Mac OS, Linux / Unix
· Software toolchain: Compilers, interpreters, command shell, programming languages (C, Perl,
  Python, etc.), database backends, data analysis software
· Supporting software / infrastructure: Libraries, R packages, dependencies
· External dependencies: Web sites, data repositories, remote databases, software repositories
· Version numbers: Ideally, for everything (if available)
                                                                                             8/13
 
 DO: Keep Track of Your Software Environment
sessionInfo()
## R version 3.0.2 Patched (2014-01-20 r64849)
## Platform: x86_64-apple-darwin13.0.0 (64-bit)
##
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
##
## attached base packages:
## [1] stats     graphics grDevices utils      datasets  base
##
## other attached packages:
## [1] slidify_0.3.3
##
## loaded via a namespace (and not attached):
## [1] evaluate_0.5.1 formatR_0.10   knitr_1.5      markdown_0.6.3
## [5] stringr_0.6.2 tools_3.0.2     whisker_0.3-2  yaml_2.1.8
                                                                     9/13
 
 DON'T: Save Output
· Avoid saving data analysis output (tables, figures, summaries, processed data, etc.), except
  perhaps temporarily for efficiency purposes.
· If a stray output file cannot be easily connected with the means by which it was created, then it is
  not reproducible.
· Save the data + code that generated the output, rather than the output itself
· Intermediate files are okay as long as there is clear documentation of how they were created
                                                                                                10/13
 
 DO: Set Your Seed
· Random number generators generate pseudo-random numbers based on an initial seed (usually
  a number or set of numbers)
    - In R you can use the set.seed() function to set the seed and to specify the random
       number generator to use
· Setting the seed allows for the stream of random numbers to be exactly reproducible
· Whenever you generate random numbers for a non-trivial purpose, always set the seed
                                                                                      11/13
 
 DO: Think About the Entire Pipeline
· Data analysis is a lengthy process; it is not just tables / figures / reports
· Raw data → processed data → analysis → report
· How you got the end is just as important as the end itself
· The more of the data analysis pipeline you can make reproducible, the better for everyone
                                                                                            12/13
 
 Summary: Checklist
· Are we doing good science?
· Was any part of this analysis done by hand?
    - If so, are those parts precisely document?
    - Does the documentation match reality?
· Have we taught a computer to do as much as possible (i.e. coded)?
· Are we using a version control system?
· Have we documented our software environment?
· Have we saved any output that we cannot reconstruct from original data + code?
· How far back in the analysis pipeline can we go before our results are no longer (automatically)
  reproducible?
                                                                                             13/13
"
"./05_ReproducibleResearch/knitr/knitr.pdf","Literate	  Sta)s)cal	  Programming	  
                           with	  knitr	  
                     Reproducible	  Research	  
                                     	  
    Roger	  D.	  Peng,	  Associate	  Professor	  of	  Biosta4s4cs	  
    Johns	  Hopkins	  Bloomberg	  School	  of	  Public	  Health	  
 
                    Problems,	  Problems	  
• Authors	  must	  undertake	  considerable	  eﬀort	  to	  put	  
  data/results	  on	  the	  web	  
• Readers	  must	  download	  data/results	  individually	  and	  
  piece	  together	  which	  data	  go	  with	  which	  code	  
  sec)ons,	  etc.	  
• Authors/readers	  must	  manually	  interact	  with	  websites	  
• There	  is	  no	  single	  document	  to	  integrate	  data	  analysis	  
  with	  textual	  representa)ons;	  i.e.	  data,	  code,	  and	  text	  
  are	  not	  linked	  
 
   Literate	  Sta)s)cal	  Programming	  
• Original	  idea	  comes	  from	  Don	  Knuth	  
• An	  ar)cle	  is	  a	  stream	  of	  text	  and	  code	  
• Analysis	  code	  is	  divided	  into	  text	  and	  code	  “chunks”	  
• Presenta)on	  code	  formats	  results	  (tables,	  ﬁgures,	  etc.)	  
• Ar)cle	  text	  explains	  what	  is	  going	  on	  
• Literate	  programs	  are	  weaved	  to	  produce	  human-­‐
  readable	  documents	  and	  tangled	  to	  produce	  machine-­‐
  readable	  documents	  
 
    Literate	  Sta)s)cal	  Programming	  
• Literate	  programming	  is	  a	  general	  concept.	  We	  
  need	  
   – A	  documenta)on	  language	  
   – A	  programming	  language	  
• The	  original	  Sweave	  system	  developed	  by	  
  Friedrich	  Leisch	  used	  LaTeX	  and	  R	  
• knitr	  supports	  a	  variety	  of	  documenta)on	  
  languages	  
 
   How	  Do	  I	  Make	  My	  Work	  Reproducible?	  
• Decide	  to	  do	  it	  (ideally	  from	  the	  start)	  
• Keep	  track	  of	  things,	  perhaps	  with	  a	  version	  
  control	  system	  to	  track	  snapshots/changes	  
• Use	  soVware	  whose	  opera)on	  can	  be	  coded	  
• Don’t	  save	  output	  
• Save	  data	  in	  non-­‐proprietary	  formats	  
 
      Literate	  Programming:	  Pros	  
• Text	  and	  code	  all	  in	  one	  place,	  logical	  order	  
• Data,	  results	  automa)cally	  updated	  to	  reﬂect	  
  external	  changes	  
• Code	  is	  live-­‐-­‐automa)c	  “regression	  test”	  when	  
  building	  a	  document	  
 
          Literate	  Programming:	  Cons	  
• Text	  and	  code	  all	  in	  one	  place;	  can	  make	  
  documents	  diﬃcult	  to	  read,	  especially	  if	  there	  
  is	  a	  lot	  of	  code	  
• Can	  substan)ally	  slow	  down	  processing	  of	  
  documents	  (although	  there	  are	  tools	  to	  help)	  
 
                        What	  is	  knitr?	  
• An	  R	  package	  wri\en	  by	  Yihui	  Xie	  (while	  he	  was	  a	  
  grad	  student	  at	  Iowa	  State)	  
   – Available	  on	  CRAN	  
• Supports	  RMarkdown,	  LaTeX,	  and	  HTML	  as	  
  documenta)on	  languages	  
• Can	  export	  to	  PDF,	  HTML	  
• Built	  right	  into	  RStudio	  for	  your	  convenience	  
 
                           Requirements	  
• A	  recent	  version	  of	  R	  
• A	  text	  editor	  (the	  one	  that	  comes	  with	  RStudio	  is	  
  okay)	  
• Some	  support	  packages	  also	  available	  on	  CRAN	  
• Some	  knowledge	  of	  Markdown,	  LaTeX,	  or	  HTML	  
• We	  will	  use	  Markdown	  here	  
 
               What	  is	  Markdown?	  
• A	  simpliﬁed	  version	  of	  “markup”	  languages	  
• No	  special	  editor	  required	  
• Simple,	  intui)ve	  formaang	  elements	  
• Complete	  informa)on	  available	  at	  
  h\p://goo.gl/MUt9i5	  
 
             What	  is	  knitr	  Good	  For?	  
• Manuals	  
• Short/medium-­‐length	  technical	  documents	  
• Tutorials	  
• Reports	  (esp.	  if	  generated	  periodically)	  
• Data	  preprocessing	  documents/summaries	  
 
     What	  is	  knitr	  NOT	  Good	  For?	  
• Very	  long	  research	  ar)cles	  
• Complex	  )me-­‐consuming	  computa)ons	  
• Documents	  that	  require	  precise	  formaang	  
 
                        My	  First	  knitr	  Document	  
Create	  a	  new	  
 document	  
                                                      Choose	  an	  R	  
                                                       Markdown	  
                                                       Document	  
 
 My	  First	  knitr	  Document	  
                         Start	  of	  code	  chunk	  
                          End	  of	  code	  chunk	  
 
 Processing	  a	  knitr	  Document	  
                       Push	  here	  
 
       More	  Complicated	  Way	  
library(knitr)
setwd(<working directory>)""
knit2html(“document.Rmd”)""
browseURL(“document.html”)""
 
 HTML	  Output	  
                   Code	  input	  
               Numerical	  output	  
 
  What	  knitr	  Produces:	  Markdown	  
RMarkdown	  Document	           Markdown	  Document	  (generated)	  
                   Code	  is	  
                  echoed	  
                                                             Result	  of	  
                                                           evalua)ng	  R	  
                                                               code	  
 
                                  A	  Few	  Notes	  
• knitr	  will	  ﬁll	  a	  new	  document	  with	  ﬁller	  text;	  delete	  it	  
• Code	  chunks	  begin	  with	  ```{r}	  and	  end	  with	  ```""
• All	  R	  code	  goes	  in	  between	  these	  markers	  
• Code	  chunks	  can	  have	  names,	  which	  is	  useful	  when	  we	  
  start	  making	  graphics	  
  ```{r firstchunk}
  ## R code goes here
  ```""
• By	  default,	  code	  in	  a	  code	  chunk	  is	  echoed,	  as	  will	  the	  
  results	  of	  the	  computa)on	  (if	  there	  are	  results	  to	  print)	  
 
     Processing	  of	  knitr	  Documents	  (what	  
              happens	  under	  the	  hood)	  
• You	  write	  the	  RMarkdown	  document	  (.Rmd)	  
• knitr	  produces	  a	  Markdown	  document	  (.md)	  
• knitr	  converts	  the	  Markdown	  document	  into	  
  HTML	  (by	  default)	  
• .Rmd	  à	  .md	  à	  .html	  
• You	  should	  NOT	  edit	  (or	  save)	  the	  .md	  or	  .html	  
  documents	  un)l	  you	  are	  ﬁnished	  
 
 Another	  Example	  
                 Level	  1	  heading	  
                 Level	  2	  heading	  
                 Do	  not	  echo	  code	  
 
 Output	   
 Hiding	  Results	   
 Output	   
 Inline	  Text	  Computa)ons	   
 Inline	  Text	  Computa)ons	   
 Incorpora)ng	  Graphics	  
                   Adjust	  ﬁgure	  height	  
 
 What	  knitr	  Produces	  in	  HTML	  
                                    Image	  is	  embedded	  
                                         in	  HTML	  
 
 Incorpora)ng	  Graphics	   
 Making	  Tables	  with	  xtable	   
 Making	  Tables	  with	  xtable	   
             Seang	  Global	  Op)ons	  
• Some)mes	  we	  want	  to	  set	  op)ons	  for	  every	  
  code	  chunk	  that	  are	  diﬀerent	  from	  the	  
  defaults	  
• For	  example,	  we	  may	  want	  to	  suppress	  all	  
  code	  echoing	  and	  results	  output	  
• We	  have	  to	  write	  some	  code	  to	  set	  these	  
  global	  op)ons	  
 
 Seang	  Global	  Op)ons	  
                       Set	  default	  to	  NOT	  
                              echo	  code	  
                                Override	  default	  
                        Don’t	  echo	  code	  here	  
 
 Seang	  Global	  Op)ons	   
          Some	  Common	  Op)ons	  
• Output	  
   – results:	  “asis”,	  “hide”	  
   – echo:	  TRUE,	  FALSE	  
• Figures	  
   – ﬁg.height:	  numeric	  
   – ﬁg.width:	  numeric	  
 
               Caching	  Computa)ons	  
• What	  if	  one	  chunk	  takes	  a	  long	  )me	  to	  run?	  
• All	  chunks	  have	  to	  be	  re-­‐computed	  every	  )me	  
  you	  re-­‐knit	  the	  ﬁle	  
• The	  cache=TRUE op)on	  can	  be	  set	  on	  a	  chunk-­‐by-­‐
  chunk	  basis	  to	  store	  results	  of	  computa)on	  
• AVer	  the	  ﬁrst	  run,	  results	  are	  loaded	  from	  cache	  
 
                        Caching	  Caveats	  
• If	  the	  data	  or	  code	  (or	  anything	  external)	  
  changes,	  you	  need	  to	  re-­‐run	  the	  cached	  code	  
  chunks	  
• Dependencies	  are	  not	  checked	  explicitly	  
• Chunks	  with	  signiﬁcant	  side	  eﬀects	  may	  not	  be	  
  cacheable	  
 
                                Summary	  
• Literate	  sta)s)cal	  programming	  can	  be	  a	  
  useful	  way	  to	  put	  text,	  code,	  data,	  output	  all	  
  in	  one	  document	  
• knitr	  is	  a	  powerful	  tool	  for	  integra)ng	  code	  
  and	  text	  in	  a	  simple	  document	  format	  
"
"./05_ReproducibleResearch/lectures/baggerly.pdf","The Importance of Reproducibility in
  High-Throughput Biology: Case
 Studies in Forensic Bioinformatics
                 Keith A. Baggerly
     Bioinformatics and Computational Biology
         UT M. D. Anderson Cancer Center
            kabagg@mdanderson.org
           Cambridge, 4 September 2010
 
 G ENOMIC S IGNATURES                                          1
                 Why is RR So Important in H-TB?
Our intuition about what “makes sense” is very poor in high
dimensions. To use “genomic signatures” as biomarkers, we
need to know they’ve been assembled correctly.
Without documentation, we may need to employ forensic
bioinformatics to infer what was done to obtain the results.
Let’s examine some case studies involving an important
clinical problem: can we predict how a given patient will
respond to available chemotherapeutics?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          2
             Using the NCI60 to Predict Sensitivity
          Potti et al (2006), Nature Medicine, 12:1294-1300.
The main conclusion is that we can use microarray data from
cell lines (the NCI60) to define drug response “signatures”,
which can be used to predict whether patients will respond.
They provide examples using 7 commonly used agents.
This got people at MDA very excited.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          3
                                       Fit Training Data
We want the test data to split like this...
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          4
                                        Fit Testing Data
But it doesn’t. Did we do something wrong?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES               5
                     5-FU Heatmaps
     Nat Med Paper
 
 G ENOMIC S IGNATURES                5
                     5-FU Heatmaps
     Nat Med Paper      Our t-tests
 
 G ENOMIC S IGNATURES                                                         5
                                         5-FU Heatmaps
     Nat Med Paper                                Our t-tests Reported Genes
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          6
                                   Their List and Ours
> temp <- cbind(
          sort(rownames(pottiUpdated)[fuRows]),
          sort(rownames(pottiUpdated)[
                       fuTQNorm@p.values <= fuCut]);
> colnames(temp) <- c(""Theirs"", ""Ours"");
> temp
             Theirs                               Ours
...
[3,] ""1881_at""                                    ""1882_g_at""
[4,] ""31321_at""                                   ""31322_at""
[5,] ""31725_s_at"" ""31726_at""
[6,] ""32307_r_at"" ""32308_r_at""
...
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                                                                                                                                                                 7
                             Offset P-Values: Other Drugs
                                           Topotecan+1                                                   Etoposide+1                                                        Adriamycin+1
                                 1.0                                                         1.0                                                                  1.0
                                 0.8                                                         0.8                                                                  0.8
                                                                                                                                                                                                                 ●
                                 0.6                                                         0.6                                                                  0.6
                       P Value                                                     P Value                                                              P Value                                              ●
                                                                                                                                                                                                             ●
                                 0.4                                                         0.4                                                                  0.4
                                 0.2                                                         0.2                                                                  0.2
                                                                                                                                                                                                             ●
                                 0.0                                                         0.0                                                                  0.0
                                                                                                                                                                                                             ●
                                       ●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●●
                                           ●
                                           ●
                                           ●●
                                            ●
                                            ●
                                            ●●
                                             ●
                                             ●
                                             ●●
                                              ●
                                              ●
                                              ●●
                                               ●
                                               ●
                                               ●●
                                                ●
                                                ●
                                                ●●
                                                 ●
                                                 ●
                                                 ●●
                                                  ●
                                                  ●
                                                  ●●
                                                   ●
                                                   ●
                                                   ●●
                                                    ●
                                                    ●
                                                    ●●
                                                     ●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●●
                                                                 ●
                                                                 ●
                                                                 ●●
                                                                  ●
                                                                  ●
                                                                  ●●
                                                                   ●
                                                                   ●
                                                                   ●●
                                                                    ●
                                                                    ●
                                                                    ●●
                                                                     ●
                                                                     ●
                                                                     ●●
                                                                      ●
                                                                      ●
                                                                      ●●
                                                                       ●
                                                                       ●
                                                                       ●●
                                                                        ●
                                                                        ●
                                                                        ●●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●                      ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●                   ●
                                                                                                                                                                        ●●
                                                                                                                                                                         ●●
                                                                                                                                                                          ●●
                                                                                                                                                                           ●●
                                                                                                                                                                            ●●
                                                                                                                                                                             ●●
                                                                                                                                                                              ●●
                                                                                                                                                                               ●●
                                                                                                                                                                                ●●
                                                                                                                                                                                 ●●
                                                                                                                                                                                  ●●
                                                                                                                                                                                   ●●
                                                                                                                                                                                    ●●
                                                                                                                                                                                     ●●
                                                                                                                                                                                      ●●
                                                                                                                                                                                       ●●
                                                                                                                                                                                        ●●
                                                                                                                                                                                         ●●
                                                                                                                                                                                          ●●
                                                                                                                                                                                           ●●
                                                                                                                                                                                            ●●
                                                                                                                                                                                             ●●
                                                                                                                                                                                              ●●
                                                                                                                                                                                               ●●
                                                                                                                                                                                                ●●
                                                                                                                                                                                                 ●●
                                                                                                                                                                                                  ●●
                                                                                                                                                                                                   ●●
                                                                                                                                                                                                    ●●
                                                                                                                                                                                                     ●●
                                                                                                                                                                                                      ●●
                                                                                                                                                                                                       ●●
                                                                                                                                                                                                        ●●
                                                                                                                                                                                                         ●●
                                                                                                                                                                                                          ●●
                                                                                                                                                                                                           ●●
                                                                                                                                                                                                            ●●
                                       0         50         100             150                    0      10 20 30 40 50                                                0      20        40        60        80
                                                     Index                                                           Index                                                            Index
                                            Paclitaxel+1                                                 Docetaxel+1                                                           Cytoxan+1
                                 1.0                                           ●             1.0                                                                  1.0                                      ●●
                                                                                                                                                                                                             ●
                                                                              ●                                                                   ●                                                       ●
                                                                            ●                                                                                                                            ●
                                                                                                                                                  ●                                                    ●●
                                 0.8                                                         0.8                                                                  0.8                                 ●
                                                                                                                                                                                                     ●
                                                                                                                                               ●
                                                                                                                                               ●
                                 0.6                                                         0.6                                                                  0.6
                       P Value                                                     P Value
                                                                                                                                             ●●
                                                                                                                                                        P Value
                                                                                                                                           ●●●                                                      ●
                                                                                                                                          ●●                                                       ●
                                                                          ●
                                 0.4                                                         0.4                                                                  0.4                          ●
                                                                                                                                       ●                                                      ●
                                                                        ●                                                                                                                   ●●
                                                                                                                                                                                         ●●●
                                                                      ●                                                                                                                ●●
                                 0.2                                                         0.2                                      ●
                                                                                                                                                                  0.2               ●●●
                                                                                                                                                                                 ●●●
                                                                                                                                     ●●                                         ●
                                                                    ●                                                              ●●                                         ●●
                                 0.0   ●●●●●●●●●●●●●●●●●●●●●●●●●●●●                          0.0   ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●                              0.0   ●●●●●●
                                       0 5           15        25             35                   0      10 20 30 40 50                                                0 5           15         25          35
                                                     Index                                                           Index                                                            Index
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                              8
                                  Using Their Software
Their software requires two input files:
1. a quantification matrix, genes by samples, with a header
    giving classifications (0 = Resistant, 1 = Sensitive, 2 = Test)
2. a list of probeset ids in the same order as the quantification
    matrix. This list must not have a header row.
What do we get?
 c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                           9
        Heatmaps Match Exactly for Most Drugs!
From the paper:
From the software:
 
 G ENOMIC S IGNATURES                                          9
        Heatmaps Match Exactly for Most Drugs!
From the paper:
From the software:
We match heatmaps but not gene lists? We’ll come back to
this, because their software also gives predictions.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          10
                  Predicting Docetaxel (Chang 03)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          11
             Predicting Adriamycin (Holleman 04)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                            12
                          There Were Other Genes...
The 50-gene list for docetaxel has 19 “outliers”.
The initial paper on the test data (Chang et al) gave a list of
92 genes that separated responders from nonresponders.
Entries 7-20 in Chang et al’s list comprise 14/19 outliers.
The others: ERCC1, ERCC4, ERBB2, BCL2L11, TUBA3.
These are the genes named to explain the biology.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          13
            RR Theme: Don’t Take My Word For It!
Read the paper! Coombes, Wang & Baggerly, Nat Med, Nov
6, 2007, 13:1276-7, author reply 1277-8.
Try it yourselves! All of the raw data, documentation*, and
code* is available from our web site (*and from Nat Med):
http://bioinformatics.mdanderson.org/
Supplements/ReproRsch-Chemo.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          14
          Potti/Nevins Reply (Nat Med 13:1277-8)
Labels for Adria are correct – details on their web page.
They’ve gotten the approach to work again. (Twice!)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                            15
        Adriamycin 0.9999+ Correlations (Reply)
 
 G ENOMIC S IGNATURES                                          15
        Adriamycin 0.9999+ Correlations (Reply)
Redone in Aug 08, “using only the 95 unique samples”
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                         16
                     The First 20 Files Now Named
Sample ID Response
   1 GSM44303                       RES                       11 GSM9694 RES
   2 GSM44304                       RES                       12 GSM9695 RES
   3 GSM9653                        RES                       13 GSM9696 RES
   4 GSM9653                        RES                       14 GSM9698 RES
   5 GSM9654                        RES                       15 GSM9699 SEN
   6 GSM9655                        RES                       16 GSM9701 RES
   7 GSM9656                        RES                       17 GSM9708 RES
   8 GSM9657                        RES                       18 GSM9708 SEN
   9 GSM9658                        SEN                       19 GSM9709 RES
10 GSM9658                          SEN                       20 GSM9711 RES
15 duplicates; 6 inconsistent. (61R, 13S, 6B) vs (22,48,10).
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          17
                                Validation 1: Hsu et al
J Clin Oncol, Oct 1, 2007, 25:4350-7.
Same approach, using Cisplatin and Pemetrexed.
For cisplatin, U133A arrays were used for training. ERCC1,
ERCC4 and DNA repair genes are identified as “important”.
With some work, we matched the heatmaps. (Gene lists?)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          18
                      The 4 We Can’t Match (Reply)
203719          at, ERCC1,
210158          at, ERCC4,
228131          at, ERCC1, and
231971          at, FANCM (DNA Repair).
The last two probesets are special.
These probesets aren’t on the U133A arrays that were used.
They’re on the U133B.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                19
                     Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
 
 G ENOMIC S IGNATURES                                        19
                     Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
May/June 2009: we learn clinical trials had begun.
2007: pemetrexed vs cisplatin, pem vs vinorelbine.
2008: docetaxel vs doxorubicin, topotecan vs dox (Moffitt).
 
 G ENOMIC S IGNATURES                                           19
                                 Some Timeline Here...
Nat Med Nov 06*, Nov 07*, Aug 08. JCO Lung Oct 07*.
Lancet Oncology Breast Dec 07*. (* errors reported)
May/June 2009: we learn clinical trials had begun.
2007: pemetrexed vs cisplatin, pem vs vinorelbine.
2008: docetaxel vs doxorubicin, topotecan vs dox (Moffitt).
Sep 1. Paper submitted to Annals of Applied Statistics.
Sep 14. Paper online at Annals of Applied Statistics.
Sep-Oct: Story covered by The Cancer Letter, Duke starts
internal investigation, suspends trials.
So, what happened next?
 c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                             20
                                            Jan 29, 2010
Their investigation’s results “strengthen ... confidence in this
evolving approach to personalized cancer treatment.”
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                         21
                     Why We’re Unhappy...
“While the reviewers approved of our sharing the report with
the NCI, we consider it a confidential document” (Duke). A
future paper will explain the methods.
 
 G ENOMIC S IGNATURES                                          21
                                 Why We’re Unhappy...
“While the reviewers approved of our sharing the report with
the NCI, we consider it a confidential document” (Duke). A
future paper will explain the methods.
                          oh, there’s just one more thing...
In mid-Nov (mid-investigation), the Duke team posted new
data for cisplatin and pemetrexed (in trials since ’07).
These included quantifications for 59 ovarian cancer test
samples (from GSE3149) used for predictor validation.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          22
                   We Tried Matching The Samples
We correlated the 59 vectors with all samples in GSE3149.
43 samples are mislabeled; 16 don’t match at all.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                         23
                          FOI(L)A!
April 7: Paul Goldberg of the Cancer Letter requests “access
to and copies of the report (and attendant data)” from the NCI
under the Freedom of Information Act (FOIA).
May 3: redacted report supplied.
 
 G ENOMIC S IGNATURES                                          23
                                                 FOI(L)A!
April 7: Paul Goldberg of the Cancer Letter requests “access
to and copies of the report (and attendant data)” from the NCI
under the Freedom of Information Act (FOIA).
May 3: redacted report supplied.
“we were unable to identify a place where the statistical
methods were described in sufficient detail to independently
replicate the findings of the papers.” – review panel
The report makes no mention of the problems with
cisplatin/pemetrexed that arose during the investigation.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          24
                                            May 14, 2010
“We have asked [CALGB] to remove the Lung Metagene
Score from the trial, because we were unable to confirm the
score’s utility” – Jeff Abrams, CTEP director
(The NCI doesn’t directly sponsor the resumed trials.)
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          25
                                           July 16, 2010
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          26
                                           July 19, 2010
“Duke administrators accomplished something monumental:
they triggered a public expression of outrage from
biostatisticians.”
A Baron, K Bandeen-Roche, D Berry, J Bryan,
V Carey, K Chaloner, M Delorenzi, B Efron,
R Elston, D Ghosh, J Goldberg, S Goodman,
F Harrell, S Hilsenbeck, W Huber, R Irizarry,
C Kendziorski, M Kosorok, T Louis, JS Marron,
M Newton, M Ochs, G Parmigiani*, J Quackenbush,
G Rosner, I Ruczinski, Y Shyr*, S Skates,
TP Speed, JD Storey, Z Szallasi, R Tibshirani,
S Zeger
Req to Varmus, DoD, ORI, Duke: suspend trials.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                   27
                 Subsequent Events, and a Caveat
Duke announces trials resuspended
NPR blog, Science blog, Nature blog, NYT blog, article
Lancet Oncology issues Expression of Concern
Varmus & Duke request IOM Involvement
Questions raised about NEJM paper
JCO launches investigation
More awards found to be wrong, COI claims
http://groups.google.com/group/reproducible-research
Correspondence to Nature
 
 G ENOMIC S IGNATURES                                          27
                 Subsequent Events, and a Caveat
Duke announces trials resuspended
NPR blog, Science blog, Nature blog, NYT blog, article
Lancet Oncology issues Expression of Concern
Varmus & Duke request IOM Involvement
Questions raised about NEJM paper
JCO launches investigation
More awards found to be wrong, COI claims
http://groups.google.com/group/reproducible-research
Correspondence to Nature
We’ve seen problems like these before. CAMDA 2002.
Proteomics 2003-5. TCGA current. Others at MDA.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          28
                                   Some Observations
The most common mistakes are simple.
Confounding in the Experimental Design
Mixing up the sample labels
Mixing up the gene labels
Mixing up the group labels
(Most mixups involve simple switches or offsets)
This simplicity is often hidden.
Incomplete documentation
Unfortunately, we suspect
The most simple mistakes are common.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          29
                        What Should the Norm Be?
For papers?
Things we look for:
1. Data (often mentioned, given MIAME)
2. Provenance
3. Code
4. Descriptions of Nonscriptable Steps
5. Descriptions of Planned Design, if Used.
For clinical trials?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          30
                                         Some Lessons
Is our own work reproducible?
Literate Programming. For the past two years, we have
required reports to be prepared in Sweave.
Reusing Templates.
Report Structure.
Executive Summaries.
Appendices. Some things we want to know all the time:
SessionInfo, Saves, and File Location.
The buzz phrase is reproducible research.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          31
                           Some Acknowledgements
Kevin Coombes
Shannon Neeley, Jing Wang
David Ransohoff, Gordon Mills
Jane Fridlyand, Lajos Pusztai, Zoltan Szallasi
MDACC Ovarian SPORE, Lung SPORE, Breast SPORE
Now in the Annals of Applied Statistics! Baggerly and
Coombes (2009), 3(4):1309-34.
http://bioinformatics.mdanderson.org/
Supplements/ReproRsch-All
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          32
                         Validation 2: Bonnefoi et al
Lancet Oncology, Dec 2007, 8:1071-8. (early access Nov 14)
Similar approach, using signatures for Fluorouracil, Epirubicin
Cyclophosphamide, and Taxotere to predict response to
combination therapies: FEC and TET.
Potentially improves ER- response from 44% to 70%.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                                          33
             We Might Expect Some Differences...
   High Sample Correlations                                   Array Run Dates
     after Centering by Gene
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                             34
                       How Are Results Combined?
Potti et al predict response to TFAC, Bonnefoi et al to TET
and FEC. Let P() indicate prob sensitive. The rules used are
as follows.
P (T F AC) = P (T )+P (F )+P (A)+P (C)−P (T )P (F )P (A)P (C).
                                 P (ET ) = max[P (E), P (T )].
                                           5                   1
                   P (F EC) = [P (F ) + P (E) + P (C)] − .
                                           8                   4
Each rule is different.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          35
       Predictions for Individual Drugs? (Reply)
Does cytoxan make sense?
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                        36
                  What About Blinded Validation?
“Data was made available to us, blinded. All we got was the
gene expression data. We ran the predictions and sent it
back to the EORTC investigators” – Joe Nevins, Oct 2.
 
 G ENOMIC S IGNATURES                                          36
                   What About Blinded Validation?
“Data was made available to us, blinded. All we got was the
gene expression data. We ran the predictions and sent it
back to the EORTC investigators” – Joe Nevins, Oct 2.
Sample info supplied:
Arm, Composite label
A, npCR Ep P- T3 N1 HB01 ...
A, pCR Ep Pp T2 N1 HB04
The data weren’t blinded.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                       37
                     Temozolomide Heatmaps
Augustine et al., 2009, Clin
Can Res, 15:502-10, Fig 4A.
Temozolomide, NCI-60.
 
 G ENOMIC S IGNATURES                                                                      37
                            Temozolomide Heatmaps
Augustine et al., 2009, Clin                                  Hsu et al., 2007, J Clin
Can Res, 15:502-10, Fig 4A.                                   Oncol, 25:4350-7, Fig 1A.
Temozolomide, NCI-60.                                         Cisplatin, Gyorffy cell lines.
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
 
 G ENOMIC S IGNATURES                                          38
                                                     Index
Title
Cell Line Story
1. Trying it Ourselves
2. Matching Features
3. Using Software/Making Predictions
4. The Reply
5. Adriamycin Followup
6. Hsu et al (Cisplatin)
7. Bonnefoi et al (Combination Therapy)
8. More Recent (Temozolomide)
9. Timeline, Trials, Cancer Letter
10. Trial Restart and Objections
11. FOIA
12. Final Lessons
c Copyright 2007-2010, Keith A. Baggerly and Kevin R. Coombes
"
"./05_ReproducibleResearch/lectures/caching.pdf"," 
 "
"./05_ReproducibleResearch/lectures/CaseStudy_AP.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./05_ReproducibleResearch/lectures/CaseStudy_Baggerly.pdf"," 
 "
"./05_ReproducibleResearch/lectures/Checklist.pdf","Reproducible Research Checklist
What to Do and What Not to Do
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 DO: Start With Good Science
· Garbage in, garbage out
· Coherent, focused question simplifies many problems
· Working with good collaborators reinforces good practices
· Something that's interesting to you will (hopefully) motivate good habits
                                                                            2/13
 
 DON'T: Do Things By Hand
 · Editing spreadsheets of data to ""clean it up""
     - Removing outliers
     - QA / QC
     - Validating
 · Editing tables or figures (e.g. rounding, formatting)
 · Downloading data from a web site (clicking links in a web browser)
 · Moving data around your computer; splitting / reformatting data files
 · ""We're just going to do this once....""
Things done by hand need to be precisely documented (this is harder than it sounds)
                                                                                    3/13
 
 DON'T: Point And Click
· Many data processing / statistical analysis packages have graphical user interfaces (GUIs)
· GUIs are convenient / intuitive but the actions you take with a GUI can be difficult for others to
  reproduce
· Some GUIs produce a log file or script which includes equivalent commands; these can be saved
  for later examination
· In general, be careful with data analysis software that is highly interactive; ease of use can
  sometimes lead to non-reproducible analyses
· Other interactive software, such as text editors, are usually fine
                                                                                                4/13
 
 DO: Teach a Computer
 · If something needs to be done as part of your analysis / investigation, try to teach your computer
   to do it (even if you only need to do it once)
 · In order to give your computer instructions, you need to write down exactly what you mean to do
   and how it should be done
 · Teaching a computer almost guarantees reproducibilty
For example, by hand, you can
  1. Go to the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/
  2. Download the Bike Sharing Dataset by clicking on the link to the Data Folder, then clicking on
      the link to the zip file of dataset, and choosing ""Save Linked File As..."" and then saving it to a
      folder on your computer
                                                                                                     5/13
 
 DO: Teach a Computer
Or You can teach your computer to do the same thing using R:
 download.file(""http://archive.ics.uci.edu/ml/machine-learning-databases/00275/
                 Bike-Sharing-Dataset.zip"", ""ProjectData/Bike-Sharing-Dataset.zip"")
Notice here that
 · The full URL to the dataset file is specified (no clicking through a series of links)
 · The name of the file saved to your local computer is specified
 · The directory in which the file was saved is specified (""ProjectData"")
 · Code can always be executed in R (as long as link is available)
                                                                                         6/13
 
 DO: Use Some Version Control
· Slow things down
· Add changes in small chunks (don't just do one massive commit)
· Track / tag snapshots; revert to old versions
· Software like GitHub / BitBucket / SourceForge make it easy to publish results
                                                                                 7/13
 
 DO: Keep Track of Your Software Environment
· If you work on a complex project involving many tools / datasets, the software and computing
  environment can be critical for reproducing your analysis
· Computer architecture: CPU (Intel, AMD, ARM), GPUs,
· Operating system: Windows, Mac OS, Linux / Unix
· Software toolchain: Compilers, interpreters, command shell, programming languages (C, Perl,
  Python, etc.), database backends, data analysis software
· Supporting software / infrastructure: Libraries, R packages, dependencies
· External dependencies: Web sites, data repositories, remote databases, software repositories
· Version numbers: Ideally, for everything (if available)
                                                                                             8/13
 
 DO: Keep Track of Your Software Environment
sessionInfo()
## R version 3.0.2 Patched (2014-01-20 r64849)
## Platform: x86_64-apple-darwin13.0.0 (64-bit)
##
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
##
## attached base packages:
## [1] stats     graphics grDevices utils      datasets  base
##
## other attached packages:
## [1] slidify_0.3.3
##
## loaded via a namespace (and not attached):
## [1] evaluate_0.5.1 formatR_0.10   knitr_1.5      markdown_0.6.3
## [5] stringr_0.6.2 tools_3.0.2     whisker_0.3-2  yaml_2.1.8
                                                                     9/13
 
 DON'T: Save Output
· Avoid saving data analysis output (tables, figures, summaries, processed data, etc.), except
  perhaps temporarily for efficiency purposes.
· If a stray output file cannot be easily connected with the means by which it was created, then it is
  not reproducible.
· Save the data + code that generated the output, rather than the output itself
· Intermediate files are okay as long as there is clear documentation of how they were created
                                                                                                10/13
 
 DO: Set Your Seed
· Random number generators generate pseudo-random numbers based on an initial seed (usually
  a number or set of numbers)
    - In R you can use the set.seed() function to set the seed and to specify the random
       number generator to use
· Setting the seed allows for the stream of random numbers to be exactly reproducible
· Whenever you generate random numbers for a non-trivial purpose, always set the seed
                                                                                      11/13
 
 DO: Think About the Entire Pipeline
· Data analysis is a lengthy process; it is not just tables / figures / reports
· Raw data → processed data → analysis → report
· How you got the end is just as important as the end itself
· The more of the data analysis pipeline you can make reproducible, the better for everyone
                                                                                            12/13
 
 Summary: Checklist
· Are we doing good science?
· Was any part of this analysis done by hand?
    - If so, are those parts precisely document?
    - Does the documentation match reality?
· Have we taught a computer to do as much as possible (i.e. coded)?
· Are we using a version control system?
· Have we documented our software environment?
· Have we saved any output that we cannot reconstruct from original data + code?
· How far back in the analysis pipeline can we go before our results are no longer (automatically)
  reproducible?
                                                                                             13/13
"
"./05_ReproducibleResearch/lectures/EvidenceBasedDataAnalysis.pdf","Reproducible Research with Evidence-
based Data Analysis
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Replication and Reproducibility
Replication
 · Focuses on the validity of the scientific claim
 · ""Is this claim true?""
 · The ultimate standard for strengthening scientific evidence
 · New investigators, data, analytical methods, laboratories, instruments, etc.
 · Particularly important in studies that can impact broad policy or regulatory decisions
                                                                                          2/46
 
 Replication and Reproducibility
Reproducibility
 · Focuses on the validity of the data analysis
 · ""Can we trust this analysis?""
 · Arguably a minimum standard for any scientific study
 · New investigators, same data, same methods
 · Important when replication is impossible
                                                        3/46
 
 Background and Underlying Trends
· Some studies cannot be replicated: No time, No money, Unique/opportunistic
· Technology is increasing data collection throughput; data are more complex and high-
  dimensional
· Existing databases can be merged to become bigger databases (but data are used off-label)
· Computing power allows more sophisticated analyses, even on ""small"" data
· For every field ""X"" there is a ""Computational X""
                                                                                            4/46
 
 The Result?
· Even basic analyses are difficult to describe
· Heavy computational requirements are thrust upon people without adequate training in statistics
  and computing
· Errors are more easily introduced into long analysis pipelines
· Knowledge transfer is inhibited
· Results are difficult to replicate or reproduce
· Complicated analyses cannot be trusted
                                                                                             5/46
 
 What is Reproducible Research?
                               6/46
 
 What is Reproducible Research?
                               7/46
 
 What is Reproducible Research?
                               8/46
 
 What is Reproducible Research?
                               9/46
 
 What is Reproducible Research?
                               10/46
 
 What Problem Does Reproducibility Solve?
What we get
· Transparency
· Data Availability
· Software / Methods Availability
· Improved Transfer of Knowledge
                                         11/46
 
 What Problem Does Reproducibility Solve?
What we get
· Transparency
· Data Availability
· Software / Methods Availability
· Improved Transfer of Knowledge
What we do NOT get
· Validity / Correctness of the analysis
                                         12/46
 
 What Problem Does Reproducibility Solve?
What we get
 · Transparency
 · Data Availability
 · Software / Methods Availability
 · Improved Transfer of Knowledge
What we do NOT get
 · Validity / Correctness of the analysis
An analysis can be reproducible and still be wrong
We want to know “can we trust this analysis?”
Does requiring reproducibility deter bad analysis?
                                                   13/46
 
 Problems with Reproducibility
The premise of reproducible research is that with data/code available, people can check each other
and the whole system is self-correcting
 · Addresses the most “downstream” aspect of the research process – post-publication
 · Assumes everyone plays by the same rules and wants to achieve the same goals (i.e. scientific
   discovery)
                                                                                              14/46
 
 An Analogy from Asthma
                       15/46
 
 An Analogy from Asthma
                       16/46
 
 An Analogy from Asthma
                       17/46
 
 Scientific Dissemination Process
                                 18/46
 
 Scientific Dissemination Process
                                 19/46
 
 Scientific Dissemination Process
                                 20/46
 
 Scientific Dissemination Process
                                 21/46
 
 Scientific Dissemination Process
                                 22/46
 
 At Biostatistics
                 23/46
 
 At Biostatistics
                 24/46
 
 Who Reproduces Research?
· For reproducibility to be effective as a means to check validity, someone needs to do something
    - Re-run the analysis; check results match
    - Check the code for bugs/errors
    - Try alternate approaches; check sensitivity
· The need for someone to do something is inherited from traditional notion of replication
· Who is ""someone"" and what are their goals?
                                                                                               25/46
 
 Who Reproduces Research?
                         26/46
 
 Who Reproduces Research?
                         27/46
 
 Who Reproduces Research?
                         28/46
 
 Who Reproduces Research?
                         29/46
 
 The Story So Far
· Reproducibility brings transparency (wrt code+data) and increased transfer of knowledge
· A lot of discussion about how to get people to share data
· Key question of ""can we trust this analysis?"" is not addressed by reproducibility
· Reproducibility addresses potential problems long after they’ve occurred (""downstream"")
· Secondary analyses are inevitably coloured by the interests/motivations of others
                                                                                          30/46
 
 Evidence-based Data Analysis
· Most data analyses involve stringing together many different tools and methods
· Some methods may be standard for a given field, but others are often applied ad hoc
· We should apply thoroughly studied (via statistical research), mutually agreed upon methods to
  analyze data whenever possible
· There should be evidence to justify the application of a given method
                                                                                            31/46
 
 Evidence-based Data Analysis
                             32/46
 
 Evidence-based Data Analysis
                             33/46
 
 Evidence-based Data Analysis
· Create analytic pipelines from evidence-based components – standardize it
· A Deterministic Statistical Machine http://goo.gl/Qvlhuv
· Once an evidence-based analytic pipeline is established, we shouldn’t mess with it
· Analysis with a “transparent box”
· Reduce the ""researcher degrees of freedom""
· Analogous to a pre-specified clinical trial protocol
                                                                                     34/46
 
 Deterministic Statistical Machine
                                  35/46
 
 Case Study: Estimating Acute Effects of
Ambient Air Pollution Exposure
· Acute/short-term effects typically estimated via panel studies or time series studies
· Work originated in late 1970s early 1980s
· Key question: ""Are short-term changes in pollution associated with short-term changes in a
  population health outcome?""
· Studies usually conducted at community level
· Long history of statistical research investigating proper methods of analysis
                                                                                        36/46
 
 Data from New York City
                        37/46
 
 Case Study: Estimating Acute Effects of
Ambient Air Pollution Exposure
· Can we encode everything that we have found in statistical/epidemiological research into a single
  package?
· Time series studies do not have a huge range of variation; typically involves similar types of data
  and similar questions
· We can create a deterministic statistical machine for this area?
                                                                                                 38/46
 
 DSM Modules for Time Series Studies of Air
Pollution and Health
 1. Check for outliers, high leverage, overdispersion
 2. Fill in missing data? NO!
 3. Model selection: Estimate degrees of freedom to adjust for unmeasured confounders
    · Other aspects of model not as critical
 4. Multiple lag analysis
 5. Sensitivity analysis wrt
    · Unmeasured confounder adjustment
    · Influential points
                                                                                      39/46
 
 Where to Go From Here?
· One DSM is not enough, we need many!
· Different problems warrant different approaches and expertise
· A curated library of machines providing state-of-the art analysis pipelines
· A CRAN/CPAN/CTAN/… for data analysis
· Or a “Cochrane Collaboration” for data analysis
                                                                              40/46
 
 A Model: Cochrane Collaboration
                                41/46
 
 A Model: Cochrane Collaboration
                                42/46
 
 A Model: Cochrane Collaboration
                                43/46
 
 A Model: Cochrane Collaboration
                                44/46
 
 A Curated Library of Data Analysis
· Provide packages that encode data analysis pipelines for given problems, technologies,
  questions
· Curated by experts knowledgeable in the field
· Documentation/references given supporting each module in the pipeline
· Changes introduced after passing relevant benchmarks/unit tests
                                                                                   45/46
 
 Summary
· Reproducible research is important, but does not necessarily solve the critical question of
  whether a data analysis is trustworthy
· Reproducible research focuses on the most ""downstream"" aspect of research dissemination
· Evidence-based data analysis would provide standardized, best practices for given scientific
  areas and questions
· Gives reviewers an important tool without dramatically increasing the burden on them
· More effort should be put into improving the quality of ""upstream"" aspects of scientific research
                                                                                                  46/46
"
"./05_ReproducibleResearch/lectures/knitr.pdf"," 
 "
"./05_ReproducibleResearch/lectures/LevelsOfDetail.pdf","Communicating Results
Specifying Levels of Detail
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 tl;dr
 · People are busy, especially managers and leaders
 · Results of data analyses are sometimes presented in oral form, but often the first cut is presented
   via email
 · It is often useful to breakdown the results of an analysis into different levels of granularity / detail
 · Getting responses from busy people: http://goo.gl/sJDb9V
                                                                                                         2/5
 
 Hierarchy of Information: Research Paper
· Title / Author list
· Abstract
· Body / Results
· Supplementary Materials / the gory details
· Code / Data / really gory details
                                             3/5
 
 Hierarchy of Information: Email Presentation
· Subject line / Sender info
    - At a minimum; include one
    - Can you summarize findings in one sentence?
· Email body
    - A brief description of the problem / context; recall what was proposed and executed;
      summarize findings / results; 1–2 paragraphs
    - If action needs to be taken as a result of this presentation, suggest some options and make
      them as concrete as possible.
    - If questions need to be addressed, try to make them yes / no
                                                                                               4/5
 
 Hierarchy of Information: Email Presentation
· Attachment(s)
    - R Markdown file
    - knitr report
    - Stay concise; don't spit out pages of code (because you used knitr we know it's available)
· Links to Supplementary Materials
    - Code / Software / Data
    - GitHub repository / Project web site
                                                                                                 5/5
"
"./05_ReproducibleResearch/lectures/Markdown.pdf","Introduction to Markdown
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is Markdown?
""Markdown is a text-to-HTML conversion tool for web writers. Markdown
allows you to write using an easy-to-read, easy-to-write plain text format,
then convert it to structurally valid XHTML (or HTML).""
- John Gruber, creator of Markdown
                                                                            2/11
 
 Markdown Syntax
Italics
 *This text will appear italicized!*
This text will appear italicized!
                                     3/11
 
 Markdown Syntax
Bold
 **This text will appear bold!**
This text will appear bold!
                                 4/11
 
 Markdown Syntax
Headings
 ## This is a secondary heading
 ### This is a tertiary heading
This is a secondary heading
This is a tertiary heading
                                5/11
 
 Markdown Syntax
Unordered Lists
 - first item in list
 - second item in list
 - third item in list
· first item in list
· second item in list
· third item in list
                       6/11
 
 Markdown Syntax
Ordered Lists
1. first item in list
2. second item in list
3. third item in list
 1. first item in list
 2. second item in list
 3. third item in list
                        7/11
 
 Markdown Syntax
Links
 [Johns Hopkins Bloomberg School of Public Health](http://www.jhsph.edu/)
 [Download R](http://www.r-project.org/)
 [RStudio](http://www.rstudio.com/)
Johns Hopkins Bloomberg School of Public Health
Download R
RStudio
                                                                          8/11
 
 Markdown Syntax
Advanced Linking
     I spend so much time reading [R bloggers][1] and [Simply Statistics][2]!
     [1]: http://www.r-bloggers.com/    ""R bloggers""
     [2]: http://simplystatistics.org/ ""Simply Statistics""
I spend so much time reading R bloggers and Simply Statistics!
                                                                              9/11
 
 Markdown Syntax
Newlines
 · Newlines require a double space after the end of a line.
 First line
 Second line
First line Second line
 First line
 Second line
First line
Second line
                                                            10/11
 
 Markdown Resources
· The Offical Markdown Documentation
· Github's Markdown Guide
                                     11/11
"
"./05_ReproducibleResearch/lectures/organizingADataAnalysis.pdf","Organizing a Data Analysis
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Data analysis files
· Data
    - Raw data
    - Processed data
· Figures
    - Exploratory figures
    - Final figures
· R code
    - Raw / unused scripts
    - Final scripts
    - R Markdown files
· Text
    - README files
    - Text of analysis / report
                                2/12
 
 Raw Data
· Should be stored in your analysis folder
· If accessed from the web, include url, description, and date accessed in README
                                                                                  3/12
 
 Processed data
· Processed data should be named so it is easy to see which script generated the data.
· The processing script - processed data mapping should occur in the README
· Processed data should be tidy
                                                                                       4/12
 
 Exploratory figures
· Figures made during the course of your analysis, not necessarily part of your final report.
· They do not need to be ""pretty""
                                                                                              5/12
 
 Final Figures
· Usually a small subset of the original figures
· Axes/colors set to make the figure clear
· Possibly multiple panels
                                                 6/12
 
 Raw scripts
· May be less commented (but comments help you!)
· May be multiple versions
· May include analyses that are later discarded
                                                 7/12
 
 Final scripts
· Clearly commented
    - Small comments liberally - what, when, why, how
    - Bigger commented blocks for whole sections
· Include processing details
· Only analyses that appear in the final write-up
                                                      8/12
 
 R markdown files
· R markdown files can be used to generate reproducible reports
· Text and R code are integrated
· Very easy to create in Rstudio
                                                                9/12
 
 Readme files
· Not necessary if you use R markdown
· Should contain step-by-step instructions for analysis
· Here is an example https://github.com/jtleek/swfdr/blob/master/README
                                                                        10/12
 
 Text of the document
· It should include a title, introduction (motivation), methods (statistics you used), results (including
  measures of uncertainty), and conclusions (including potential problems)
· It should tell a story
· It should not include every analysis you performed
· References should be included for statistical methods
                                                                                                     11/12
 
 Further resources
· Information about a non-reproducible study that led to cancer patients being mistreated: The
  Duke Saga Starter Set
· Reproducible research and Biostatistics
· Managing a statistical analysis project guidelines and best practices
· Project template - a pre-organized set of files for data analysis
                                                                                           12/12
"
"./05_ReproducibleResearch/lectures/ReproducibleResearchConcepts.pdf"," 
 "
"./05_ReproducibleResearch/lectures/RMarkdown.pdf","R Markdown
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What is Markdown?
· Created by John Gruber and Aaron Swartz
· A simplified version of ""markup"" languages
· Allows one to focus on writing as opposed to formatting
· Simple/minimal intuitive formatting elements
· Easily converted to valid HTML (and other formats) using existing tools
· Complete information is available at http://daringfireball.net/projects/markdown/
· Some background information at http://daringfireball.net/2004/03/dive_into_markdown
                                                                                      2/4
 
 What is R Markdown?
· R markdown is the integration of R code with markdown
· Allows one to create documents containing ""live"" R code
· R code is evaluated as part of the processing of the markdown
· Results from R code are inserted into markdown document
· A core tool in literate statistical programming
                                                                3/4
 
 What is R Markdown?
· R markdown can be converted to standard markdown using the knitr package in R
· Markdown can be converted to HTML using the markdown package in R
· Any basic text editor can be used to create a markdown document; no special editing tools
  needed
· The R markdown --> markdown --> HTML work flow can be easily managed using R Studio (but
  not required)
· These slides were written in R markdown and converted to slides using the slidify package
                                                                                            4/4
"
"./05_ReproducibleResearch/lectures/RRCaseStudy.pdf","Reproducible Research Case Study
Identifying Harmful Constituents in Particulate Matter Air Pollution
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 What Causes PM to be Toxic?
· PM is composed of many different chemical elements
· Some components of PM may be more harmful than others
· Some sources of PM may be more dangerous than others
· Identifying harmful chemical constituents may lead us to strategies for controlling sources of PM
                                                                                                  2/12
 
 NMMAPS
· The National Morbidity, Mortality, and Air Pollution Study (NMMAPS) was a national study of the
  short-term health effects of ambient air pollution
· Focused primarily on particulate matter (PM10 ) and ozone (O3 )
· Health outcomes included mortality from all causes and hospitalizations for cardiovascular and
  respiratory diseases
· Key publications
    - http://www.ncbi.nlm.nih.gov/pubmed/11098531
    - http://www.ncbi.nlm.nih.gov/pubmed/11354823
· Funded by the Health Effects Institute
    - Roger Peng currently serves on the Health Effects Institute Health Review Committee
                                                                                              3/12
 
 NMMAPS and Reproducibility
· Data made available at the Internet-based Health and Air Pollution Surveillance System
  (http://www.ihapss.jhsph.edu)
· Research results and software also available at iHAPSS
· Many studies (over 67 published) have been conducted based on the public data
  http://www.ncbi.nlm.nih.gov/pubmed/22475833
· Has served as an important test bed for methodological development
                                                                                      4/12
 
 What Causes Particulate Matter to be Toxic?
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1665439/
 · Lippmann et al. found strong evidence that Ni modified the short-term effect of PM10 across 60
    US communities
 · No other PM chemical constituent seemed to have the same modifying effect
 · To simple to be true?
                                                                                              5/12
 
 A Reanalysis of the Lippmann et al. Study
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2137127/
 · Reexamine the data from NMMAPS and link with PM chemical constituent data
 · Are the findings sensitive to levels of Nickel in New York City?
                                                                             6/12
 
 Does Nickel Make PM Toxic?
· Long-term average nickel concentrations appear correlated with PM risk
· There appear to be some outliers on the right-hand side (New York City)
                                                                          7/12
 
 Does Nickel Make PM Toxic?
· Regression line statistically significant (p < 0.01 )
                                                        8/12
 
 Does Nickel Make PM Toxic?
· Adjusted regression line (blue) no longer statistically significant (p < 0.31)
                                                                                 9/12
 
 Does Nickel Make PM Toxic?
                           10/12
 
 What Have We Learned?
· New York does have very high levels of nickel and vanadium, much higher than any other US
  community
· There is evidence of a positive relationship between Ni concentrations and PM10 risk
· The strength of this relationship is highly sensitive to the observations from New York City
· Most of the information in the data is derived from just 3 observations
                                                                                               11/12
 
 Lessons Learned
· Reproducibility of NMMAPS allowed for a secondary analysis (and linking with PM chemical
  constituent data) investigating a novel hypothesis (Lippmann et al.)
· Reproducibility also allowed for a critique of that new analysis and some additional new analysis
  (Dominici et al.)
· Original hypothesis not necessarily invalidated, but evidence not as strong as originally
  suggested (more work should be done)
· Reproducibility allows for the scientific discussion to occur in a timely and informed manner
· This is how science works
                                                                                                12/12
"
"./05_ReproducibleResearch/lectures/Scripting.pdf"," 
 "
"./05_ReproducibleResearch/lectures/structureOfADataAnalysis1.pdf","Structure of a Data Analysis
Part 1
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Steps in a data analysis
· Define the question
· Define the ideal data set
· Determine what data you can access
· Obtain the data
· Clean the data
· Exploratory data analysis
· Statistical prediction/modeling
· Interpret results
· Challenge results
· Synthesize/write up results
· Create reproducible code
                                     2/15
 
 Steps in a data analysis
· Define the question
· Define the ideal data set
· Determine what data you can access
· Obtain the data
· Clean the data
· Exploratory data analysis
· Statistical prediction/modeling
· Interpret results
· Challenge results
· Synthesize/write up results
· Create reproducible code
                                     3/15
 
 The key challenge in data analysis
""Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew all of
the given information in advance? Where you didn’t have a surplus of information and have to filter it
out, or you had insufficient information and have to go find some?""
                                     Dan Myer, Mathematics Educator
                                                                                                 4/15
 
 Defining a question
 1. Statistical methods development
 2. Danger zone!!!
 3. Proper data analysis
                                    5/15
 
 An example
Start with a general question
Can I automatically detect emails that are SPAM that are not?
Make it concrete
Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
                                                                                   6/15
 
 Define the ideal data set
· The data set may depend on your goal
    - Descriptive - a whole population
    - Exploratory - a random sample with many variables measured
    - Inferential - the right population, randomly sampled
    - Predictive - a training and test data set from the same population
    - Causal - data from a randomized study
    - Mechanistic - data about all components of the system
                                                                         7/15
 
 Our example
http://www.google.com/about/datacenters/inside/
                                                8/15
 
 Determine what data you can access
· Sometimes you can find data free on the web
· Other times you may need to buy the data
· Be sure to respect the terms of use
· If the data don't exist, you may need to generate it yourself
                                                                9/15
 
 Back to our example
                    10/15
 
 A possible solution
http://archive.ics.uci.edu/ml/datasets/Spambase
                                                11/15
 
 Obtain the data
· Try to obtain the raw data
· Be sure to reference the source
· Polite emails go a long way
· If you will load the data from an internet source, record the url and time accessed
                                                                                      12/15
 
 Our data set
http://search.r-project.org/library/kernlab/html/spam.html
                                                           13/15
 
 Clean the data
· Raw data often needs to be processed
· If it is pre-processed, make sure you understand how
· Understand the source of the data (census, sample, convenience sample, etc.)
· May need reformating, subsampling - record these steps
· Determine if the data are good enough - if not, quit or change data
                                                                               14/15
 
 Our cleaned data set
 # If it isn't installed, install the kernlab package with install.packages()
 library(kernlab)
 data(spam)
 str(spam[, 1:5])
 'data.frame':     4601 obs. of 5 variables:
  $ make    : num 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
  $ address: num 0.64 0.28 0 0 0 0 0 0 0 0.12 ...
  $ all     : num 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
  $ num3d : num 0 0 0 0 0 0 0 0 0 0 ...
  $ our     : num 0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
http://search.r-project.org/library/kernlab/html/spam.html
                                                                              15/15
"
"./05_ReproducibleResearch/lectures/structureOfADataAnalysis2.pdf","Structure of a Data Analysis
Part 2
Roger D. Peng, Associate Professor of Biostatistics
Johns Hopkins Bloomberg School of Public Health
 
 Steps in a data analysis
· Define the question
· Define the ideal data set
· Determine what data you can access
· Obtain the data
· Clean the data
· Exploratory data analysis
· Statistical prediction/modeling
· Interpret results
· Challenge results
· Synthesize/write up results
· Create reproducible code
                                     2/25
 
 Steps in a data analysis
· Define the question
· Define the ideal data set
· Determine what data you can access
· Obtain the data
· Clean the data
· Exploratory data analysis
· Statistical prediction/modeling
· Interpret results
· Challenge results
· Synthesize/write up results
· Create reproducible code
                                     3/25
 
 An example
Start with a general question
Can I automatically detect emails that are SPAM or not?
Make it concrete
Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
                                                                                   4/25
 
 Our data set
http://search.r-project.org/library/kernlab/html/spam.html
                                                           5/25
 
 Subsampling our data set
We need to generate a test and training set (prediction)
# If it isn't installed, install the kernlab package
library(kernlab)
data(spam)
# Perform the subsampling
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)
## trainIndicator
##    0    1
## 2314 2287
trainSpam = spam[trainIndicator == 1, ]
testSpam = spam[trainIndicator == 0, ]
                                                         6/25
 
 Exploratory data analysis
· Look at summaries of the data
· Check for missing data
· Create exploratory plots
· Perform exploratory analyses (e.g. clustering)
                                                 7/25
 
 Names
names(trainSpam)
##  [1] ""make""            ""address""          ""all""
##  [4] ""num3d""           ""our""              ""over""
##  [7] ""remove""          ""internet""         ""order""
## [10] ""mail""            ""receive""          ""will""
## [13] ""people""          ""report""           ""addresses""
## [16] ""free""            ""business""         ""email""
## [19] ""you""             ""credit""           ""your""
## [22] ""font""            ""num000""           ""money""
## [25] ""hp""              ""hpl""              ""george""
## [28] ""num650""          ""lab""              ""labs""
## [31] ""telnet""          ""num857""           ""data""
## [34] ""num415""          ""num85""            ""technology""
## [37] ""num1999""         ""parts""            ""pm""
## [40] ""direct""          ""cs""               ""meeting""
## [43] ""original""        ""project""          ""re""
## [46] ""edu""             ""table""            ""conference""
## [49] ""charSemicolon""   ""charRoundbracket"" ""charSquarebracket""
## [52] ""charExclamation"" ""charDollar""       ""charHash""
## [55] ""capitalAve""      ""capitalLong""      ""capitalTotal""
## [58] ""type""                                                   8/25
 
 Head
head(trainSpam)
##    make address all num3d our over remove internet order mail receive
## 1  0.00    0.64 0.64     0 0.32 0.00   0.00        0 0.00 0.00     0.00
## 7  0.00    0.00 0.00     0 1.92 0.00   0.00        0 0.00 0.64     0.96
## 9  0.15    0.00 0.46     0 0.61 0.00   0.30        0 0.92 0.76     0.76
## 12 0.00    0.00 0.25     0 0.38 0.25   0.25        0 0.00 0.00     0.12
## 14 0.00    0.00 0.00     0 0.90 0.00   0.90        0 0.00 0.90     0.90
## 16 0.00    0.42 0.42     0 1.27 0.00   0.42        0 0.00 1.27     0.00
##    will people report addresses free business email you credit your font
## 1  0.64   0.00      0         0 0.32        0 1.29 1.93    0.00 0.96    0
## 7  1.28   0.00      0         0 0.96        0 0.32 3.85    0.00 0.64    0
## 9  0.92   0.00      0         0 0.00        0 0.15 1.23    3.53 2.00    0
## 12 0.12   0.12      0         0 0.00        0 0.00 1.16    0.00 0.77    0
## 14 0.00   0.90      0         0 0.00        0 0.00 2.72    0.00 0.90    0
## 16 0.00   0.00      0         0 1.27        0 0.00 1.70    0.42 1.27    0
##    num000 money hp hpl george num650 lab labs telnet num857 data num415
## 1       0 0.00 0     0      0      0   0    0      0      0 0.00      0
## 7       0 0.00 0     0      0      0   0    0      0      0 0.00      0
## 9       0 0.15 0     0      0      0   0    0      0      0 0.15      0
## 12      0 0.00 0     0      0      0   0    0      0      0 0.00      0
## 14      0 0.00 0     0      0      0   0    0      0      0 0.00      0   9/25
 
 Summaries
table(trainSpam$type)
##
## nonspam    spam
##    1381     906
                      10/25
 
 Plots
plot(trainSpam$capitalAve ~ trainSpam$type)
                                            11/25
 
 Plots
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
                                                       12/25
 
 Relationships between predictors
plot(log10(trainSpam[, 1:4] + 1))
                                  13/25
 
 Clustering
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
                                              14/25
 
 New clustering
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated)
                                                                15/25
 
 Statistical prediction/modeling
· Should be informed by the results of your exploratory analysis
· Exact methods depend on the question of interest
· Transformations/processing should be accounted for when necessary
· Measures of uncertainty should be reported
                                                                    16/25
 
 Statistical prediction/modeling
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y > 0.5))
cvError = rep(NA, 55)
library(boot)
for (i in 1:55) {
    lmFormula = reformulate(names(trainSpam)[i], response = ""numType"")
    glmFit = glm(lmFormula, family = ""binomial"", data = trainSpam)
    cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
## Which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)]
## [1] ""charDollar""
                                                                       17/25
 
 Get a measure of uncertainty
## Use the best model from the group
predictionModel = glm(numType ~ charDollar, family = ""binomial"", data = trainSpam)
## Get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep(""nonspam"", dim(testSpam)[1])
## Classify as `spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5] = ""spam""
                                                                                   18/25
 
 Get a measure of uncertainty
## Classification table
table(predictedSpam, testSpam$type)
##
## predictedSpam nonspam spam
##       nonspam    1346 458
##       spam         61 449
## Error rate
(61 + 458)/(1346 + 458 + 61 + 449)
## [1] 0.2243
                                    19/25
 
 Interpret results
 · Use the appropriate language
      - describes
      - correlates with/associated with
      - leads to/causes
      - predicts
 · Give an explanation
 · Interpret coefficients
 · Interpret measures of uncertainty
                                        20/25
 
 Our example
· The fraction of charcters that are dollar signs can be used to predict if an email is Spam
· Anything with more than 6.6% dollar signs is classified as Spam
· More dollar signs always means more Spam under our prediction
· Our test set error rate was 22.4%
                                                                                             21/25
 
 Challenge results
· Challenge all steps:
    - Question
    - Data source
    - Processing
    - Analysis
    - Conclusions
· Challenge measures of uncertainty
· Challenge choices of terms to include in models
· Think of potential alternative analyses
                                                  22/25
 
 Synthesize/write-up results
· Lead with the question
· Summarize the analyses into the story
· Don't include every analysis, include it
    - If it is needed for the story
    - If it is needed to address a challenge
· Order analyses according to the story, rather than chronologically
· Include ""pretty"" figures that contribute to the story
                                                                     23/25
 
 In our example
 · Lead with the question
      - Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
 · Describe the approach
      - Collected data from UCI -> created training/test sets
      - Explored relationships
      - Choose logistic model on training set by cross validation
      - Applied to test, 78% test set accuracy
 · Interpret results
      - Number of dollar signs seems reasonable, e.g. ""Make money with Viagra $ $ $ $!""
 · Challenge results
      - 78% isn't that great
      - I could use more variables
      - Why logistic regression?
                                                                                           24/25
 
 Create reproducible code
                         25/25
"
"./05_ReproducibleResearch/organizingADataAnalysis/Organizing a Data Analysis.pdf","7/24/13                                                                                            Organizing a Data Analysis
                          Organizing a Data Analysis
                          Jeffrey Leek, Assistant Professor of Biostatistics
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                       1/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Data analysis files
                 · Data
                         - Raw data
                         - Processed data
                 · Figures
                         - Exploratory figures
                         - Final figures
                 · R code
                         - Raw scripts
                         - Final scripts
                         - R Markdown files (optional)
                 · Text
                         - Readme files
                         - Text of analysis
                                                                                                                              2/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            2/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Raw Data
                 · Should be stored in your analysis folder
                 · If accessed from the web, include url, description, and date accessed in README
                                                                                                                              3/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            3/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Processed data
                 · Processed data should be named so it is easy to see which script generated the data.
                 · The processing script - processed data mapping should occur in the README
                 · Processed data should be tidy
                                                                                                                              4/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            4/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Exploratory figures
                 · Figures made during the course of your analysis, not necessarily part of your final report.
                 · They do not need to be ""pretty""
                                                                                                                              5/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            5/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Final Figures
                 · Usually a small subset of the original figures
                 · Axes/colors set to make the figure clear
                 · Possibly multiple panels
                                                                                                                              6/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            6/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Raw scripts
                 · May be less commented (but comments help you!)
                 · May be multiple versions
                 · May include analyses that are later discarded
                                                                                                                              7/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            7/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Final scripts
                 · Clearly commented
                         - Small comments liberally - what, when, why, how
                         - Bigger commented blocks for whole sections
                 · Include processing details
                 · Only analyses that appear in the final write-up
                                                                                                                              8/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            8/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              R markdown files
                 · R markdown files can be used to generate reproducible reports
                 · Text and R code are integrated
                 · Very easy to create in Rstudio
                                                                                                                              9/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                            9/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Readme files
                 · Not necessary if you use R markdown
                 · Should contain step-by-step instructions for analysis
                 · Here is an example https://github.com/jtleek/swfdr/blob/master/README
                                                                                                                              10/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                             10/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Text of the document
                 · It should include a title, introduction (motivation), methods (statistics you used), results (including
                    measures of uncertainty), and conclusions (including potential problems)
                 · It should tell a story
                 · It should not include every analysis you performed
                 · References should be included for statistical methods
                                                                                                                              11/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                             11/12
 
 7/24/13                                                                                            Organizing a Data Analysis
              Further resources
                 · Information about a non-reproducible study that led to cancer patients being mistreated: The Duke
                    Saga Starter Set
                 · Reproducible research and Biostatistics
                 · Managing a statistical analysis project guidelines and best practices
                 · Project template - a pre-organized set of files for data analysis
                                                                                                                              12/12
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/003organizingADataAnalysis/index.html#1                             12/12
"
"./05_ReproducibleResearch/ReproducibleResearchConcepts/ReproResearch.pdf","   Reproducible	  Research:	  	  
         Concepts	  and	  Ideas	  
               Reproducible	  Research	  
                                 	  
Roger	  D.	  Peng,	  Associate	  Professor	  of	  Biosta4s4cs	  
Johns	  Hopkins	  Bloomberg	  School	  of	  Public	  Health	  
 
                         Replica5on	  
• The	  ul5mate	  standard	  for	  strengthening	  scien5ﬁc	  
  evidence	  is	  replica5on	  of	  ﬁndings	  and	  conduc5ng	  
  studies	  with	  independent	  
   – Inves5gators 	  	  
   – Data	  
   – Analy5cal	  methods	  
   – Laboratories	  
   – Instruments	  
• Replica5on	  is	  par5cularly	  important	  in	  studies	  
  that	  can	  impact	  broad	  policy	  or	  regulatory	  
  decisions	  
 
   What’s	  Wrong	  with	  Replica5on?	  
• Some	  studies	  cannot	  be	  replicated	  
   – No	  5me,	  opportunis5c	  
   – No	  money	  
   – Unique	  
• Reproducible	  Research:	  Make	  analy5c	  data	  
  and	  code	  available	  so	  that	  others	  may	  
  reproduce	  ﬁndings	  
 
 How	  Can	  We	  Bridge	  the	  Gap?	  
          Replica5on               	  
                        ?	  
               Nothing        	  
 
 How	  Can	  We	  Bridge	  the	  Gap?	  
          Replica5on               	  
          Reproduciblity	  
               Nothing        	  
 
                    Why	  Do	  We	  Need	  
            Reproducible	  Research?	  
• New	  technologies	  increasing	  data	  collec5on	  
  throughput;	  data	  are	  more	  complex	  and	  
  extremely	  high	  dimensional	  
• Exis5ng	  databases	  can	  be	  merged	  into	  new	  
  “megadatabases”	  
• Compu5ng	  power	  is	  greatly	  increased,	  
  allowing	  more	  sophis5cated	  analyses	  
• For	  every	  ﬁeld	  “X”	  there	  is	  a	  ﬁeld	  
  “Computa5onal	  X”	  
 
   Example:	  Reproducible	  Air	  Pollu5on	  
                  and	  Health	  Research	  
• Es5ma5ng	  small	  (but	  important)	  health	  eﬀects	  
  in	  the	  presence	  of	  much	  stronger	  signals	  
• Results	  inform	  substan5al	  policy	  decisions,	  
  aﬀect	  many	  stakeholders	  
   – EPA	  regula5ons	  can	  cost	  billions	  of	  dollars	  
• Complex	  sta5s5cal	  methods	  are	  needed	  and	  
  subjected	  to	  intense	  scru5ny	  
 
     Internet-­‐based	  Health	  and	  Air	  
Pollu5on	  Surveillance	  System	  (iHAPSS)	  
                                   h[p://www.ihapss.jhsph.edu	  
 
 Research	  Pipeline	  
                         Ar5cle	  
                           Reader	  
 
                                  Research	  Pipeline	  
Author	  
                                                       Presenta5on	  code	  
          Processing	  code	     Analy5c	  code	                             Figures	  
 Measured	               Analy5c	            Computa5onal	  
                                                                                 Tables	    Ar5cle	  
 Data	                   Data	               Results	  
                                                                               Numerical	  
                                                                               Summaries	    Text	  
                                                                                               Reader	  
 
 Recent	  Developments	  in	  
 Reproducible	  Research	  
 
                 Recent	  Developments	  in	  
                 Reproducible	  Research	  
The	  Duke	  
Saga	  
 
 Recent	  Developments	  in	  
 Reproducible	  Research	  
 
                             The	  IOM	  Report	  
In	  the	  Discovery/Test	  Valida5on	  stage	  of	  omics-­‐based	  
      tests:	  
• Data/metadata	  used	  to	  develop	  test	  should	  be	  made	  
      publicly	  available	  
• The	  computer	  code	  and	  fully	  speciﬁed	  computa5onal	  
      procedures	  used	  for	  development	  of	  the	  candidate	  
      omics-­‐based	  test	  should	  be	  made	  sustainably	  available	  
• “Ideally,	  the	  computer	  code	  that	  is	  released	  will	  
      encompass	  all	  of	  the	  steps	  of	  computa3onal	  analysis,	  
      including	  all	  data	  preprocessing	  steps,	  that	  have	  been	  
      described	  in	  this	  chapter.	  All	  aspects	  of	  the	  analysis	  
      need	  to	  be	  transparently	  reported.”	  
	  
 
              What	  do	  We	  Need?	  
• Analy5c	  data	  are	  available	  
• Analy5c	  code	  are	  available	  
• Documenta5on	  of	  code	  and	  data	  
• Standard	  means	  of	  distribu5on	  
 
                Who	  are	  the	  Players?	  
• Authors	  
   – Want	  to	  make	  their	  research	  reproducible	  
   – Want	  tools	  for	  RR	  to	  make	  their	  lives	  easier	  (or	  at	  
     least	  not	  much	  harder)	  
• Readers	  
   – Want	  to	  reproduce	  (and	  perhaps	  expand	  upon)	  
     interes5ng	  ﬁndings	  
   – Want	  tools	  for	  RR	  to	  make	  their	  lives	  easier	  
 
                            Challenges	  
• Authors	  must	  undertake	  considerable	  eﬀort	  to	  
  put	  data/results	  on	  the	  web	  (may	  not	  have	  
  resources	  like	  a	  web	  server)	  
• Readers	  must	  download	  data/results	  individually	  
  and	  piece	  together	  which	  data	  go	  with	  which	  
  code	  sec5ons,	  etc.	  
• Readers	  may	  not	  have	  the	  same	  resources	  as	  
  authors	  
• Few	  tools	  to	  help	  authors/readers	  (although	  
  toolbox	  is	  growing!)	  
 
                             In	  Reality…	  
• Authors	  
   – Just	  put	  stuﬀ	  on	  the	  web	  
   – (Infamous)	  Journal	  supplementary	  materials	  
   – There	  are	  some	  central	  databases	  for	  various	  
     ﬁelds	  (e.g.	  biology,	  ICPSR)	  
• Readers	  
   – Just	  download	  the	  data	  and	  (try	  to)	  ﬁgure	  it	  out	  
   – Piece	  together	  the	  socware	  and	  run	  it	  
 
  Literate	  (Sta5s5cal)	  Programming	  
• An	  ar5cle	  is	  a	  stream	  of	  text	  and	  code	  
• Analysis	  code	  is	  divided	  into	  text	  and	  code	  
  “chunks”	  
• Each	  code	  chunk	  loads	  data	  and	  computes	  results	  
• Presenta5on	  code	  formats	  results	  (tables,	  ﬁgures,	  
  etc.)	  
• Ar5cle	  text	  explains	  what	  is	  going	  on	  
• Literate	  programs	  can	  be	  weaved	  to	  produce	  
  human-­‐readable	  documents	  and	  tangled	  to	  
  produce	  machine-­‐readable	  documents	  
 
   Literate	  (Sta5s5cal)	  Programming	  
• Literate	  programming	  is	  a	  general	  concept	  that	  
   requires	  
    1. A	  documenta5on	  language	  (human	  readable)	  
    2. A	  programming	  language	  (machine	  readable)	  
•    Sweave	  uses	  LATEX	  and	  R	  as	  the	  documenta5on	  
     and	  programming	  languages	  
•    Sweave	  was	  developed	  by	  Friedrich	  Leisch	  
     (member	  of	  the	  R	  Core)	  and	  is	  maintained	  by	  R	  
     core	  
•    Main	  web	  site:	  http://www.statistik.lmu.de/
     ~leisch/Sweave
 
               Sweave	  Limita5ons	  
• Sweave	  has	  many	  limita5ons	  
• Focused	  primarily	  on	  LaTeX,	  a	  diﬃcult	  to	  learn	  
  markup	  language	  used	  only	  by	  weirdos	  
• Lacks	  features	  like	  caching,	  mul5ple	  plots	  per	  
  chunk,	  mixing	  programming	  languages	  and	  
  many	  other	  technical	  items	  
• Not	  frequently	  updated	  or	  very	  ac5vely	  
  developed	  
 
  Literate	  (Sta5s5cal)	  Programming	  
• knitr	  is	  an	  alterna5ve	  (more	  recent)	  package	  
• Brings	  together	  many	  features	  added	  on	  to	  
  Sweave	  to	  address	  limita5ons	  
• knitr	  uses	  R	  as	  the	  programming	  language	  
  (although	  others	  are	  allowed)	  and	  variety	  of	  
  documenta5on	  languages	  
   – LaTeX,	  Markdown,	  HTML	  
• knitr	  was	  developed	  by	  Yihui	  Xie	  (while	  a	  
  graduate	  student	  in	  sta5s5cs	  at	  Iowa	  State)	  
• See	  h[p://yihui.name/knitr/ 	  	  
 
                            Summary	  
• Reproducible	  research	  is	  important	  as	  a	  
  minimum	  standard,	  par5cularly	  for	  studies	  
  that	  are	  diﬃcult	  to	  replicate	  
• Infrastructure	  is	  needed	  for	  crea3ng	  and	  
  distribu3ng	  reproducible	  documents,	  beyond	  
  what	  is	  currently	  available	  
• There	  is	  a	  growing	  number	  of	  tools	  for	  
  crea5ng	  reproducible	  documents	  
"
"./05_ReproducibleResearch/structureOfADataAnalysis2/Structure of a Data Analysis.pdf","7/24/13                                                                                           Structure of a Data Analysis
                          Structure of a Data Analysis
                          Part 2
                          Jeffrey Leek, Assistant Professor of Biostatistics
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                     1/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Steps in a data analysis
                 · Define the question
                 · Define the ideal data set
                 · Determine what data you can access
                 · Obtain the data
                 · Clean the data
                 · Exploratory data analysis
                 · Statistical prediction/modeling
                 · Interpret results
                 · Challenge results
                 · Synthesize/write up results
                 · Create reproducible code
                                                                                                                               2/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          2/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Steps in a data analysis
                 · Define the question
                 · Define the ideal data set
                 · Determine what data you can access
                 · Obtain the data
                 · Clean the data
                 · Exploratory data analysis
                 · Statistical prediction/modeling
                 · Interpret results
                 · Challenge results
                 · Synthesize/write up results
                 · Create reproducible code
                                                                                                                               3/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          3/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              An example
              Start with a general question
              Can I automatically detect emails that are SPAM that are not?
              Make it concrete
              Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
                                                                                                                               4/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          4/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Our data set
              http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html
                                                                                                                               5/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          5/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Subsampling our data set
              We need to generate a test and training set (prediction)
                 # If it isn't installed, install the kernlab package
                 library(kernlab)
                 data(spam)
                 # Perform the subsampling
                 set.seed(3435)
                 trainIndicator = rbinom(4601, size = 1, prob = 0.5)
                 table(trainIndicator)
                 ## trainIndicator
                 ##         0        1
                 ## 2314 2287
                 trainSpam = spam[trainIndicator == 1, ]
                 testSpam = spam[trainIndicator == 0, ]
                                                                                                                               6/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          6/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Exploratory data analysis
                 · Look at summaries of the data
                 · Check for missing data
                 · Create exploratory plots
                 · Perform exploratory analyses (e.g. clustering)
                                                                                                                               7/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          7/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Names
                 names(trainSpam)
                 ## [1] ""make""                                       ""address""                            ""all""
                 ## [4] ""num3d""                                      ""our""                                ""over""
                 ## [7] ""remove""                                     ""internet""                           ""order""
                 ## [10] ""mail""                                      ""receive""                            ""will""
                 ## [13] ""people""                                    ""report""                             ""addresses""
                 ## [16] ""free""                                      ""business""                           ""email""
                 ## [19] ""you""                                       ""credit""                             ""your""
                 ## [22] ""font""                                      ""num000""                             ""money""
                 ## [25] ""hp""                                        ""hpl""                                ""george""
                 ## [28] ""num650""                                    ""lab""                                ""labs""
                 ## [31] ""telnet""                                    ""num857""                             ""data""
                 ## [34] ""num415""                                    ""num85""                              ""technology""
                 ## [37] ""num1999""                                   ""parts""                              ""pm""
                 ## [40] ""direct""                                    ""cs""                                 ""meeting""
                 ## [43] ""original""                                  ""project""                            ""re""
                 ## [46] ""edu""                                       ""table""                              ""conference""
                 ## [49] ""charSemicolon""                             ""charRoundbracket""                   ""charSquarebracket""
                 ## [52] ""charExclamation""                           ""charDollar""                         ""charHash""
                 ## [55] ""capitalAve""                                ""capitalLong""                        ""capitalTotal""
                 ## [58] ""type""
                                                                                                                               8/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                          8/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Head
                 head(trainSpam)
                 ##         make address all num3d our over remove internet order mail receive
                 ## 1 0.00                0.64 0.64                  0 0.32 0.00 0.00                                 0 0.00 0.00     0.00
                 ## 7 0.00                0.00 0.00                  0 1.92 0.00 0.00                                 0 0.00 0.64     0.96
                 ## 9 0.15                0.00 0.46                  0 0.61 0.00 0.30                                 0 0.92 0.76     0.76
                 ## 12 0.00               0.00 0.25                  0 0.38 0.25 0.25                                 0 0.00 0.00     0.12
                 ## 14 0.00               0.00 0.00                  0 0.90 0.00 0.90                                 0 0.00 0.90     0.90
                 ## 16 0.00               0.42 0.42                  0 1.27 0.00 0.42                                 0 0.00 1.27     0.00
                 ##         will people report addresses free business email you credit your font
                 ## 1 0.64 0.00                            0                  0 0.32                    0 1.29 1.93 0.00 0.96              0
                 ## 7 1.28 0.00                            0                  0 0.96                    0 0.32 3.85 0.00 0.64              0
                 ## 9 0.92 0.00                            0                  0 0.00                    0 0.15 1.23 3.53 2.00              0
                 ## 12 0.12 0.12                           0                  0 0.00                    0 0.00 1.16 0.00 0.77              0
                 ## 14 0.00 0.90                           0                  0 0.00                    0 0.00 2.72 0.00 0.90              0
                 ## 16 0.00 0.00                           0                  0 1.27                    0 0.00 1.70 0.42 1.27              0
                 ##         num000 money hp hpl george num650 lab labs telnet num857 data num415
                 ## 1                0 0.00 0 0                            0         0 0                0             0        0 0.00    0
                 ## 7                0 0.00 0 0                            0         0 0                0             0        0 0.00    0
                 ## 9                0 0.15 0 0                            0         0 0                0             0        0 0.15    0
                 ## 12               0 0.00 0 0                            0         0 0                0             0        0 0.00    0
                 ## 14               0 0.00 0 0                            0         0 0                0             0        0 0.00    0
                 ## 16               0 0.42 0 0                            0         0 0                0             0        0 0.00    0   9/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                                        9/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Summaries
                 table(trainSpam$type)
                 ##
                 ## nonspam               spam
                 ##         1381            906
                                                                                                                               10/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           10/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Plots
                 plot(trainSpam$capitalAve ~ trainSpam$type)
                                                                                                                               11/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           11/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Plots
                 plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
                                                                                                                               12/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           12/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Relationships between predictors
                 plot(log10(trainSpam[, 1:4] + 1))
                                                                                                                               13/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           13/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Clustering
                 hCluster = hclust(dist(t(trainSpam[, 1:57])))
                 plot(hCluster)
                                                                                                                               14/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           14/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              New clustering
                 hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
                 plot(hClusterUpdated)
                                                                                                                               15/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           15/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Statistical prediction/modeling
                 · Should be informed by the results of your exploratory analysis
                 · Exact methods depend on the question of interest
                 · Transformations/processing should be accounted for when necessary
                 · Measures of uncertainty should be reported
                                                                                                                               16/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           16/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Statistical prediction/modeling
                 trainSpam$numType = as.numeric(trainSpam$type) - 1
                 costFunction = function(x, y) {
                         sum(x != (y > 0.5))
                 }
                 cvError = rep(NA, 55)
                 library(boot)
                 for (i in 1:55) {
                         lmFormula = as.formula(paste(""numType~"", names(trainSpam)[i], sep = """"))
                         glmFit = glm(lmFormula, family = ""binomial"", data = trainSpam)
                         cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
                 }
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred                                         17/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           17/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Get a measure of uncertainty
                 predictionModel = glm(numType ~ charDollar, family = ""binomial"", data = trainSpam)
                 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
                 predictionTest = predict(predictionModel, testSpam)
                 predictedSpam = rep(""nonspam"", dim(testSpam)[1])
                 predictedSpam[predictionModel$fitted > 0.5] = ""spam""
                 table(predictedSpam, testSpam$type)
                 ##
                 ## predictedSpam nonspam spam
                 ##              nonspam              1346 458
                 ##              spam                     61 449
                 (61 + 458)/(1346 + 458 + 61 + 449)
                 ## [1] 0.2243
                                                                                                                               18/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           18/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Interpret results
                 · Use the appropriate language
                         - describes
                         - correlates with/associated with
                         - leads to/causes
                         - predicts
                 · Give an explanation
                 · Interpret coefficients
                 · Interpret measures of uncertainty
                                                                                                                               19/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           19/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Our example
                 · The fraction of charcters that are dollar signs can be used to predict if an email is Spam
                 · Anything with more than 6.6% dollar signs is classified as Spam
                 · More dollar signs always means more Spam under our prediction
                 · Our test set error rate was 22.4%
                                                                                                                               20/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           20/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Challenge results
                 · Challenge all steps:
                         - Question
                         - Data source
                         - Processing
                         - Analysis
                         - Conclusions
                 · Challenge measures of uncertainty
                 · Challenge choices of terms to include in models
                 · Think of potential alternative analyses
                                                                                                                               21/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           21/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Synthesize/write-up results
                 · Lead with the question
                 · Summarize the analyses into the story
                 · Don't include every analysis, include it
                         - If it is needed for the story
                         - If it is needed to address a challenge
                 · Order analyses according to the story, rather than chronologically
                 · Include ""pretty"" figures that contribute to the story
                                                                                                                               22/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           22/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              In our example
                 · Lead with the question
                         - Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
                 · Describe the approach
                         - Collected data from UCI -> created training/test sets
                         - Explored relationships
                         - Choose logistic model on training set by cross validation
                         - Applied to test, 78% test set accuracy
                 · Interpret results
                         - Number of dollar signs seems reasonable, e.g. ""Make money with Viagra $ $ $ $!""
                 · Challenge results
                         - 78% isn't that great
                         - I could use more variables
                         - Why logistic regression?
                                                                                                                               23/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           23/24
 
 7/24/13                                                                                           Structure of a Data Analysis
              Create reproducible code
                                                                                                                               24/24
file://localhost/Users/sean/Developer/GitHub/modules/jeff/week2/002structureOfADataAnalysis2/index.html#24                           24/24
"
"./06_StatisticalInference/03_03_pValues/P-values.pdf","8/29/13                                                                                         P-values
                            P-values
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1          1/19
 
 8/29/13                                                                                         P-values
              P-values
                 · Most common measure of ""statistical significance""
                 · Commonly reported in papers
                 · Used for decision making (e.g. FDA)
                 · Controversial among statisticians
                          - http://warnercnr.colostate.edu/~anderson/thompson1.html
                                                                                                         2/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1               2/19
 
 8/29/13                                                                                         P-values
              Not everyone thinks P-values are awful
              http://simplystatistics.org/2012/01/06/p-values-and-hypothesis-testing-get-a-bad-rap-but-we/
                                                                                                           3/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                 3/19
 
 8/29/13                                                                                         P-values
              What is a P-value?
              Idea: Suppose nothing is going on - how unusual is it to see the estimate we got?
              Approach:
                   1. Define the hypothetical distribution of a data summary (statistic) when ""nothing is going on"" (null
                          hypothesis)
                   2. Calculate the summary/statistic with the data we have (test statistic)
                   3. Compare what we calculated to our hypothetical distribution and see if the value is ""extreme"" (p-
                          value)
                                                                                                                     4/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                           4/19
 
 8/29/13                                                                                         P-values
              Galton data
                 library(UsingR); data(galton)
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lm1 <- lm(galton$child ~ galton$parent)
                 abline(lm1,col=""red"",lwd=3)
              If there was no relation between mid-parent/child height would we be surprised to see a line that looks
              like this?
                                                                                                                  5/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                        5/19
 
 8/29/13                                                                                                    P-values
              Null hypothesis/distribution
                                                                                               ė
                                                                                              b1 J   b1
                                                                                                             t      J
                                                                                                     ė
                                                                                             S. E. ( b 1)
                                                                                                                 tn   2
              H0    : That there is no relationship between parent and child height (b                                  1 = 0 ). Under the null hypothesis
              the distribution is:
                                                                                                 ė
                                                                                                 b1
                                                                                                     ė       t   tn J 2
                                                                                             S. E. ( b 1 )
                                                                                                                                                       6/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                                                             6/19
 
 8/29/13                                                                                         P-values
              Null distribution
                 x <- seq(-20,20,length=100)
                 plot(x,dt(x,df=(928-2)),col=""blue"",lwd=3,type=""l"")
                                                                                                         7/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1               7/19
 
 8/29/13                                                                                         P-values
              Null distribution + observed statistic
                 x <- seq(-20,20,length=100)
                 plot(x,dt(x,df=(928-2)),col=""blue"",lwd=3,type=""l"")
                 arrows(summary(lm1)$coeff[2,3],0.25,summary(lm1)$coeff[2,3],0,col=""red"",lwd=4)
                                                                                                         8/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1               8/19
 
 8/29/13                                                                                          P-values
              Calculating p-values
                 summary(lm1)
                 Call:
                 lm(formula = galton$child ~ galton$parent)
                 Residuals:
                       Min            1Q Median                   3Q        Max
                 -7.805 -1.366 0.049 1.634 5.926
                 Coefficients:
                                             Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                   23.9415               2.8109            8.52 <2e-16 ***
                 galton$parent 0.6463                                0.0411 15.71 <2e-16 ***
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                 Residual standard error: 2.24 on 926 degrees of freedom
                 Multiple R-squared: 0.21, Adjusted R-squared: 0.21
                 F-statistic: 247 on 1 and 926 DF, p-value: <2e-16
                                                                                                          9/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                9/19
 
 8/29/13                                                                                              P-values
              A quick simulated example
                 set.seed(9898324)
                 yValues <- rnorm(10); xValues <- rnorm(10)
                 lm2 <- lm(yValues ~ xValues)
                 summary(lm2)
                 Call:
                 lm(formula = yValues ~ xValues)
                 Residuals:
                       Min            1Q Median                   3Q        Max
                 -1.546 -0.570 0.136 0.771 1.052
                 Coefficients:
                                        Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                   0.310               0.351           0.88         0.40
                 xValues                       0.289               0.389           0.74         0.48
                 Residual standard error: 0.989 on 8 degrees of freedom
                 Multiple R-squared: 0.0644,                                 Adjusted R-squared: -0.0525
                 F-statistic: 0.551 on 1 and 8 DF, p-value: 0.479
                                                                                                              10/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                     10/19
 
 8/29/13                                                                                         P-values
              A quick simulated example
                 x <- seq(-5,5,length=100)
                 plot(x,dt(x,df=(10-2)),col=""blue"",lwd=3,type=""l"")
                 arrows(summary(lm2)$coeff[2,3],0.25,summary(lm2)$coeff[2,3],0,col=""red"",lwd=4)
                                                                                                         11/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                11/19
 
 8/29/13                                                                                         P-values
              A quick simulated example
                 xCoords <- seq(-5,5,length=100)
                 plot(xCoords,dt(xCoords,df=(10-2)),col=""blue"",lwd=3,type=""l"")
                 xSequence <- c(seq(summary(lm2)$coeff[2,3],5,length=10),summary(lm2)$coeff[2,3])
                 ySequence <- c(dt(seq(summary(lm2)$coeff[2,3],5,length=10),df=8),0)
                 polygon(xSequence,ySequence,col=""red""); polygon(-xSequence,ySequence,col=""red"")
                                                                                                         12/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                12/19
 
 8/29/13                                                                                         P-values
              Simulate a ton of data sets with no signal
                 set.seed(8323); pValues <- rep(NA,100)
                 for(i in 1:100){
                    xValues <- rnorm(20);yValues <- rnorm(20)
                    pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
                 }
                 hist(pValues,col=""blue"",main="""",freq=F)
                 abline(h=1,col=""red"",lwd=3)
                                                                                                         13/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                13/19
 
 8/29/13                                                                                         P-values
              Simulate a ton of data sets with signal
                 set.seed(8323); pValues <- rep(NA,100)
                 for(i in 1:100){
                    xValues <- rnorm(20);yValues <- 0.2 * xValues + rnorm(20)
                    pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
                 }
                 hist(pValues,col=""blue"",main="""",freq=F,xlim=c(0,1)); abline(h=1,col=""red"",lwd=3)
                                                                                                         14/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                14/19
 
 8/29/13                                                                                         P-values
              Simulate a ton of data sets with signal
                 set.seed(8323); pValues <- rep(NA,100)
                 for(i in 1:100){
                    xValues <- rnorm(100);yValues <- 0.2* xValues + rnorm(100)
                    pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
                 }
                 hist(pValues,col=""blue"",main="""",freq=F,xlim=c(0,1)); abline(h=1,col=""red"",lwd=3)
                                                                                                         15/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                15/19
 
 8/29/13                                                                                         P-values
              Some typical values (single test)
                 · P < 0.05 (significant)
                 · P < 0.01 (strongly significant)
                 · P < 0.001 (very significant)
              In modern analyses, people generally report both the confidence interval and P-value. This is less true
              if many many hypotheses are tested.
                                                                                                                16/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                       16/19
 
 8/29/13                                                                                         P-values
              How you interpret the results
                 summary(lm(galton$child ~ galton$parent))$coeff
                                             Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                   23.9415             2.81088 8.517 6.537e-17
                 galton$parent 0.6463                              0.04114 15.711 1.733e-49
              A one inch increase in parental height is associated with a 0.77 inch increase in child's height (95%
              CI: 0.42-1.12 inches). This difference was statistically significant (P < 0.001 ).
                                                                                                                17/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                       17/19
 
 8/29/13                                                                                         P-values
              Be careful!
              http://xkcd.com/882/
                                                                                                         18/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                18/19
 
 8/29/13                                                                                         P-values
              Be careful!
              http://xkcd.com/882/
                                                                                                         19/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/004pValues/index.html#1                19/19
"
"./06_StatisticalInference/03_05_MultipleTesting/Multiple testing.pdf","9/3/13                                                                                                  Multiple testing
                            Multiple testing
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                  1/19
 
 9/3/13                                                                                                  Multiple testing
              Key ideas
                 · Hypothesis testing/significance analysis is commonly overused
                 · Correcting for multiple testing avoids false positives or discoveries
                 · Two key components
                          - Error measure
                          - Correction
                                                                                                                         2/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                       2/19
 
 9/3/13                                                                                                  Multiple testing
              Three eras of statistics
              The age of Quetelet and his successors, in which huge census-level data sets were brought to
              bear on simple but important questions: Are there more male than female births? Is the rate of
              insanity rising?
              The classical period of Pearson, Fisher, Neyman, Hotelling, and their successors, intellectual giants
              who developed a theory of optimal inference capable of wringing every drop of information out
              of a scientific experiment. The questions dealt with still tended to be simple Is treatment A better
              than treatment B?
              The era of scientific mass production, in which new technologies typified by the microarray allow a
              single team of scientists to produce data sets of a size Quetelet would envy. But now the flood of data
              is accompanied by a deluge of questions, perhaps thousands of estimates or hypothesis tests that the
              statistician is charged with answering together; not at all what the classical masters had in mind.
              Which variables matter among the thousands measured? How do you relate unrelated information?
              http://www-stat.stanford.edu/~ckirby/brad/papers/2010LSIexcerpt.pdf
                                                                                                                         3/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                       3/19
 
 9/3/13                                                                                                  Multiple testing
              Reasons for multiple testing
                                                                                                                         4/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                       4/19
 
 9/3/13                                                                                                  Multiple testing
              Why correct for multiple tests?
              http://xkcd.com/882/
                                                                                                                         5/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                       5/19
 
 9/3/13                                                                                                  Multiple testing
              Why correct for multiple tests?
              http://xkcd.com/882/
                                                                                                                         6/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                       6/19
 
 9/3/13                                                                                                    Multiple testing
              Types of errors
              Suppose you are testing a hypothesis that a parameter ! equals zero versus the alternative that it
              does not equal zero. These are the possible outcomes.
                                                                         ! = 0                          ! ¹   0            HYPOTHESES
                Claim !       = 0                                        U                              T                  m J R
                Claim !       ¹  0                                       V                              S                  R
                Claims                                                   m0                             m  J  m0           m
              Type I error or false positive (V ) Say that the parameter does not equal zero when it does
              Type II error or false negative (T ) Say that the parameter equals zero when it doesn't
                                                                                                                                      7/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                                    7/19
 
 9/3/13                                                                                                  Multiple testing
              Error rates
              False positive rate - The rate at which false results (!                                            = 0    ) are called significant: E[   V
                                                                                                                                                       m0  ]
                                                                                                                                                             *
              Family wise error rate (FWER) - The probability of at least one false positive Pr(V                                                     ¾   1)
                                                                                                                                                   V
              False discovery rate (FDR) - The rate at which claims of significance are false E[                                                    R ]
                 · The             false           positive            rate          is        closely          related         to    the     type      I      error rate
                    http://en.wikipedia.org/wiki/False_positive_rate
                                                                                                                                                                      8/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                                                                    8/19
 
 9/3/13                                                                                                  Multiple testing
              Controlling the false positive rate
              If P-values are correctly calculated calling all                                        P <       significant will control the false positive rate at
              level on average.
              Problem: Suppose that you perform 10,000 tests and !                                                 = 0    for all of them.
              Suppose that you call all P                         < 0.05      significant.
              The expected number of false positives is: 10, 000 × 0.05                                                 = 500   false positives.
              How do we avoid so many false positives?
                                                                                                                                                                9/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                                                              9/19
 
 9/3/13                                                                                                   Multiple testing
              Controlling family-wise error rate (FWER)
              The Bonferroni correction is the oldest multiple testing correction.
              Basic idea:
                 · Suppose you do m tests
                 · You want to control FWER at level                                     so Pr(V        ¾ 1) <
                 · Calculate P-values normally
                 · Set         f wer   =      /m
                 · Call all P-values less than                          f wer  significant
              Pros: Easy to calculate, conservative Cons: May be very conservative
                                                                                                                          10/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                         10/19
 
 9/3/13                                                                                                  Multiple testing
              Controlling false discovery rate (FDR)
              This is the most popular correction when performing lots of tests say in genomics, imagining,
              astronomy, or other signal-processing disciplines.
              Basic idea:
                 · Suppose you do m tests
                                                                                                  V
                 · You want to control FDR at level                                  so E[ R ]
                 · Calculate P-values normally
                 · Order the P-values from smallest to largest P(1) , . . . , P(m)
                 · Call any P(i)              ½        ×
                                                            m
                                                             i
                                                                 significant
              Pros: Still pretty easy to calculate, less conservative (maybe much less)
              Cons: Allows for more false positives, may behave strangely under dependence
                                                                                                                         11/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        11/19
 
 9/3/13                                                                                                  Multiple testing
              Example with 10 P-values
              Controlling all error rates at                          = 0.20
                                                                                                                         12/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        12/19
 
 9/3/13                                                                                                  Multiple testing
              Adjusted P-values
                 · One approach is to adjust the threshold
                 · A different approach is to calculate ""adjusted p-values""
                 · They are not p-values anymore
                 · But they can be used directly without adjusting
              Example:
                 · Suppose P-values are P1 , Q , Pm
                                                                                    f wer
                 · You could adjust them by taking Pi                                        = max m × Pi , 1            for each P-value.
                                                           f wer
                 · Then if you call all Pi                         <       significant you will control the FWER.
                                                                                                                                           13/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                                          13/19
 
 9/3/13                                                                                                  Multiple testing
              Case study I: no true positives
                 set.seed(1010093)
                 pValues <- rep(NA,1000)
                 for(i in 1:1000){
                    y <- rnorm(20)
                    x <- rnorm(20)
                    pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
                 }
                 # Controls false positive rate
                 sum(pValues < 0.05)
                 [1] 51
                                                                                                                         14/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        14/19
 
 9/3/13                                                                                                  Multiple testing
              Case study I: no true positives
                 # Controls FWER
                 sum(p.adjust(pValues,method=""bonferroni"") < 0.05)
                 [1] 0
                 # Controls FDR
                 sum(p.adjust(pValues,method=""BH"") < 0.05)
                 [1] 0
                                                                                                                         15/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        15/19
 
 9/3/13                                                                                                  Multiple testing
              Case study II: 50% true positives
                 set.seed(1010093)
                 pValues <- rep(NA,1000)
                 for(i in 1:1000){
                    x <- rnorm(20)
                    # First 500 beta=0, last 500 beta=2
                    if(i <= 500){y <- rnorm(20)}else{ y <- rnorm(20,mean=2*x)}
                    pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
                 }
                 trueStatus <- rep(c(""zero"",""not zero""),each=500)
                 table(pValues < 0.05, trueStatus)
                               trueStatus
                                 not zero zero
                    FALSE                      0 476
                    TRUE                   500 24
                                                                                                                         16/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        16/19
 
 9/3/13                                                                                                  Multiple testing
              Case study II: 50% true positives
                 # Controls FWER
                 table(p.adjust(pValues,method=""bonferroni"") < 0.05,trueStatus)
                               trueStatus
                                 not zero zero
                    FALSE                    23 500
                    TRUE                   477          0
                 # Controls FDR
                 table(p.adjust(pValues,method=""BH"") < 0.05,trueStatus)
                               trueStatus
                                 not zero zero
                    FALSE                      0 487
                    TRUE                   500 13
                                                                                                                         17/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        17/19
 
 9/3/13                                                                                                  Multiple testing
              Case study II: 50% true positives
              P-values versus adjusted P-values
                 par(mfrow=c(1,2))
                 plot(pValues,p.adjust(pValues,method=""bonferroni""),pch=19)
                 plot(pValues,p.adjust(pValues,method=""BH""),pch=19)
                                                                                                                         18/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        18/19
 
 9/3/13                                                                                                  Multiple testing
              Notes and resources
              Notes:
                 · Multiple testing is an entire subfield
                 · A basic Bonferroni/BH correction is usually enough
                 · If there is strong dependence between tests there may be problems
                          - Consider method=""BY""
              Further resources:
                 · Multiple testing procedures with applications to genomics
                 · Statistical significance for genome-wide studies
                 · Introduction to multiple testing
                                                                                                                         19/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/001multipleTesting/index.html#1                        19/19
"
"./06_StatisticalInference/03_06_resampledInference/Bootstrapping.pdf","8/27/13                                                                      Bootstrapping
                          Bootstrapping
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1               1/20
 
 8/27/13                                                                      Bootstrapping
              Table of contents
                   1. The jackknife
                   2. The bootstrap principle
                   3. The bootstrap
                                                                                           2/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    2/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · The jackknife is a tool for estimating standard errors and the bias of estimators
                 · As its name suggests, the jackknife is a small, handy tool; in contrast to the bootstrap, which is
                    then the moral equivalent of a giant workshop full of tools
                 · Both the jackknife and the bootstrap involve resampling data; that is, repeatedly creating new data
                    sets from the original data
                                                                                                                   3/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                            3/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · The jackknife deletes each observation and calculates an estimate based on the remaining        n J 1
                    of them
                 · It uses this collection of estimates to do things like estimate the bias and the standard error
                 · Note that estimating the bias and having a standard error are not needed for things like sample
                    means, which we know are unbiased estimates of population means and what their standard
                    errors are
                                                                                                                     4/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                              4/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · We'll consider the jackknife for univariate data
                 · Let X , Q , X be a collection of data used to estimate a parameter '
                              1            n
                 · Let 'ė be the estimate based on the full data set
                 · Let 'ė be the estimate of ' obtained by deleting observation i
                              i
                 · Let 'Ě       =
                                    1
                                    n
                                       I   n
                                           i=1
                                               'ė i
                                                                                           5/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    5/20
 
 8/27/13                                                                                  Bootstrapping
              Continued
                 · Then, the jackknife estimate of the bias is
                                                                                 (n J  1)
                                                                                          (
                                                                                             'Ě  J 'ė  )
                    (how far the average delete-one estimate is from the actual estimate)
                 · The jackknife estimate of the standard error is
                                                                               n J 1
                                                                                       n
                                                                                          ( 'ė i J 'Ě  )
                                                                                                         2
                                                                                                             1/2
                                                                                 n   ∑
                                                                             [                             ]
                                                                                      i=1
                    (the deviance of the delete-one estimates from the average delete-one estimate)
                                                                                                                 6/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          6/20
 
 8/27/13                                                                                  Bootstrapping
              Example
                 · Consider the data set of                            630   measurements of gray matter volume for workers from a lead
                    manufacturing plant
                 · The median gray matter volume is around 589 cubic centimeters
                 · We want to estimate the bias and standard error of the median
                                                                                                                                    7/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                             7/20
 
 8/27/13                                                                      Bootstrapping
              Example
              The gist of the code
                 n <- length(gmVol)
                 theta <- median(gmVol)
                 jk <- sapply(1 : n,
                                         function(i) median(gmVol[-i])
                                         )
                 thetaBar <- mean(jk)
                 biasEst <- (n - 1) * (thetaBar - theta)
                 seEst <- sqrt((n - 1) * mean((jk - thetaBar)^2))
                                                                                           8/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    8/20
 
 8/27/13                                                                      Bootstrapping
              Example
              Or, using the bootstrappackage
                 library(bootstrap)
                 out <- jackknife(gmVol, median)
                 out$jack.se
                 out$jack.bias
                                                                                           9/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    9/20
 
 8/27/13                                                                      Bootstrapping
              Example
                 · Both methods (of course) yield an estimated bias of 0 and a se of 9.94
                 · Odd little fact: the jackknife estimate of the bias for the median is always  0 when the number of
                    observations is even
                 · It has been shown that the jackknife is a linear approximation to the bootstrap
                 · Generally do not use the jackknife for sample quantiles like the median; as it has been shown to
                    have some poor properties
                                                                                                                10/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          10/20
 
 8/27/13                                                                                 Bootstrapping
              Pseudo observations
                 · Another interesting way to think about the jackknife uses pseudo observations
                 · Let
                                                                             Pseudo Obs = n   'ė J J (n 1)'ė i
                 · Think of these as ``whatever observation i contributes to the estimate of ' ''
                 · Note when 'ė is the sample mean, the pseudo observations are the data themselves
                 · Then the sample standard error of these observations is the previous jackknife estimated standard
                    error.
                 · The mean of these observations is a bias-corrected estimate of '
                                                                                                                11/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          11/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap
                 · The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating
                    standard errors for difficult statistics
                 · For example, how would one derive a confidence interval for the median?
                 · The bootstrap procedure follows from the so called bootstrap principle
                                                                                                                12/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          12/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap principle
                 · Suppose that I have a statistic that estimates some population parameter, but I don't know its
                    sampling distribution
                 · The bootstrap principle suggests using the distribution defined by the data to approximate its
                    sampling distribution
                                                                                                            13/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                      13/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap in practice
                 · In practice, the bootstrap principle is always carried out using simulation
                 · We will cover only a few aspects of bootstrap resampling
                 · The general procedure follows by first simulating complete data sets from the observed data with
                    replacement
                         - This is approximately drawing from the sampling distribution of that statistic, at least as far as
                             the data is able to approximate the true population distribution
                 · Calculate the statistic for each simulated data set
                 · Use the simulated statistics to either define a confidence interval or take the standard deviation to
                    calculate a standard error
                                                                                                                         14/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                   14/20
 
 8/27/13                                                                                  Bootstrapping
              Example
                 · Consider again, the data set of                           630 measurements of gray matter volume for workers from a lead
                    manufacturing plant
                 · The median gray matter volume is around 589 cubic centimeters
                 · We want a confidence interval for the median of these measurements
                                                                                                                                       15/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                                 15/20
 
 8/27/13                                                                                        Bootstrapping
                 · Bootstrap procedure for calculating confidence interval for the median from a data set of                        n
                    observations
                    i. Sample n observations with replacement from the observed data resulting in one simulated
                    complete data set
                    ii. Take the median of the simulated data set
                    iii. Repeat these two steps B times, resulting in B simulated medians
                    iv. These medians are approximately drawn from the sampling distribution of the median of                       n
                    observations; therefore we can
                         - Draw a histogram of them
                         - Calculate their standard deviation to estimate the standard error of the median
                                                  th                     th
                         - Take the 2.5                and 97.5              percentiles as a confidence interval for the median
                                                                                                                                 16/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                           16/20
 
 8/27/13                                                                              Bootstrapping
              Example code
                 B <- 1000
                 n <- length(gmVol)
                 resamples <- matrix(sample(gmVol,
                                                                    n * B,
                                                                    replace = TRUE),
                                                      B, n)
                 medians <- apply(resamples, 1, median)
                 sd(medians)
                 [1] 3.148706
                 quantile(medians, c(.025, .975))
                         2.5%          97.5%
                 582.6384 595.3553
                                                                                                   17/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                             17/20
 
 8/27/13                                                                      Bootstrapping
                                                                                           18/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                     18/20
 
 8/27/13                                                                      Bootstrapping
              Notes on the bootstrap
                 · The bootstrap is non-parametric
                 · However, the theoretical arguments proving the validity of the bootstrap rely on large samples
                 · Better percentile bootstrap confidence intervals correct for bias
                 · There are lots of variations on bootstrap procedures; the book ""An Introduction to the Bootstrap""""
                    by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information
                                                                                                                   19/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                             19/20
 
 8/27/13                                                                          Bootstrapping
                 library(boot)
                 stat <- function(x, i) {median(x[i])}
                 boot.out <- boot(data = gmVol,
                                               statistic = stat,
                                               R = 1000)
                 boot.ci(boot.out)
                 Level             Percentile                                BCa
                 95% (583.1, 595.2 ) (583.2, 595.3 )
                                                                                               20/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                         20/20
"
"./06_StatisticalInference/lectures/01_01_Introduction.pdf","Introduction to statistical inference
Statistical inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Statistical inference defined
Statistical inference is the process of drawing formal conclusions from data.
In our class, we wil define formal statistical inference as settings where one wants to infer facts about
a population using noisy statistical data where uncertainty must be accounted for.
                                                                                                      2/11
 
 Motivating example: who's going to win the
election?
In every major election, pollsters would like to know, ahead of the actual election, who's going to win.
Here, the target of estimation (the estimand) is clear, the percentage of people in a particular group
(city, state, county, country or other electoral grouping) who will vote for each candidate.
We can not poll everyone. Even if we could, some polled may change their vote by the time the
election occurs. How do we collect a reasonable subset of data and quantify the uncertainty in the
process to produce a good guess at who will win?
                                                                                                    3/11
 
 Motivating example: is hormone replacement
therapy effective?
A large clinical trial (the Women’s Health Initiative) published results in 2002 that contradicted prior
evidence on the efficacy of hormone replacement therapy for post menopausal women and suggested
a negative impact of HRT for several key health outcomes. Based on a statistically based protocol,
the study was stopped early due an excess number of negative events.
Here's there's two inferential problems.
  1. Is HRT effective?
  2. How long should we continue the trial in the presence of contrary evidence?
See WHI writing group paper JAMA 2002, Vol 288:321 - 333. for the paper and Steinkellner et al.
Menopause 2012, Vol 19:616 621 for adiscussion of the long term impacts
                                                                                                    4/11
 
 Motivating example: ECMO
In 1985 a group at a major neonatal intensive care center published the results of a trial comparing a
standard treatment and a promising new extracorporeal membrane oxygenation treatment (ECMO) for
newborn infants with severe respiratory failure. Ethical considerations lead to a statistical
randomization scheme whereby one infant received the control therapy, thereby opening the study to
sample-size based criticisms.
For a review and statistical discussion, see Royall Statistical Science 1991, Vol 6, No. 1, 52-88
                                                                                                   5/11
 
 Summary
· These examples illustrate many of the difficulties of trying to use data to create general
  conclusions about a population.
· Paramount among our concerns are:
     - Is the sample representative of the population that we'd like to draw inferences about?
     - Are there known and observed, known and unobserved or unknown and unobserved variables
       that contaminate our conclusions?
     - Is there systematic bias created by missing data or the design or conduct of the study?
     - What randomness exists in the data and how do we use or adjust for it? Here randomness
       can either be explicit via randomization or random sampling, or implicit as the aggregation of
       many complex uknown processes.
     - Are we trying to estimate an underlying mechanistic model of phenomena under study?
· Statistical inference requires navigating the set of assumptions and tools and subsequently
  thinking about how to draw conclusions from data.
                                                                                                 6/11
 
 Example goals of inference
 1. Estimate and quantify the uncertainty of an estimate of a population quantity (the proportion of
    people who will vote for a candidate).
 2. Determine whether a population quantity is a benchmark value (""is the treatment effective?"").
 3. Infer a mechanistic relationship when quantities are measured with noise (""What is the slope for
    Hooke's law?"")
 4. Determine the impact of a policy? (""If we reduce polution levels, will asthma rates decline?"")
                                                                                                   7/11
 
 Example tools of the trade
 1. Randomization: concerned with balancing unobserved variables that may confound inferences of
    interest
 2. Random sampling: concerned with obtaining data that is representative of the population of
    interest
 3. Sampling models: concerned with creating a model for the sampling process, the most common
    is so called ""iid"".
 4. Hypothesis testing: concerned with decision making in the presence of uncertainty
 5. Confidence intervals: concerned with quantifying uncertainty in estimation
 6. Probability models: a formal connection between the data and a population of interest. Often
    probability models are assumed or are approximated.
 7. Study design: the process of designing an experiment to minimize biases and variability.
 8. Nonparametric bootstrapping: the process of using the data to, with minimal probability model
    assumptions, create inferences.
 9. Permutation, randomization and exchangeability testing: the process of using data permutations
    to perform inferences.
                                                                                               8/11
 
 Different thinking about probability leads to
different styles of inference
We won't spend too much time talking about this, but there are several different styles of inference.
Two broad categories that get discussed a lot are:
  1. Frequency probability: is the long run proportion of times an event occurs in independent,
     identically distributed repetitions.
  2. Frequency inference: uses frequency interpretations of probabilities to control error rates.
     Answers questions like ""What should I decide given my data controlling the long run proportion
     of mistakes I make at a tolerable level.""
  3. Bayesian probability: is the probability calculus of beliefs, given that beliefs follow certain rules.
  4. Bayesian inference: the use of Bayesian probability representation of beliefs to perform
     inference. Answers questions like ""Given my subjective beliefs and the objective information
     from the data, what should I believe now?""
Data scientists tend to fall within shades of gray of these and various other schools of inference.
                                                                                                         9/11
 
 In this class
 · In this class, we will primarily focus on basic sampling models, basic probability models and
   frequency style analyses to create standard inferences.
 · Being data scientists, we will also consider some inferential strategies that rely heavily on the
   observed data, such as permutation testing and bootstrapping.
 · As probability modeling will be our starting point, we first build up basic probability.
                                                                                                10/11
 
 Where to learn more on the topics not covered
 1. Explicit use of random sampling in inferences: look in references on ""finite population statistics"".
    Used heavily in polling and sample surveys.
 2. Explicit use of randomization in inferences: look in references on ""causal inference"" especially in
    clinical trials.
 3. Bayesian probability and Bayesian statistics: look for basic itroductory books (there are many).
 4. Missing data: well covered in biostatistics and econometric references; look for references to
    ""multiple imputation"", a popular tool for addressing missing data.
 5. Study design: consider looking in the subject matter area that you are interested in; some
    examples with rich histories in design:
      1. The epidemiological literature is very focused on using study design to investigate public
          health.
      2. The classical development of study design in agriculture broadly covers design and design
          principles.
      3. The industrial quality control literature covers design thoroughly.
                                                                                                   11/11
"
"./06_StatisticalInference/lectures/01_02_Probability.pdf","Probability
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Notation
· The sample space, Ω, is the collection of possible outcomes of an experiment
    - Example: die roll Ω =     {1, 2, 3, 4, 5, 6}
· An event, say E , is a subset of Ω
    - Example: die roll is even E    = {2, 4, 6}
· An elementary or simple event is a particular result of an experiment
    - Example: die roll is a four, ω  = 4
· ∅ is called the null event or the empty set
                                                                               2/21
 
 Interpretation of set operations
Normal set operations have particular interpretations in this setting
  1. ω ∈ E   implies that E occurs when ω occurs
  2. ω ∉ E   implies that E does not occur when ω occurs
  3. E ⊂ F    implies that the occurrence of E implies the occurrence of F
  4. E ∩ F   implies the event that both E and F occur
  5. E ∪ F   implies the event that at least one of E or F occur
  6. E ∩ F = ∅    means that E and F are mutually exclusive, or cannot both occur
  7. E
       c
         or Eˉ is the event that E does not occur
                                                                                  3/21
 
 Probability
A probability measure, P , is a function from the collection of possible events so that the following hold
   1. For an event E   ⊂ Ω , 0 ≤ P (E) ≤ 1
   2. P (Ω) = 1
   3. If E and E are mutually exclusive events P (E
          1        2                                         1 ∪ E 2 ) = P (E 1 ) + P ( E 2 ) .
Part 3 of the definition implies finite additivity
                                                             n
                                                 n
                                           P (∪      Ai ) = ∑ P ( Ai )
                                                 i=1
                                                            i=1
where the {A   i} are mutually exclusive. (Note a more general version of additivity is used in advanced
classes.)
                                                                                                       4/21
 
 Example consequences
· P (∅) = 0
                      c
· P (E) = 1 − P ( E     )
· P (A ∪ B) = P (A) + P (B) − P (A ∩ B)
· if A ⊂ B then P (A) ≤ P (B)
                            c      c
· P (A ∪ B) = 1 − P (A        ∩ B )
            c
· P (A ∩ B ) = P (A) − P (A ∩ B)
                    n
· P (∪
       n
       i=1
           Ei ) ≤ ∑
                    i=1
                          P (E i )
· P (∪
       n
       i=1
           E i ) ≥ maxi P (E i )
                                        5/21
 
 Example
The National Sleep Foundation (www.sleepfoundation.org) reports that around 3% of the American
population has sleep apnea. They also report that around 10% of the North American and European
population has restless leg syndrome. Does this imply that 13% of people will have at least one sleep
problems of these sorts?
                                                                                                  6/21
 
 Example continued
Answer: No, the events are not mutually exclusive. To elaborate let:
                                     A 1 = {Person has sleep apnea}
                                     A 2 = {Person has RLS}
Then
                           P ( A 1 ∪ A2 ) = P ( A1 ) + P (A 2 ) − P (A 1 ∩ A 2 )
                                          = 0.13 − Probability of having both
Likely, some fraction of the population has both.
                                                                                 7/21
 
 Random variables
· A random variable is a numerical outcome of an experiment.
· The random variables that we study will come in two varieties, discrete or continuous.
· Discrete random variable are random variables that take on only a countable number of
  possibilities.
    - P (X = k)
· Continuous random variable can take any value on the real line or some subset of the real line.
    - P (X ∈ A)
                                                                                                  8/21
 
 Examples of variables that can be thought of
as random variables
· The (0 − 1) outcome of the flip of a coin
· The outcome from the roll of a die
· The BMI of a subject four years after a baseline measurement
· The hypertension status of a subject randomly drawn from a population
                                                                        9/21
 
 PMF
A probability mass function evaluated at a value corresponds to the probability that a random variable
takes that value. To be a valid pmf a function, p , must satisfy
  1. p(x) ≥ 0  for all x
  2. ∑
        x
          p(x) = 1
The sum is taken over all of the possible values for x.
                                                                                                  10/21
 
 Example
Let X be the result of a coin flip where X = 0 represents tails and X = 1 represents heads.
                                                 x        1−x
                                  p(x) = (1/2 )    (1/2 )      for  x = 0, 1
Suppose that we do not know whether or not the coin is fair; Let             θ be the probability of a head
expressed as a proportion (between 0 and 1).
                                             x         1−x
                                    p(x) = θ   (1 − θ)        for  x = 0, 1
                                                                                                        11/21
 
 PDF
A probability density function (pdf), is a function associated with a continuous random variable
Areas under pdfs correspond to probabilities for that random variable
To be a valid pdf, a function f must satisfy
  1. f(x) ≥ 0   for all x
  2. The area under f(x) is one.
                                                                                                 12/21
 
 Example
Suppose that the proportion of help calls that get addressed in a random day by a help line is given by
                                               2x    for 1 > x > 0
                                      f(x) = {
                                               0     otherwise
Is this a mathematically valid density?
                                                                                                   13/21
 
 x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = ""l"")
                                               14/21
 
 Example continued
What is the probability that 75% or fewer of calls get addressed?
                                                                  15/21
 
 1.5 * 0.75/2
## [1] 0.5625
pbeta(0.75, 2, 1)
## [1] 0.5625
                  16/21
 
 CDF and survival function
· The cumulative distribution function (CDF) of a random variable X is defined as the function
                                           F (x) = P (X ≤ x)
· This definition applies regardless of whether X is discrete or continuous.
· The survival function of a random variable X is defined as
                                           S(x) = P (X > x)
· Notice that S(x) = 1 − F (x)
· For continuous random variables, the PDF is the derivative of the CDF
                                                                                               17/21
 
 Example
What are the survival function and CDF from the density considered before?
For 1 ≥ x ≥ 0
                                          1                   1
                                                                               2
                    F (x) = P (X ≤ x) =     Base × H eight =    (x) × (2x) = x
                                          2                   2
                                                         2
                                            S(x) = 1 − x
 pbeta(c(0.4, 0.5, 0.6), 2, 1)
 ## [1] 0.16 0.25 0.36
                                                                                 18/21
 
 Quantiles
· The α th
           quantile of a distribution with distribution function F is the point x so that
                                                                                 α
                                                  F (x α ) = α
· A percentile is simply a quantile with α expressed as a percent
                         th
· The median is the 50      percentile
                                                                                          19/21
 
 Example
· We want to solve 0.5 = F (x) =   x
                                    2
· Resulting in the solution
sqrt(0.5)
## [1] 0.7071
· Therefore, about 0.7071 of calls being answered on a random day is the median.
· R can approximate quantiles for you for common distributions
qbeta(0.5, 2, 1)
## [1] 0.7071
                                                                                 20/21
 
 Summary
· You might be wondering at this point ""I've heard of a median before, it didn't require integration.
  Where's the data?""
· We're referring to are population quantities. Therefore, the median being discussed is the
  population median.
· A probability model connects the data to the population using assumptions.
· Therefore the median we're discussing is the estimand, the sample median will be the estimator
                                                                                                21/21
"
"./06_StatisticalInference/lectures/01_03_Expectations.pdf","Expected values
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Expected values
· The expected value or mean of a random variable is the center of its distribution
· For discrete random variable X with PMF p(x), it is defined as follows
                                        E[X] = ∑ xp(x).
                                                     x
  where the sum is taken over the possible values of x
· E[X]   represents the center of mass of a collection of locations and weights, {x, p(x)}
                                                                                           2/20
 
 Example
Find the center of mass of the bars
                                    3/20
 
 Using manipulate
library(manipulate)
myHist <- function(mu){
  hist(galton$child,col=""blue"",breaks=100)
  lines(c(mu, mu), c(0, 150),col=""red"",lwd=5)
  mse <- mean((galton$child - mu)^2)
  text(63, 150, paste(""mu = "", mu))
  text(63, 140, paste(""Imbalance = "", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
                                                        4/20
 
 The center of mass is the empirical mean
hist(galton$child, col = ""blue"", breaks = 100)
meanChild <- mean(galton$child)
lines(rep(meanChild, 100), seq(0, 150, length = 100), col = ""red"", lwd = 5)
                                                                            5/20
 
 Example
· Suppose a coin is flipped and X is declared 0 or 1 corresponding to a head or a tail, respectively
· What is the expected value of X ?
                                       E[X] = .5 × 0 + .5 × 1 = .5
· Note, if thought about geometrically, this answer is obvious; if two equal weights are spaced at 0
  and 1, the center of mass will be .5
                                                                                                   6/20
 
 Example
· Suppose that a die is rolled and X is the number face up
· What is the expected value of X ?
                                 1        1       1        1       1       1
                  E[X] = 1 ×       + 2 ×    + 3 ×   + 4 ×    + 5 ×   + 6 ×   = 3.5
                                 6        6       6        6       6       6
· Again, the geometric argument makes this answer obvious without calculation.
                                                                                   7/20
 
 Continuous random variables
· For a continuous random variable, X , with density, f , the expected value is defined as follows
                              E[X] = the area under the function       tf(t)
· This definition borrows from the definition of center of mass for a continuous body
                                                                                                   8/20
 
 Example
· Consider a density where f(x) = 1 for x between zero and one
· (Is this a valid density?)
· Suppose that X follows this density; what is its expected value?
                                                                   9/20
 
 Rules about expected values
· The expected value is a linear operator
· If a and b are not random and X and Y are two random variables then
     -  E[aX + b] = aE[X] + b
     -  E[X + Y ] = E[X] + E[Y ]
                                                                      10/20
 
 Example
· You flip a coin, X and simulate a uniform random number        Y , what is the expected value of their
  sum?
                               E[X + Y ] = E[X] + E[Y ] = .5 + .5 = 1
· Another example, you roll a die twice. What is the expected value of the average?
· Let X and X be the results of the two rolls
        1       2
                                         1                        1
                    E[( X 1 + X 2 )/2] =   (E[X 1 ] + E[X 2 ]) =    (3.5 + 3.5) = 3.5
                                         2                        2
                                                                                                   11/20
 
 Example
 1. Let X for i = 1, … , n be a collection of random variables, each from a distribution with mean μ
         i
 2. Calculate the expected value of the sample average of the X   i
                                             n                n
                                         1            1
                                     E[    ∑ Xi] =       E[ ∑ X i ]
                                         n            n
                                            i=1              i=1
                                                          n
                                                      1
                                                    =    ∑ E[X i ]
                                                      n
                                                         i=1
                                                          n
                                                      1
                                                    =    ∑ μ = μ.
                                                      n
                                                         i=1
                                                                                                 12/20
 
 Remark
· Therefore, the expected value of the sample mean is the population mean that it's trying to
  estimate
· When the expected value of an estimator is what its trying to estimate, we say that the estimator is
  unbiased
                                                                                                 13/20
 
 The variance
 · The variance of a random variable is a measure of spread
 · If X is a random variable with mean μ, the variance of X is defined as
                                                             2
                                       V ar(X) = E[(X − μ)     ]
the expected (squared) distance from the mean
 · Densities with a higher variance are more spread out than densities with a lower variance
                                                                                             14/20
 
 · Convenient computational form
                                                      2          2
                                        V ar(X) = E[X   ] − E[X]
· If a is constant then V ar(aX) = a 2
                                       V ar(X)
· The square root of the variance is called the standard deviation
· The standard deviation has the same units as X
                                                                   15/20
 
 Example
· What's the sample variance from the result of a toss of a die?
    -  E[X] = 3.5
           2       2     1     2    1     2   1     2   1     2   1     2   1
    -  E[X   ] = 1    ×
                         6
                           + 2    ×
                                    6
                                      + 3   ×
                                              6
                                                + 4   ×
                                                        6
                                                          + 5   ×
                                                                  6
                                                                    + 6   ×
                                                                            6
                                                                              = 15.17
                    2           2
· V ar(X) = E[X       ] − E[X ]   ≈ 2.92
                                                                                      16/20
 
 Example
· What's the sample variance from the result of the toss of a coin with probability of heads (1) of p ?
    -  E[X] = 0 × (1 − p) + 1 × p = p
           2
    -  E[X   ] = E[X] = p
                  2          2
· V ar(X) = E[X     ] − E[X ]  = p − p
                                       2
                                         = p(1 − p)
                                                                                                    17/20
 
 Interpreting variances
 · Chebyshev's inequality is useful for interpreting variances
 · This inequality states that
                                                                1
                                          P (|X − μ| ≥ kσ) ≤
                                                                 2
                                                               k
 · For example, the probability that a random variable lies beyond        k standard deviations from its
                           2
   mean is less than 1/k
                                                 2σ → 25%
                                                 3σ → 11%
                                                 4σ → 6%
 · Note this is only a bound; the actual probability might be quite a bit smaller
                                                                                                    18/20
 
 Example
· IQs are often said to be distributed with a mean of 100 and a sd of 15
· What is the probability of a randomly drawn person having an IQ higher than 160 or below 40 ?
· Thus we want to know the probability of a person being more than     4 standard deviations from the
  mean
· Thus Chebyshev's inequality suggests that this will be no larger than 6\%
· IQs distributions are often cited as being bell shaped, in which case this bound is very
  conservative
· The probability of a random draw from a bell curve being   4  standard deviations from the mean is
                     −5
  on the order of 10    (one thousandth of one percent)
                                                                                                 19/20
 
 Example
· A former buzz phrase in industrial quality control is Motorola's ""Six Sigma"" whereby businesses
  are suggested to control extreme events or rare defective parts
                                                                                             2
· Chebyshev's inequality states that the probability of a ""Six Sigma"" event is less than 1/6   ≈ 3%
                                                                                            −9
· If a bell curve is assumed, the probability of a ""six sigma"" event is on the order of  10    (one ten
  millionth of a percent)
                                                                                                   20/20
"
"./06_StatisticalInference/lectures/01_04_Independence.pdf","Independence
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Independent events
 · Two events A and B are independent if
                                      P (A ∩ B) = P (A)P (B)
 · Two random variables, X and Y are independent if for any two sets A and B
                           P ([X ∈ A] ∩ [Y ∈ B]) = P (X ∈ A)P (Y ∈ B)
 · If A is independent of B then
           c
      - A    is independent of B
                               c
      - A   is independent of B
           c                     c
      - A    is independent of B
                                                                             2/18
 
 Example
· What is the probability of getting two consecutive heads?
· A = {Head on flip 1}    ~ P (A) = .5
· B = {Head on flip 2}    ~ P (B) = .5
· A ∩ B = {Head on flips 1 and 2}
· P (A ∩ B) = P (A)P (B) = .5 × .5 = .25
                                                            3/18
 
 Example
· Volume 309 of Science reports on a physician who was on trial for expert testimony in a criminal
  trial
· Based on an estimated prevalence of sudden infant death syndrome of 1 out of 8, 543 , Dr Meadow
                                                                                             2
                                                                                        1
  testified that that the probability of a mother having two children with SIDS was ( 8,543
                                                                                            )
· The mother on trial was convicted of murder
                                                                                               4/18
 
 Example: continued
· For the purposes of this class, the principal mistake was to assume that the probabilities of having
  SIDs within a family are independent
· That is, P (A 1 ∩ A2 ) is not necessarily equal to P (A1 )P (A 2 )
· Biological processes that have a believed genetic or familiar environmental component, of course,
  tend to be dependent within families
· (There are many other statistical points of discussion for this case.)
                                                                                                   5/18
 
 Useful fact
We will use the following fact extensively in this class:
If a collection of random variables   X1 , X2 , … , Xn     are independent, then their joint distribution is the
product of their individual densities or mass functions
That is, if f is the density for random variable X we have that
             i                                       i
                                                               n
                                         f(x 1 , … , x n ) = ∏ fi (x i )
                                                              i=1
                                                                                                             6/18
 
 IID random variables
 · Random variables are said to be iid if they are independent and identically distributed
 · iid random variables are the default model for random samples
 · Many of the important theories of statistics are founded on assuming that variables are iid
                                                                                               7/18
 
 Example
· Suppose that we flip a biased coin with success probability            p n   times, what is the join density of
  the collection of outcomes?
                                                                   1−xi
· These random variables are iid with densities p      xi
                                                          (1 − p )
· Therefore
                                              n
                                                 xi          1−x i      ∑ xi          n−∑ x i
                         f( x1 , … , x n ) = ∏ p    (1 − p )       = p       (1 − p )
                                             i=1
                                                                                                              8/18
 
 Correlation
· The covariance between two random variables X and Y is defined as
                     Cov(X, Y ) = E[(X − μ      )(Y − μ )] = E[XY ] − E[X]E[Y ]
                                              x        y
· The following are useful facts about covariance
   1. Cov(X, Y ) = Cov(Y , X)
   2. Cov(X, Y )  can be negative or positive
                        −− −−− −−− −−−−
   3. |Cov(X, Y )| ≤ √V ar(X)V ar(y)
                                                                                9/18
 
 Correlation
· The correlation between X and Y is
                                                          −−−−−−−  −−− −−
                               Cor(X, Y ) = Cov(X, Y )/√ V ar(X)V ar(y)
 1. −1 ≤ Cor(X, Y ) ≤ 1
 2. Cor(X, Y ) = ±1    if and only if X = a + bY for some constants a and b
 3. Cor(X, Y )  is unitless
 4. X and Y are uncorrelated if Cor(X, Y ) = 0
 5. X and Y are more positively correlated, the closer Cor(X, Y ) is to 1
 6. X and Y are more negatively correlated, the closer Cor(X, Y ) is to −1
                                                                            10/18
 
 Some useful results
             n
· Let {X i } i=1 be a collection of random variables
    - When the {X     i} are uncorrelated
                                             n               n
                                                                2
                                      V ar(∑ ai X i + b) = ∑ a V ar( X i )
                                                                i
                                            i=1             i=1
· A commonly used subcase from these properties is that if a collection of random variables     {X i }
  are uncorrelated, then the variance of the sum is the sum of the variances
                                               n         n
                                        V ar( ∑ X i ) = ∑ V ar( X i )
                                              i=1       i=1
· Therefore, it is sums of variances that tend to be useful, not sums of standard deviations; that is,
  the standard deviation of the sum of bunch of independent random variables is the square root of
  the sum of the variances, not the sum of the standard deviations
                                                                                                 11/18
 
 The sample mean
Suppose X are iid with variance σ
         i
                                  2
                                                          n
                                                      1
                                         ˉ
                                    V ar(X ) = V ar(    ∑ Xi)
                                                      n
                                                        i=1
                                                            n
                                                1
                                             =     V ar(∑ X i )
                                                 2
                                               n
                                                          i=1
                                                    n
                                                1
                                             =     ∑ V ar( X i )
                                                 2
                                               n
                                                   i=1
                                                1
                                                         2
                                             =     × nσ
                                                 2
                                               n
                                                 2
                                               σ
                                             =
                                                n
                                                                 12/18
 
 Some comments
                                                                       2
                                                               ˉ     σ
· When X are independent with a common variance V ar( X
           i                                                     ) =
                                                                     n
· σ/ √n  is called the standard error of the sample mean
· The standard error of the sample mean is the standard deviation of the distribution of the sample
  mean
· σ is the standard deviation of the distribution of a single observation
· Easy way to remember, the sample mean has to be less variable than a single observation,
  therefore its standard deviation is divided by a √n
                                                                                               13/18
 
 The sample variance
· The sample variance is defined as
                                                 n
                                                            ˉ 2
                                              ∑      (X i − X )
                                          2      i=1
                                        S   =
                                                     n − 1
· The sample variance is an estimator of σ  2
· The numerator has a version that's quicker for calculation
                                     n                  n
                                                                  2
                                              ˉ 2           2   ˉ
                                    ∑ (X i  − X ) = ∑ X i − nX
                                    i=1                i=1
· The sample variance is (nearly) the mean of the squared deviations from the mean
                                                                                   14/18
 
 The sample variance is unbiased
          n                 n
                     2                             2
                  ˉ                 2            ˉ
       E[∑ (X i − X ) ] = ∑ E[X ] − nE[ X ]
                                    i
         i=1               i=1
                            n
                                               2               ˉ 2
                        = ∑ {V ar( X i ) + μ } − n{V ar(X ) + μ }
                           i=1
                            n
                                  2     2            2       2
                        = ∑ {σ      + μ } − n{ σ       /n + μ }
                           i=1
                              2       2     2        2
                        = nσ    + nμ    − σ   − nμ
                                     2
                        = (n − 1)σ
                                                                   15/18
 
 Hoping to avoid some confusion
· Suppose X are iid with mean μ and variance σ
               i
                                                     2
    2
· S   estimates σ  2
                         2
· The calculation of S involves dividing by n − 1
· S/ √n   estimates σ/√n the standard error of the mean
· S/ √n   is called the sample standard error (of the mean)
                                                            16/18
 
 Example
data(father.son)
x <- father.son$sheight
n <- length(x)
                        17/18
 
 round(c(sum((x - mean(x))^2)/(n - 1), var(x), var(x)/n, sd(x), sd(x)/sqrt(n)),
    2)
## [1] 7.92 7.92 0.01 2.81 0.09
                                                                               18/18
"
"./06_StatisticalInference/lectures/01_05_ConditionalProbability.pdf","Conditional Probability
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Conditional probability, motivation
· The probability of getting a one when rolling a (standard) die is usually assumed to be one sixth
· Suppose you were given the extra information that the die roll was an odd number (hence 1, 3 or
  5)
· conditional on this new information, the probability of a one is now one third
                                                                                                    2/15
 
 Conditional probability, definition
· Let B be an event so that P (B) > 0
· Then the conditional probability of an event A given that B has occurred is
                                                       P (A ∩ B)
                                         P (A | B) =
                                                         P (B)
· Notice that if A and B are independent, then
                                                 P (A)P (B)
                                    P (A | B) =               = P (A)
                                                    P (B)
                                                                              3/15
 
 Example
· Consider our die roll example
· B = {1, 3, 5}
· A = {1}
                              P (one given that roll is odd) = P (A | B)
                                                               P (A ∩ B)
                                                             =
                                                                  P (B)
                                                               P (A)
                                                             =
                                                               P (B)
                                                               1/6      1
                                                             =       =
                                                               3/6      3
                                                                          4/15
 
 Bayes' rule
                                 P (A | B)P (B)
            P (B | A) =                                   .
                                                 c     c
                        P (A | B)P (B) + P (A | B )P (B )
                                                            5/15
 
 Diagnostic tests
· Let + and − be the events that the result of a diagnostic test is positive or negative respectively
                 c
· Let D  and   D    be the event that the subject of the test has or does not have the disease
  respectively
· The sensitivity is the probability that the test is positive given that the subject actually has the
  disease, P (+ | D)
· The specificity is the probability that the test is negative given that the subject does not have the
                     c
  disease, P (− | D    )
                                                                                                      6/15
 
 More definitions
· The positive predictive value is the probability that the subject has the disease given that the test is
  positive, P (D | +)
· The negative predictive value is the probability that the subject does not have the disease given
                                 c
  that the test is negative, P (D  | −)
· The prevalence of the disease is the marginal probability of disease, P (D)
                                                                                                      7/15
 
 More definitions
                                                                                           c
· The diagnostic likelihood ratio of a positive test, labeled DLR + , is P (+ | D)/P (+ | D ) , which is
  the
                                      sensitivity/(1 − specificity)
                                                                                           c
· The diagnostic likelihood ratio of a negative test, labeled DLR , is
                                                                  −      P (− | D)/P (− | D ) , which is
  the
                                      (1 − sensitivity)/specificity
                                                                                                    8/15
 
 Example
· A study comparing the efficacy of HIV tests, reports on an experiment which concluded that HIV
  antibody tests have a sensitivity of 99.7% and a specificity of 98.5%
· Suppose that a subject, from a population with a .1% prevalence of HIV, receives a positive test
  result. What is the probability that this subject has HIV?
· Mathematically, we want P (D | +) given the sensitivity,         P (+ | D) = .997 , the specificity,
           c
  P (− | D ) = .985 , and the prevalence P (D) = .001
                                                                                                   9/15
 
 Using Bayes' formula
                                            P (+ | D)P (D)
                   P (D | +) =
                                                             c      c
                                  P (+ | D)P (D) + P (+ | D )P (D )
                                                   P (+ | D)P (D)
                              =
                                                                  c
                                  P (+ | D)P (D) + {1 − P (− | D )}{1 − P (D)}
                                         .997 × .001
                              =
                                  .997 × .001 + .015 × .999
                              = .062
· In this population a positive test result only suggests a 6% probability that the subject has the
  disease
· (The positive predictive value is 6% for this test)
                                                                                               10/15
 
 More on this example
· The low positive predictive value is due to low prevalence of disease and the somewhat modest
  specificity
· Suppose it was known that the subject was an intravenous drug user and routinely had intercourse
  with an HIV infected partner
· Notice that the evidence implied by a positive test result does not change because of the
  prevalence of disease in the subject's population, only our interpretation of that evidence changes
                                                                                                   11/15
 
 Likelihood ratios
· Using Bayes rule, we have
                                                 P (+ | D)P (D)
                          P (D | +) =
                                                                 c    c
                                       P (+ | D)P (D) + P (+ | D )P (D )
  and
                                                         c    c
                                                P (+ | D )P (D )
                              c
                         P (D   | +) =                                   .
                                                                 c    c
                                       P (+ | D)P (D) + P (+ | D )P (D )
                                                                           12/15
 
 Likelihood ratios
· Therefore
                                     P (D | +)    P (+ | D)    P (D)
                                                =            ×
                                         c                c        c
                                    P (D   | +)   P (+ | D )   P (D )
  ie
                          post-test odds of D = DLR + × pre-test odds of D
· Similarly, DLR relates the decrease in the odds of the disease after a negative test result to the
                 −
  odds of disease prior to the test.
                                                                                                13/15
 
 HIV example revisited
· Suppose a subject has a positive HIV test
· DLR + = .997/(1 − .985) ≈ 66
· The result of the positive test is that the odds of disease is now 66 times the pretest odds
· Or, equivalently, the hypothesis of disease is 66 times more supported by the data than the
  hypothesis of no disease
                                                                                               14/15
 
 HIV example revisited
· Suppose that a subject has a negative test result
· DLR − = (1 − .997)/.985 ≈ .003
· Therefore, the post-test odds of disease is now .3% of the pretest odds given the negative test.
· Or, the hypothesis of disease is supported .003 times that of the hypothesis of absence of disease
  given the negative test result
                                                                                                  15/15
"
"./06_StatisticalInference/lectures/02_01_CommonDistributions.pdf","Some Common Distributions
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 The Bernoulli distribution
· The Bernoulli distribution arises as the result of a binary outcome
· Bernoulli random variables take (only) the values 1 and 0 with probabilities of (say) p and 1 −p
  respectively
· The PMF for a Bernoulli random variable X is
                                                         x        1−x
                                     P (X = x) = p (1 − p )
· The mean of a Bernoulli random variable is p and the variance is p(1 − p)
· If we let X be a Bernoulli random variable, it is typical to call X = 1 as a ""success"" and X = 0
  as a ""failure""
                                                                                                2/25
 
 iid Bernoulli trials
 · If several iid Bernoulli observations, say x 1, … , xn  , are observed the likelihood is
                                    n
                                        xi         1−x i     ∑ xi         n−∑ x i
                                   ∏p      (1 − p)       = p      (1 − p)
                                   i=1
 · Notice that the likelihood depends only on the sum of the x       i
 · Because n is fixed and assumed known, this implies that the sample proportion            ∑ x i /n
                                                                                              i
                                                                                                     contains
   all of the relevant information about p
 · We can maximize the Bernoulli likelihood over          p   to obtain that  p̂ = ∑ x i /n
                                                                                     i
                                                                                            is the maximum
   likelihood estimator for p
                                                                                                          3/25
 
 Plotting all possible likelihoods for a small n
n <- 5
pvals <- seq(0, 1, length = 1000)
plot(c(0, 1), c(0, 1.2), type = ""n"", frame = FALSE, xlab = ""p"", ylab = ""likelihood"")
text((0 : n) /n, 1.1, as.character(0 : n))
sapply(0 : n, function(x) {
  phat <- x / n
  if (x == 0) lines(pvals, ( (1 - pvals) / (1 - phat) )^(n-x), lwd = 3)
  else if (x == n) lines(pvals, (pvals / phat) ^ x, lwd = 3)
  else lines(pvals, (pvals / phat ) ^ x * ( (1 - pvals) / (1 - phat) ) ^ (n-x), lwd = 3)
  }
)
title(paste(""Likelihoods for n = "", n))
                                                                                         4/25
 
 5/25 
 Binomial trials
· The binomial random variables are obtained as the sum of iid Bernoulli trials
                                                                    n
· In specific, let X 1, … , Xn be iid Bernoulli (p) ; then X = ∑    i=1
                                                                        Xi  is a binomial random variable
· The binomial mass function is
                                                         n    x         n−x
                                      P (X = x) = (        )p   (1 − p)
                                                         x
  for x = 0, … , n
                                                                                                          6/25
 
 Choose
· Recall that the notation
                                         n          n!
                                       (   ) =
                                         x      x!(n − x)!
  (read ""n choose x"") counts the number of ways of selecting x items out of n without replacement
  disregarding the order of the items
                                        n       n
                                      (   ) = (   ) = 1
                                        0       n
                                                                                              7/25
 
 Example justification of the binomial
likelihood
 · Consider the probability of getting 6 heads out of  10 coin flips from a coin with success probability
   p
 · The probability of getting 6 heads and 4 tails in any specific order is
                                                   6       4
                                                 p (1 − p)
 · There are
                                                      10
                                                    (    )
                                                      6
   possible orders of 6 heads and 4 tails
                                                                                                     8/25
 
 Example
· Suppose a friend has 8 children (oh my!), 7 of which are girls and none are twins
· If each gender has an independent 50 % probability for each birth, what's the probability of getting
  7  or more girls out of 8 births?
                                  8     7          1     8     8           0
                                (   ).5   (1 − .5)   + (   ).5   (1 − .5 )   ≈ 0.04
                                  7                      8
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8
[1] 0.03516
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
[1] 0.03516
                                                                                                    9/25
 
 plot(pvals, dbinom(7, 8, pvals) / dbinom(7, 8, 7/8) ,
     lwd = 3, frame = FALSE, type = ""l"", xlab = ""p"", ylab = ""likelihood"")
                                                                          10/25
 
 The normal distribution
· A random variable is said to follow a normal or Gaussian distribution with mean μ and variance σ
                                                                                                   2
  if the associated density is
                                                                 2     2
                                              2   −1/2   −(x−μ )   /2σ
                                        (2π σ   )      e
  If X a RV with this density then E[X] = μ and V ar(X) = σ            2
· We write X ∼ N(μ, σ   2
                          )
· When μ = 0 and σ = 1 the resulting distribution is called the standard normal distribution
· The standard normal density function is labeled ϕ
· Standard normal RVs are often labeled Z
                                                                                               11/25
 
 zvals <- seq(-3, 3, length = 1000)
plot(zvals, dnorm(zvals),
     type = ""l"", lwd = 3, frame = FALSE, xlab = ""z"", ylab = ""Density"")
sapply(-3 : 3, function(k) abline(v = k))
[[1]]
NULL
[[2]]
                                                                       12/25
NULL
 
 Facts about the normal density
                             X−μ
· If X ∼ N(μ, σ  2
                   ) the Z =
                              σ
                                 is standard normal
· If Z is standard normal
                                                            2
                                        X = μ + σZ ∼ N(μ, σ   )
· The non-standard normal density is
                                           ϕ{(x − μ)/σ}/σ
                                                                13/25
 
 More facts about the normal density
 1. Approximately   68%  , 95%   and 99%   of the normal density lies within            1 ,  2  and  3  standard
    deviations from the mean, respectively
                                                    th     th       th          st
 2. −1.28  , −1.645 , −1.96  and  −2.33  are the 10    , 5    , 2.5     and   1    percentiles of the standard
    normal distribution respectively
                                                              th      th        th         th
 3. By symmetry, 1.28 , 1.645 , 1.96 and 2.33 are the     90     , 95    , 97.5    and 99      percentiles of the
    standard normal distribution respectively
                                                                                                            14/25
 
 Question
                 th
· What is the 95    percentile of a N (μ, σ 2
                                              ) distribution?
     - Quick answer in R qnorm(.95, mean = mu, sd = sd)
· We want the point x so that P (X ≤ x
                       0                    0)  = .95
                                                       X − μ      x0 − μ
                                  P (X ≤ x 0 ) = P (           ≤         )
                                                          σ         σ
                                                             x0 − μ
                                                = P (Z ≤            ) = .95
                                                               σ
· Therefore
                                                x0 − μ
                                                         = 1.645
                                                   σ
  or x 0 = μ + σ1.645
· In general x 0 = μ + σz 0 where z is the appropriate standard normal quantile
                                     0
                                                                                15/25
 
 Question
· What is the probability that a N(μ, σ 2
                                          ) RV is 2 standard deviations above the mean?
· We want to know
                                                      X − μ    μ + 2σ − μ
                             P (X > μ + 2σ) = P (            >             )
                                                        σ            σ
                                               = P (Z ≥ 2)
                                               ≈ 2.5%
                                                                                        16/25
 
 Other properties
· The normal distribution is symmetric and peaked about its mean (therefore the mean, median and
  mode are all equal)
· A constant times a normally distributed random variable is also normally distributed (what is the
  mean and variance?)
· Sums of normally distributed random variables are again normally distributed even if the variables
  are dependent (what is the mean and variance?)
· Sample means of normally distributed random variables are again normally distributed (with what
  mean and variance?)
· The square of a standard normal random variable follows what is called chi-squared distribution
· The exponent of a normally distributed random variables follows what is called the log-normal
  distribution
· As we will see later, many random variables, properly normalized, limit to a normal distribution
                                                                                                   17/25
 
 Final thoughts on normal likelihoods
                    ˉ
· The MLE for μ is X  .
· The MLE for σ is
                 2
                                              n
                                                         ˉ 2
                                           ∑      (X i − X )
                                              i=1
                                                    n
  (Which is the biased version of the sample variance.)
· The MLE of σ is simply the square root of this estimate
                                                             18/25
 
 The Poisson distribution
· Used to model counts
· The Poisson mass function is
                                                          x   −λ
                                                        λ   e
                                         P (X = x; λ) =
                                                           x!
  for x = 0, 1, …
· The mean of this distribution is λ
· The variance of this distribution is λ
· Notice that x ranges from 0 to ∞
                                                                 19/25
 
 Some uses for the Poisson distribution
· Modeling event/time data
· Modeling radioactive decay
· Modeling survival data
· Modeling unbounded count data
· Modeling contingency tables
· Approximating binomials when n is large and p is small
                                                         20/25
 
 Poisson derivation
· λ is the mean number of events per unit time
· Let h be very small
· Suppose we assume that
    - Prob. of an event in an interval of length   h is λh while the prob. of more than one event is
       negligible
    - Whether or not an event occurs in one small interval does not impact whether or not an event
       occurs in another small interval then, the number of events per unit time is Poisson with mean
       λ
                                                                                                  21/25
 
 Rates and Poisson random variables
· Poisson random variables are used to model rates
· X ∼ P oisson(λt)     where
    - λ = E[X/t]     is the expected count per unit of time
    - t is the total monitoring time
                                                            22/25
 
 Poisson approximation to the binomial
· When n is large and   p  is small the Poisson distribution is an accurate approximation to the
  binomial distribution
· Notation
    - λ = np
    - X ∼ Binomial(n, p)  , λ = np and
    - n  gets large
    - p gets small
    - λ  stays constant
                                                                                            23/25
 
 Example
The number of people that show up at a bus stop is Poisson with a mean of 2.5 per hour.
If watching the bus stop for 4 hours, what is the probability that 3 or fewer people show up for the
whole time?
  ppois(3, lambda = 2.5 * 4)
  [1] 0.01034
                                                                                                24/25
 
 Example, Poisson approximation to the
binomial
We flip a coin with success probablity 0.01 five hundred times.
What's the probability of 2 or fewer successes?
 pbinom(2, size = 500, prob = .01)
 [1] 0.1234
 ppois(2, lambda=500 * .01)
 [1] 0.1247
                                                                25/25
"
"./06_StatisticalInference/lectures/02_02_Asymptopia.pdf","A trip to Asymptopia
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Asymptotics
· Asymptotics is the term for the behavior of statistics as the sample size (or some other relevant
  quantity) limits to infinity (or some other relevant number)
· (Asymptopia is my name for the land of asymptotics, where everything works out well and there's
  no messes. The land of infinite data is nice that way.)
· Asymptotics are incredibly useful for simple statistical inference and approximations
· (Not covered in this class) Asymptotics often lead to nice understanding of procedures
· Asymptotics generally give no assurances about finite sample performance
     - The kinds of asymptotics that do are orders of magnitude more difficult to work with
· Asymptotics form the basis for frequency interpretation of probabilities (the long run proportion of
  times an event occurs)
· To understand asymptotics, we need a very basic understanding of limits.
                                                                                                  2/20
 
 Numerical limits
· Imagine a sequence
    - a 1 = .9 ,
    - a 2 = .99  ,
    - a 3 = .999   , ...
· Clearly this sequence converges to 1
· Definition of a limit: For any fixed distance we can find a point in the sequence so that the
  sequence is closer to the limit than that distance from that point on
                                                                                            3/20
 
 Limits of random variables
· The problem is harder for random variables
            ˉ
· Consider X  n the sample average of the first n of a collection of iid observations
                  ˉ
    - Example    Xn   could be the average of the result of    n coin flips (i.e. the sample proportion of
      heads)
                 ˉ                                                                                      ˉ
· We say that X    n converges in probability to a limit if for any fixed distance the probability of  Xn
  being closer (further away) than that distance from the limit converges to one (zero)
                                                                                                       4/20
 
 The Law of Large Numbers
· Establishing that a random sequence converges to a limit is hard
· Fortunately, we have a theorem that does all the work for us, called the Law of Large Numbers
· The law of large numbers states that if X , … X
                                                1      n are iid from a population with mean   μ  and
                     ˉ
  variance σ then X
             2
                       n converges in probability to μ
· (There are many variations on the LLN; we are using a particularly lazy version, my favorite kind of
  version)
                                                                                                  5/20
 
 Law of large numbers in action
n <- 10000; means <- cumsum(rnorm(n)) / (1 : n)
plot(1 : n, means, type = ""l"", lwd = 2,
     frame = FALSE, ylab = ""cumulative means"", xlab = ""sample size"")
abline(h = 0)
                                                                     6/20
 
 Discussion
· An estimator is consistent if it converges to what you want to estimate
    - Consistency is neither necessary nor sufficient for one estimator to be better than another
    - Typically, good estimators are consistent; it's not too much to ask that if we go to the trouble of
      collecting an infinite amount of data that we get the right answer
· The LLN basically states that the sample mean is consistent
· The sample variance and the sample standard deviation are consistent as well
· Recall also that the sample mean and the sample variance are unbiased as well
· (The sample standard deviation is biased, by the way)
                                                                                                     7/20
 
 The Central Limit Theorem
· The Central Limit Theorem (CLT) is one of the most important theorems in statistics
· For our purposes, the CLT states that the distribution of averages of iid variables, properly
  normalized, becomes that of a standard normal as the sample size increases
· The CLT applies in an endless variety of settings
· Let X 1, … , Xn   be a collection of iid random variables with mean μ and variance σ 2
      ˉ
· Let X n  be their sample average
         ˉ
        X n −μ
· Then         has a distribution like that of a standard normal for large n.
        σ/√n
· Remember the form
                                  ˉ
                                 Xn − μ      Estimate − Mean of estimate
                                           =                                  .
                                  σ/ √n           Std. Err. of estimate
· Usually, replacing the standard error by its estimated value doesn't change the CLT
                                                                                           8/20
 
 Example
· Simulate a standard normal random variable by rolling n (six sided)
· Let X be the outcome for die i
       i
· Then note that μ = E[X  i] = 3.5
· V ar(X i ) = 2.92
        −− − −−−
· SE √ 2.92/n    = 1.71/ √ n
· Standardized mean
                                             ˉ
                                            X n − 3.5
                                            1.71/ √n
                                                                      9/20
 
 Simulation of mean of n dice
                             10/20
 
 Coin CLT
                                      th
· Let X be the 0 or 1 result of the i
       i                                 flip of a possibly unfair coin
    - The sample proportion, say p̂ , is the average of the coin flips
    - E[X i ] = p and V ar(X i)  = p(1 − p)
                                          − −− −− −−−−
    - Standard error of the mean is √ p(1 − p)/n
    - Then
                                                       p̂ − p
                                                     −− − −−−−− −
                                                   √ p(1 − p)/n
      will be approximately normally distributed
                                                                        11/20
 
 12/20 
 CLT in practice
· In practice the CLT is mostly useful as an approximation
                                            ˉ
                                           Xn − μ
                                        P(           ≤ z) ≈ Φ(z).
                                            σ/ √n
                                                   th
· Recall 1.96 is a good approximation to the .975     quantile of the standard normal
· Consider
                                               ˉ
                                              Xn − μ
                          .95 ≈ P (−1.96 ≤             ≤ 1.96)
                                               σ/ √n
                                    ˉ                         ˉ
                              = P ( X n + 1.96σ/√ n ≥ μ ≥ X n − 1.96σ/√ n),
                                                                                      13/20
 
 Confidence intervals
· Therefore, according to the CLT, the probability that the random interval
                                           ˉ
                                          X n ± z 1−α/2 σ/ √n
  contains μ is approximately 100(1 − α)%, where      z 1−α/2 is the 1 − α/2 quantile of the standard
  normal distribution
· This is called a 100(1 − α) % confidence interval for μ
· We can replace the unknown σ with s
                                                                                                 14/20
 
 Give a confidence interval for the average
height of sons
in Galton's data
 library(UsingR);data(father.son); x <- father.son$sheight
 (mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12
 [1] 5.710 5.738
                                                                   15/20
 
 Sample proportions
· In the event that each X is 0 or 1 with common success probability p then σ
                            i
                                                                                  2
                                                                                    = p(1 − p)
· The interval takes the form
                                                        −−−−−−−
                                                         p(1 − p)
                                          p̂ ± z 1−α/2 √
                                                             n
· Replacing p by  p̂ in the standard error results in what is called a Wald confidence interval for p
· Also note that p(1 − p) ≤ 1/4 for 0 ≤ p ≤ 1
· Let α = .05 so that z  1−α/2
                               = 1.96 ≈ 2  then
                                          −−−−−−−
                                                           − −−
                                          p(1 − p)           1       1
                                      2√             ≤ 2√       =
                                              n             4n     √n
                   1
· Therefore p̂ ±
                  √n
                      is a quick CI estimate for p
                                                                                                      16/20
 
 Example
· Your campaign advisor told you that in a random sample of 100 likely voters, 56 intent to vote for
  you.
    - Can you relax? Do you have this race in the bag?
    - Without access to a computer or calculator, how precise is this estimate?
· 1/sqrt(100)=.1 so a back of the envelope calculation gives an approximate 95% interval of
  (0.46, 0.66)
    - Not enough for you to relax, better go do more campaigning!
· Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.
round(1 / sqrt(10 ^ (1 : 6)), 3)
[1] 0.316 0.100 0.032 0.010 0.003 0.001
                                                                                               17/20
 
 Poisson interval
· A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate
  per day?
· X ∼ P oisson(λt)    .
             ˆ
· Estimate λ   = X/t
       ˆ
· V ar(λ ) = λ/t
                                      ˆ
                                     λ − λ     X − tλ
                                        −−− =    − −   → N (0, 1)
                                        ˆ       √X
                                     √λ   /t
· This isn't the best interval.
    - There are better asymptotic intervals.
    - You can get an exact CI in this case.
                                                                                                 18/20
 
 R code
 x <- 5; t <- 94.32; lambda <- x / t
 round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
 [1] 0.007 0.099
 poisson.test(x, T = 94.32)$conf
 [1] 0.01721 0.12371
 attr(,""conf.level"")
 [1] 0.95
                                                              19/20
 
 In the regression class
 exp(confint(glm(x ~ 1 + offset(log(t)), family = poisson(link = log))))
   2.5 % 97.5 %
 0.01901 0.11393
                                                                         20/20
"
"./06_StatisticalInference/lectures/02_03_tCIs.pdf","T Confidence Intervals
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Confidence intervals
· In the previous, we discussed creating a confidence interval using the CLT
· In this lecture, we discuss some methods for small samples, notably Gosset's t distribution
· To discuss the t distribution we must discuss the Chi-squared distribution
· Throughout we use the following general procedure for creating CIs
  a. Create a Pivot or statistic that does not depend on the parameter of interest
  b. Solve the probability that the pivot lies between bounds for the parameter
                                                                                              2/13
 
 The Chi-squared distribution
                  2
· Suppose that S is the sample variance from a collection of iid N (μ, σ  2
                                                                            ) data; then
                                                     2
                                           (n − 1)S
                                                           2
                                                       ∼ χ
                                                 2         n−1
                                               σ
  which reads: follows a Chi-squared distribution with n − 1 degrees of freedom
· The Chi-squared distribution is skewed and has support on 0 to ∞
· The mean of the Chi-squared is its degrees of freedom
· The variance of the Chi-squared distribution is twice the degrees of freedom
                                                                                         3/13
 
 Confidence interval for the variance
Note that if χ 2
               n−1,α
                     is the α quantile of the Chi-squared distribution then
                                                                    2
                                                         (n − 1)S
                                            2                               2
                             1 − α = P (χ            ≤                 ≤ χ            )
                                            n−1,α/2              2          n−1,1−α/2
                                                              σ
                                                      2                         2
                                        ⎛ (n − 1)S                  (n − 1)S      ⎞
                                                              2
                                   = P                   ≤ σ     ≤
                                             2                          2
                                        ⎝χ                            χ           ⎠
                                             n−1,1−α/2                  n−1,α/2
So that
                                                       2               2
                                          ⎡ (n − 1)S        (n − 1)S      ⎤
                                                          ,
                                               2               2
                                          ⎣χ                 χ            ⎦
                                               n−1,1−α/2       n−1,α/2
is a 100(1 − α)% confidence interval for σ       2
                                                                                        4/13
 
 Notes about this interval
· This interval relies heavily on the assumed normality
· Square-rooting the endpoints yields a CI for σ
                                                        5/13
 
 Example
Confidence interval for the standard deviation of sons' heights from
Galton's data
 library(UsingR)
 data(father.son)
 x <- father.son$sheight
 s <- sd(x)
 n <- length(x)
 round(sqrt((n - 1) * s^2/qchisq(c(0.975, 0.025), n - 1)), 3)
 ## [1] 2.701 2.939
                                                                     6/13
 
 Gosset's t distribution
· Invented by William Gosset (under the pseudonym ""Student"") in 1908
· Has thicker tails than the normal
· Is indexed by a degrees of freedom; gets more like a standard normal as df gets larger
· Is obtained as
                                                  Z
                                                   −
                                                   −−
                                                     2
                                                   χ
                                               √
                                                   df
  where Z and χ are independent standard normals and Chi-squared distributions respectively
                  2
                                                                                            7/13
 
 Result
                                                                                            − −  − −−−
·
                                                                                                     2
                                                                ˉ
                                                               X −μ                          (n−1)S
  Suppose that (X  1, … , Xn )  are iid N (μ, σ 2
                                                  ) , then: a. σ/√ n
                                                                     is standard normal b. √ σ
                                                                                               2
                                                                                                 (n−1)
                                                                                                       = S/σ
  is the square root of a Chi-squared divided by its df
· Therefore
                                                  ˉ
                                                 X −μ
                                                            ˉ
                                                 σ/√ n     X − μ
                                                        =
                                                S/σ        S/ √n
  follows Gosset's t distribution with n − 1 degrees of freedom
                                                                                                          8/13
 
 Confidence intervals for the mean
· Notice that the t statistic is a pivot, therefore we use it to create a confidence interval for μ
· Let tdf ,α be the α th
                         quantile of the t distribution with df degrees of freedom
                             1 − α
                                                    ˉ
                                                   X − μ
                           = P (−t n−1,1−α/2 ≤             ≤ tn−1,1−α/2 )
                                                   S/ √n
                                  ˉ                              ˉ
                           = P (X − tn−1,1−α/2 S/√n ≤ μ ≤ X + tn−1,1−α/2 S/ √n)
               ˉ
· Interval is X  ± tn−1,1−α/2
                              S/ √n
                                                                                                    9/13
 
 Note's about the t interval
· The t interval technically assumes that the data are iid normal, though it is robust to this
  assumption
· It works well whenever the distribution of the data is roughly symmetric and mound shaped
· Paired observations are often analyzed using the t interval by taking differences
· For large degrees of freedom,       t  quantiles become the same as standard normal quantiles;
  therefore this interval converges to the same interval as the CLT yielded
· For skewed distributions, the spirit of the t interval assumptions are violated
· Also, for skewed distributions, it doesn't make a lot of sense to center the interval at the mean
· In this case, consider taking logs or using a different summary like the median
· For highly discrete data, like binary, other intervals are available
                                                                                                    10/13
 
 Sleep data
In R typing data(sleep)brings up the sleep data originally analyzed in Gosset's Biometrika paper,
which shows the increase in hours for 10 patients on two soporific drugs. R treats the data as two
groups rather than paired.
                                                                                              11/13
 
 The data
data(sleep)
head(sleep)
## extra group ID
## 1 0.7     1 1
## 2 -1.6    1 2
## 3 -0.2    1 3
## 4 -1.2    1 4
## 5 -0.1    1 5
## 6 3.4     1 6
                  12/13
 
 g1 <- sleep$extra[1:10]
g2 <- sleep$extra[11:20]
difference <- g2 - g1
mn <- mean(difference)
s <- sd(difference)
n <- 10
mn + c(-1, 1) * qt(0.975, n - 1) * s/sqrt(n)
## [1] 0.7001 2.4599
t.test(difference)$conf.int
## [1] 0.7001 2.4599
## attr(,""conf.level"")
## [1] 0.95
                                             13/13
"
"./06_StatisticalInference/lectures/02_04_Likeklihood.pdf","Likelihood
Statistical Inference
Brian Caffo, Roger Peng, Jeff Leek
Johns Hopkins Bloomberg School of Public Health
 
 Likelihood
· A common and fruitful approach to statistics is to assume that the data arises from a family of
  distributions indexed by a parameter that represents a useful summary of the distribution
· The likelihood of a collection of data is the joint density evaluated as a function of the parameters
  with the data fixed
· Likelihood analysis of data uses the likelihood to perform inference regarding the unknown
  parameter
                                                                                                    2/12
 
 Likelihood
Given a statistical probability mass function or density, say f(x, θ) , where θ is an unknown parameter,
the likelihood is f viewed as a function of θ for a fixed, observed value of x .
                                                                                                    3/12
 
 Interpretations of likelihoods
The likelihood has the following properties:
  1. Ratios of likelihood values measure the relative evidence of one value of the unknown parameter
      to another.
  2. Given a statistical model and observed data, all of the relevant information contained in the data
      regarding the unknown parameter is contained in the likelihood.
  3. If {X i } are independent random variables, then their likelihoods multiply. That is, the likelihood
      of the parameters given all of the X is simply the product of the individual likelihoods.
                                          i
                                                                                                      4/12
 
 Example
· Suppose that we flip a coin with success probability θ
· Recall that the mass function for x
                                              x          1−x
                                  f(x, θ) = θ   (1 − θ )      for  θ ∈ [0, 1].
  where x is either 0 (Tails) or 1 (Heads)
· Suppose that the result is a head
· The likelihood is
                                           1         1−1
                                L(θ, 1) = θ (1 − θ)        = θ  for  θ ∈ [0, 1].
· Therefore, L(.5, 1)/L(.25, 1) = 2 ,
· There is twice as much evidence supporting the hypothesis that             θ = .5 to the hypothesis that
  θ = .25
                                                                                                       5/12
 
 Example continued
· Suppose now that we flip our coin from the previous example 4 times and get the sequence 1, 0, 1,
  1
· The likelihood is:
                                                    1        1−1  0        1−0
                                L(θ, 1, 0, 1, 1) = θ (1 − θ)     θ (1 − θ)
                                                    1        1−1  1        1−1
                                                 × θ (1 − θ)     θ (1 − θ)
                                                    3        1
                                                 = θ (1 − θ)
· This likelihood only depends on the total number of heads and the total number of tails; we might
  write L(θ, 1, 3) for shorthand
· Now consider L(.5, 1, 3)/L(.25, 1, 3) = 5.33
· There is over five times as much evidence supporting the hypothesis that θ = .5 over that θ = .25
                                                                                                  6/12
 
 Plotting likelihoods
· Generally, we want to consider all the values of θ between 0 and 1
· A likelihood plot displays θ by L(θ, x)
· Because the likelihood measures relative evidence, dividing the curve by its maximum value (or
  any other value for that matter) does not change its interpretation
                                                                                            7/12
 
 pvals <- seq(0, 1, length = 1000)
plot(pvals, dbinom(3, 4, pvals) / dbinom(3, 4, 3/4), type = ""l"", frame = FALSE, lwd = 3, xlab = ""p"", ylab 
                                                                                             8/12
 
 Maximum likelihood
· The value of θ where the curve reaches its maximum has a special meaning
· It is the value of θ that is most well supported by the data
· This point is called the maximum likelihood estimate (or MLE) of θ
                                          M LE = argmax θ L(θ, x).
· Another interpretation of the MLE is that it is the value of   θ that would make the data that we
  observed most probable
                                                                                                 9/12
 
 Some results
                  iid
· X 1 , … , X n ∼ N (μ, σ
                             2
                               ) the MLE of   μ  is  X
                                                      ˉ
                                                          and the ML of     σ
                                                                              2
                                                                                is the biased sample variance
  estimate.
                     iid
· If X  1,  … , X n ∼ Bernoulli(p)     then the MLE of p is      X
                                                                  ˉ
                                                                    (the sample proportion of 1s).
                                                            n
           iid                                           ∑ i=1 X i
· If X  i   ∼ Binomial(ni , p)   then the MLE of   p is     n       (the sample proportion of 1s).
                                                          ∑     ni
                                                            i=1
          iid
· If X    ∼ P oisson(λt)   then the MLE of λ is X/t .
                                                       n
           iid                                       ∑ i=1 X i
· If X  i   ∼ P oisson(λti )  then the MLE of λ is      n
                                                     ∑      ti
                                                        i=1
                                                                                                          10/12
 
 Example
· You saw 5 failure events per 94 days of monitoring a nuclear pump.
· Assuming Poisson, plot the likelihood
                                                                     11/12
 
 lambda <- seq(0, .2, length = 1000)
likelihood <- dpois(5, 94 * lambda) / dpois(5, 5)
plot(lambda, likelihood, frame = FALSE, lwd = 3, type = ""l"", xlab = expression(lambda))
lines(rep(5/94, 2), 0 : 1, col = ""red"", lwd = 3)
lines(range(lambda[likelihood > 1/16]), rep(1/16, 2), lwd = 2)
lines(range(lambda[likelihood > 1/8]), rep(1/8, 2), lwd = 2)
                                                                                        12/12
"
"./06_StatisticalInference/lectures/02_05_Bayes.pdf","Bayesian inference
Statistical Inference
Brian Caffo, Roger Peng, Jeff Leek
Johns Hopkins Bloomberg School of Public Health
 
 Bayesian analysis
· Bayesian statistics posits a prior on the parameter of interest
· All inferences are then performed on the distribution of the parameter given the data, called the
  posterior
· In general,
                                  Posterior ∝ Likelihood × Prior
· Therefore (as we saw in diagnostic testing) the likelihood is the factor by which our prior beliefs are
  updated to produce conclusions in the light of the data
                                                                                                      2/12
 
 Prior specification
· The beta distribution is the default prior for parameters between 0 and 1.
· The beta density depends on two parameters α and β
                                Γ(α + β)                   β−1
                                              α−1
                                            p     (1 − p )     for 0 ≤ p ≤ 1
                                Γ(α)Γ(β)
· The mean of the beta density is α/(α + β)
· The variance of the beta density is
                                                       αβ
                                                     2
                                            (α + β ) (α + β + 1)
· The uniform density is the special case where α = β = 1
                                                                             3/12
 
 ## Exploring the beta density
library(manipulate)
pvals <- seq(0.01, 0.99, length = 1000)
manipulate(
    plot(pvals, dbeta(pvals, alpha, beta), type = ""l"", lwd = 3, frame = FALSE),
    alpha = slider(0.01, 10, initial = 1, step = .5),
    beta = slider(0.01, 10, initial = 1, step = .5)
    )
                                                                                4/12
 
 Posterior
· Suppose that we chose values of α and β so that the beta prior is indicative of our degree of belief
  regarding p in the absence of data
· Then using the rule that
                                    Posterior ∝ Likelihood × Prior
  and throwing out anything that doesn't depend on p , we have that
                                             x          n−x     α−1          β−1
                               Posterior ∝ p   (1 − p )     × p     (1 − p )
                                             x+α−1          n−x+β−1
                                         = p       (1 − p )
· This density is just another beta density with parameters α̃ = x + α and β˜    = n − x + β
                                                                                                  5/12
 
 Posterior mean
                         α̃
           E[p | X] =
                            ˜
                      α̃ + β
                              x + α
                    =
                      x + α + n − x + β
                         x + α
                    =
                      n + α + β
                      x          n        α       α + β
                    =    ×            +       ×
                      n     n + α + β   α + β   n + α + β
                    = MLE × π + Prior Mean × (1 − π)
                                                          6/12
 
 Thoughts
· The posterior mean is a mixture of the MLE (p̂ ) and the prior mean
· π goes to 1 as n gets large; for large n the data swamps the prior
· For small n, the prior mean dominates
· Generalizes how science should ideally work; as data becomes increasingly available, prior beliefs
  should matter less and less
· With a prior that is degenerate at a value, no amount of data can overcome the prior
                                                                                                7/12
 
 Example
· Suppose that in a random sample of an at-risk population             13  of  20  subjects had hypertension.
  Estimate the prevalence of hypertension in this population.
· x = 13  and n = 20
· Consider a uniform prior, α = β = 1
· The posterior is proportional to (see formula above)
                                      x+α−1          n−x+β−1      x          n−x
                                    p       (1 − p)           = p   (1 − p)
  That is, for the uniform prior, the posterior is the likelihood
· Consider the instance where         α = β = 2    (recall this prior is humped around the point      .5 ) the
  posterior is
                                   x+α−1          n−x+β−1       x+1          n−x+1
                                p        (1 − p )           = p     (1 − p )
· The ""Jeffrey's prior"" which has some theoretical benefits puts α = β = .5
                                                                                                           8/12
 
 pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot <- function(alpha, beta){
    plot(0 : 1, 0 : 1, type = ""n"", xlab = ""p"", ylab = """", frame = FALSE)
    lines(pvals, dbeta(pvals, alpha, beta) / max(dbeta(pvals, alpha, beta)),
            lwd = 3, col = ""darkred"")
    lines(pvals, dbinom(x,n,pvals) / dbinom(x,n,x/n), lwd = 3, col = ""darkblue"")
    lines(pvals, dbeta(pvals, alpha+x, beta+(n-x)) / max(dbeta(pvals, alpha+x, beta+(n-x))),
        lwd = 3, col = ""darkgreen"")
    title(""red=prior,green=posterior,blue=likelihood"")
}
manipulate(
    myPlot(alpha, beta),
    alpha = slider(0.01, 10, initial = 1, step = .5),
    beta = slider(0.01, 10, initial = 1, step = .5)
    )
                                                                                             9/12
 
 Credible intervals
· A Bayesian credible interval is the Bayesian analog of a confidence interval
· A 95% credible interval, [a, b] would satisfy
                                          P (p ∈ [a, b] | x) = .95
· The best credible intervals chop off the posterior with a horizontal line in the same way we did for
  likelihoods
· These are called highest posterior density (HPD) intervals
                                                                                                 10/12
 
 Getting HPD intervals for this example
 · Install the \texttt{binom} package, then the command
 library(binom)
 binom.bayes(13, 20, type = ""highest"")
   method x n shape1 shape2 mean lower upper sig
 1 bayes 13 20 13.5           7.5 0.6429 0.4423 0.8361 0.05
gives the HPD interval.
 · The default credible level is 95% and the default prior is the Jeffrey's prior.
                                                                                   11/12
 
 pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot2 <- function(alpha, beta, cl){
    plot(pvals, dbeta(pvals, alpha+x, beta+(n-x)), type = ""l"", lwd = 3,
    xlab = ""p"", ylab = """", frame = FALSE)
    out <- binom.bayes(x, n, type = ""highest"",
        prior.shape1 = alpha,
        prior.shape2 = beta,
        conf.level = cl)
    p1 <- out$lower; p2 <- out$upper
    lines(c(p1, p1, p2, p2), c(0, dbeta(c(p1, p2), alpha+x, beta+(n-x)), 0),
        type = ""l"", lwd = 3, col = ""darkred"")
}
manipulate(
    myPlot2(alpha, beta, cl),
    alpha = slider(0.01, 10, initial = 1, step = .5),
    beta = slider(0.01, 10, initial = 1, step = .5),
    cl = slider(0.01, 0.99, initial = 0.95, step = .01)
    )
                                                                             12/12
"
"./06_StatisticalInference/lectures/03_01_TwoGroupIntervals.pdf","Two group intervals
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Independent group t confidence intervals
 · Suppose that we want to compare the mean blood pressure between two groups in a randomized
   trial; those who received the treatment to those who received a placebo
 · We cannot use the paired t test because the groups are independent and may have different
   sample sizes
 · We now present methods for comparing independent groups
                                                                                          2/13
 
 Notation
· Let X  1, … , X nx   be iid N (μ  x
                                      ,σ
                                         2
                                           )
· Let Y 1, … , Y ny  be iid N (μ  y
                                    ,σ
                                       2
                                         )
       ˉ
· Let X  , Yˉ, S , S be the means and standard deviations
                x    y
                                                                                     ˉ  ˉ
· Using the fact that linear combinations of normals are again normal, we know that Y − X is also
                                                      1     1
  normal with mean μ − μ and variance σ ( + )
                          y     x
                                                   2
                                                     nx    ny
· The pooled variance estimator
                                  2                2             2
                                S    = {( nx − 1) Sx + (ny − 1)S y }/( nx + ny − 2)
                                  p
  is a good estimator of σ    2
                                                                                              3/13
 
 Note
· The pooled estimator is a mixture of the group variances, placing greater weight on whichever has
  a larger sample size
· If the sample sizes are the same the pooled variance estimate is the average of the group
  variances
· The pooled estimator is unbiased
                                                     2                  2
                                        (nx − 1)E[S    ] + (ny − 1)E[S    ]
                                                    x                   y
                                  2
                              E[S   ] =
                                  p
                                                  nx + ny − 2
                                                  2               2
                                        (nx − 1)σ   + (ny − 1)σ
                                      =
                                              nx + ny − 2
· The pooled variance estimate is independent of Yˉ − X    ˉ
                                                             since  Sx is independent of ˉ
                                                                                         X and Sy  is
  independent of Yˉ and the groups are independent
                                                                                                 4/13
 
 Result
· The sum of two independent Chi-squared random variables is Chi-squared with degrees of
  freedom equal to the sum of the degrees of freedom of the summands
· Therefore
                                     2    2                2      2             2    2
                      (nx + ny − 2)S   /σ   = (nx − 1)S      /σ     + (ny − 1)S   /σ
                                     p                     x                   y
                                                2          2
                                            = χ        + χ
                                                n x −1     n y −1
                                                2
                                            = χ
                                                n x +ny −2
                                                                                       5/13
 
 Putting this all together
· The statistic
                          ˉ   ˉ
                          Y −X −(μy −μ x )
                                       1/2
                                            − −− −−−−− −−−−− −
                              1    1       
                                                             2    ˉ     ˉ
                          σ(    +    )
                                            (n + n − 2)S
                             nx   ny
                                                x    y           Y − X − (μ      − μ )
                                                             p                 y      x
                                           
                                                               =
                                           ⎷ (n   + ny − 2)σ
                                                             2                    1/2
                                                x                        1     1
                                                                    Sp (    +    )
                                                                         nx   ny
  is a standard normal divided by the square root of an independent Chi-squared divided by its
  degrees of freedom
· Therefore this statistic follows Gosset's t distribution with n x + ny − 2  degrees of freedom
· Notice the form is (estimator - true value) / SE
                                                                                                 6/13
 
 Confidence interval
· Therefore a (1 − α) × 100% confidence interval for μ  y
                                                          − μ
                                                             x
                                                               is
                                                                      1/2
                                                            1     1
                               ˉ    ˉ
                              Y − X ± tn +n −2,1−α/2 Sp (      +     )
                                           x  y
                                                            nx    ny
· Remember this interval is assuming a constant variance across the two groups
· If there is some doubt, assume a different variance per group, which we will discuss later
                                                                                             7/13
 
 Example
Based on Rosner, Fundamentals of Biostatistics
 · Comparing SBP for 8 oral contraceptive users versus 21 controls
    ˉ
 · X OC = 132.86  mmHg with s  OC = 15.34  mmHg
    ˉ
 · X C = 127.44  mmHg with s C  = 18.23 mmHg
 · Pooled variance estimate
 sp <- sqrt((7 * 15.34^2 + 20 * 18.23^2) / (8 + 21 - 2))
 132.86 - 127.44 + c(-1, 1) * qt(.975, 27) * sp * (1 / 8 + 1 / 21)^.5
 [1] -9.521 20.361
                                                                      8/13
 
 data(sleep)
x1 <- sleep$extra[sleep$group == 1]
x2 <- sleep$extra[sleep$group == 2]
n1 <- length(x1)
n2 <- length(x2)
sp <- sqrt( ((n1 - 1) * sd(x1)^2 + (n2-1) * sd(x2)^2) / (n1 + n2-2))
md <- mean(x1) - mean(x2)
semd <- sp * sqrt(1 / n1 + 1/n2)
md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd
[1] -3.3639 0.2039
t.test(x1, x2, paired = FALSE, var.equal = TRUE)$conf
[1] -3.3639 0.2039
attr(,""conf.level"")
[1] 0.95
t.test(x1, x2, paired = TRUE)$conf
                                                                     9/13
[1] -2.4599 -0.7001
 
 Ignoring pairing
                 10/13
 
 Unequal variances
· Under unequal variances
                                                               2      2
                                                              sx     σy
                                   ˉ     ˉ
                                  Y − X ∼ N (μ       − μ ,         +    )
                                                   y      x
                                                              nx     ny
· The statistic
                                            ˉ    ˉ
                                           Y − X − (μ − μ )
                                                        y      x
                                                           1/2
                                                 2    2
                                                sx   σy
                                              (    +    )
                                                nx   ny
  approximately follows Gosset's t distribution with degrees of freedom equal to
                                                                 2
                                              2         2
                                           (Sx /nx + S y / ny )
                                       2                    2
                                                               2
                                    2
                                  Sx                      Sy
                                (     ) /(nx − 1) + (         ) /(ny − 1)
                                  nx                      ny
                                                                                 11/13
 
 Example
· Comparing SBP for 8 oral contraceptive users versus 21 controls
   ˉ
· X OC = 132.86   mmHg with s    OC = 15.34 mmHg
   ˉ
· X C = 127.44  mmHg with s    C  = 18.23 mmHg
· df = 15.04 ,t15.04,.975 = 2.13
· Interval
                                                               1/2
                                                  2         2
                                            15.34     18.23
                  132.86 − 127.44 ± 2.13 (          +         )    = [−8.91, 19.75]
                                               8        21
· In R, t.test(..., var.equal = FALSE)
                                                                                    12/13
 
 Comparing other kinds of data
· For binomial data, there's lots of ways to compare two groups
    - Relative risk, risk difference, odds ratio.
    - Chi-squared tests, normal approximations, exact tests.
· For count data, there's also Chi-squared tests and exact tests.
· We'll leave the discussions for comparing groups of data for binary and count data until covering
  glms in the regression class.
· In addition, Mathematical Biostatistics Boot Camp 2 covers many special cases relevant to
  biostatistics.
                                                                                               13/13
"
"./06_StatisticalInference/lectures/03_02_HypothesisTesting.pdf","Hypothesis testing
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Hypothesis testing
· Hypothesis testing is concerned with making decisions using data
· A null hypothesis is specified that represents the status quo, usually labeled H0
· The null hypothesis is assumed true and statistical evidence is required to reject it in favor of a
  research or alternative hypothesis
                                                                                                  2/17
 
 Example
· A respiratory disturbance index of more than 30 events / hour, say, is considered evidence of
  severe sleep disordered breathing (SDB).
· Suppose that in a sample of 100 overweight subjects with other risk factors for sleep disordered
  breathing at a sleep clinic, the mean RDI was 32 events / hour with a standard deviation of 10
  events / hour.
· We might want to test the hypothesis that
    - H0 : μ = 30
    - Ha : μ > 30
    - where μ is the population mean RDI.
                                                                                               3/17
 
 Hypothesis testing
· The alternative hypotheses are typically of the form <, > or ≠
· Note that there are four possible outcomes of our statistical decision process
TRUTH                  DECIDE                  RESULT
H0                     H0                      Correctly accept null
H0                     Ha                      Type I error
Ha                     Ha                      Correctly reject null
Ha                     H0                      Type II error
                                                                                 4/17
 
 Discussion
· Consider a court of law; the null hypothesis is that the defendant is innocent
· We require evidence to reject the null hypothesis (convict)
· If we require little evidence, then we would increase the percentage of innocent people convicted
  (type I errors); however we would also increase the percentage of guilty people convicted
  (correctly rejecting the null)
· If we require a lot of evidence, then we increase the the percentage of innocent people let free
  (correctly accepting the null) while we would also increase the percentage of guilty people let free
  (type II errors)
                                                                                                   5/17
 
 Example
· Consider our example again
                                                               ˉ
· A reasonable strategy would reject the null hypothesis if X    was larger than some constant, say C
· Typically, C  is chosen so that the probability of a Type I error,   α , is .05 (or some other relevant
  constant)
· α  = Type I error rate = Probability of rejecting the null hypothesis when, in fact, the null hypothesis
  is correct
                                                                                                       6/17
 
 Example continued
                                      ˉ
                         0.05 = P ( X ≥ C | μ = 30)
                                        ˉ
                                        X − 30       C − 30
                              = P(          −−−  ≥       −−− | μ = 30)
                                      10/ √100      10/ √100
                                           C − 30
                              = P (Z ≥            )
                                              1
· Hence (C − 30)/1 = 1.645 implying C = 31.645
· Since our mean is 32 we reject the null hypothesis
                                                                       7/17
 
 Discussion
· In general we don't convert C back to the original scale
· We would just reject because the Z-score; which is how many standard errors the sample mean is
  above the hypothesized mean
                                              32 − 30
                                                   −− − = 2
                                             10/ √100
  is greater than 1.645
                      ˉ
· Or, whenever √n(X     − μ 0
                              )/s > Z1−α
                                                                                            8/17
 
 General rules
· The Z test for H   0 : μ = μ0 versus
    - H1 : μ < μ
                   0
    - H2 : μ ≠ μ
                   0
    - H3 : μ > μ
                   0
                          ˉ
· Test statistic T S  =
                         X −μ 0
                         S/ √ n
· Reject the null hypothesis when
    - T S ≤ − Z 1−α
    - |T S| ≥ Z 1−α/2
    - T S ≥ Z 1−α
                                       9/17
 
 Notes
· We have fixed α to be low, so if we reject       H0   (either our model is wrong) or there is a low
  probability that we have made an error
· We have not fixed the probability of a type II error,  β ; therefore we tend to say ``Fail to reject H0  ''
  rather than accepting H   0
· Statistical significance is no the same as scientific significance
· The region of TS values for which you reject H is called the rejection region
                                                  0
                                                                                                       10/17
 
 More notes
· The Z test requires the assumptions of the CLT and for n to be large enough for it to apply
· If n is small, then a Gossett's T test is performed exactly in the same way, with the normal
  quantiles replaced by the appropriate Student's T quantiles and n − 1 df
· The probability of rejecting the null hypothesis when it is false is called power
· Power is a used a lot to calculate sample sizes for experiments
                                                                                              11/17
 
 Example reconsidered
· Consider our example again. Suppose that n = 16 (rather than 100 ). Then consider that
                                              ˉ
                                             X − 30
                                   .05 = P (     −− ≥ t1−α,15 | μ = 30)
                                             s/ √ 16
                                      −−
· So that our test statistic is now √16 (32 − 30)/10 = 0.8 , while the critical value is t 1−α,15 = 1.75 .
· We now fail to reject.
                                                                                                      12/17
 
 Two sided tests
· Suppose that we would reject the null hypothesis if in fact the mean was too large or too small
· That is, we want to test the alternative H     a : μ ≠ 30   (doesn't make a lot of sense in our setting)
· Then note
                                             ∣X ˉ
                                                   − 30 ∣              ∣
                                   α = P (∣          −−  ∣ > t1−α/2,15 ∣ μ = 30)
                                             ∣ s/ √ 16 ∣               ∣
· That is we will reject if the test statistic, 0.8 , is either too large or too small, but the critical value is
  calculated using α/2
· In our example the critical value is 2.13 , so we fail to reject.
                                                                                                             13/17
 
 T test in R
library(UsingR); data(father.son)
t.test(father.son$sheight - father.son$fheight)
    One Sample t-test
data: father.son$sheight - father.son$fheight
t = 11.79, df = 1077, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 0.831 1.163
sample estimates:
mean of x
    0.997
                                                    14/17
 
 Connections with confidence intervals
· Consider testing H 0 : μ = μ0 versus H a : μ ≠ μ0
· Take the set of all possible values for which you fail to reject  H0 , this set is a  (1 − α)100%
  confidence interval for μ
· The same works in reverse; if a (1 − α)100% interval contains μ , then we fail to reject H
                                                                 0                          0
                                                                                                15/17
 
 Exact binomial test
· Recall this problem, Suppose a friend has 8 children, 7 of which are girls and none are twins
· Perform the relevant hypothesis test. H   0 : p = 0.5 Ha : p > 0.5
      - What is the relevant rejection region so that the probability of rejecting is (less than) 5%?
REJECTION REGION                                      TYPE I ERROR RATE
[0 : 8]                                               1
[1 : 8]                                               0.9961
[2 : 8]                                               0.9648
[3 : 8]                                               0.8555
[4 : 8]                                               0.6367
[5 : 8]                                               0.3633
[6 : 8]                                               0.1445
[7 : 8]                                               0.0352
[8 : 8]                                               0.0039
                                                                                                      16/17
 
 Notes
· It's impossible to get an exact 5% level test for this case due to the discreteness of the binomial.
      - The closest is the rejection region [7 : 8]
      - Any alpha level lower than 0.0039 is not attainable.
· For larger sample sizes, we could do a normal approximation, but you already knew this.
· Two sided test isn't obvious.
      - Given a way to do two sided tests, we could take the set of values of     p0 for which we fail to
        reject to get an exact binomial confidence interval (called the Clopper/Pearson interval, BTW)
· For these problems, people always create a P-value (next lecture) rather than computing the
  rejection region.
                                                                                                    17/17
"
"./06_StatisticalInference/lectures/03_03_pValues.pdf","P-values
Statistical inference
Brian Caffo, Jeffrey Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 P-values
· Most common measure of ""statistical significance""
· Their ubiquity, along with concern over their interpretation and use makes them controversial
  among statisticians
    - http://warnercnr.colostate.edu/~anderson/thompson1.html
    - Also see Statistical Evidence: A Likelihood Paradigm by Richard Royall
    - Toward Evidence-Based Medical Statistics. 1: The P Value Fallacy by Steve Goodman
    - The hilariously titled: The Earth is Round (p < .05) by Cohen.
· Some positive comments
    - simply statistics
    - normal deviate
    - Error statistics
                                                                                            2/8
 
 What is a P-value?
Idea: Suppose nothing is going on - how unusual is it to see the estimate we got?
Approach:
  1. Define the hypothetical distribution of a data summary (statistic) when ""nothing is going on"" (null
      hypothesis)
  2. Calculate the summary/statistic with the data we have (test statistic)
  3. Compare what we calculated to our hypothetical distribution and see if the value is ""extreme"" (p-
      value)
                                                                                                     3/8
 
 P-values
· The P-value is the probability under the null hypothesis of obtaining evidence as extreme or more
  extreme than would be observed by chance alone
· If the P-value is small, then either H is true and we have observed a rare event or H is false
                                         0                                             0
· In our example the T statistic was 0.8 .
      - What's the probability of getting a T statistic as large as 0.8 ?
pt(0.8, 15, lower.tail = FALSE)
[1] 0.2181
· Therefore, the probability of seeing evidence as extreme or more extreme than that actually
  obtained under H is 0.2181
                     0
                                                                                                 4/8
 
 The attained significance level
· Our test statistic was 2 for H0 : μ0 = 30  versus H    a : μ > 30 .
· Notice that we rejected the one sided test when α = 0.05 , would we reject if α = 0.01 , how about
  0.001 ?
· The smallest value for alpha that you still reject the null hypothesis is called the attained
  significance level
· This is equivalent, but philosophically a little different from, the P-value
                                                                                                  5/8
 
 Notes
· By reporting a P-value the reader can perform the hypothesis test at whatever    α  level he or she
  choses
· If the P-value is less than α you reject the null hypothesis
· For two sided hypothesis test, double the smaller of the two one sided hypothesis test Pvalues
                                                                                                   6/8
 
 Revisiting an earlier example
· Suppose a friend has 8 children, 7 of which are girls and none are twins
· If each gender has an independent 50 % probability for each birth, what's the probability of getting
  7  or more girls out of 8 births?
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8
[1] 0.03516
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
[1] 0.03516
                                                                                                     7/8
 
 Poisson example
· Suppose that a hospital has an infection rate of 10 infections per 100 person/days at risk (rate of
  0.1) during the last monitoring period.
· Assume that an infection rate of 0.05 is an important benchmark.
· Given the model, could the observed rate being larger than 0.05 be attributed to chance?
· Under H  0 : λ = 0.05  so that λ 0 100 = 5
· Consider H   a : λ > 0.05 .
ppois(9, 5, lower.tail = FALSE)
[1] 0.03183
                                                                                                  8/8
"
"./06_StatisticalInference/lectures/03_04_Power.pdf","Power
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Power
· Power is the probability of rejecting the null hypothesis when it is false
· Ergo, power (as it's name would suggest) is a good thing; you want more power
· A type II error (a bad thing, as its name would suggest) is failing to reject the null hypothesis when
  it's false; the probability of a type II error is usually called β
· Note Power =      1 −β
                                                                                                     2/12
 
 Notes
· Consider our previous example involving RDI
· H0 : μ = 30   versus H  a : μ > 30
· Then power is
                                         ˉ
                                         X − 30
                                     P(          > t1−α,n−1 | μ = μ )
                                                                    a
                                         s/ √ n
· Note that this is a function that depends on the specific value of μ !
                                                                      a
· Notice as μ approaches 30 the power approaches α
              a
                                                                         3/12
 
 Calculating power for Gaussian data
Assume that n is large and that we know σ
                                      ˉ
                                      X − 30
                         1 − β = P(           > z 1−α | μ = μa )
                                      σ/ √n
                                      ˉ
                                      X − μ + μ − 30
                                            a    a
                                = P(                     > z 1−α | μ = μ )
                                                                         a
                                           σ/ √n
                                      ˉ
                                      X −μ               μ   − 30
                                            a              a
                                = P(          > z 1−α −            | μ = μ )
                                                                           a
                                      σ/ √n               σ/ √n
                                                  μ   − 30
                                                    a
                                = P (Z > z 1−α −             | μ = μa )
                                                   σ/ √ n
                                                                             4/12
 
 Example continued
· Suppose that we wanted to detect a increase in mean RDI of at least 2 events / hour (above 30).
· Assume normality and that the sample in question will have a standard deviation of 4 ;
· What would be the power if we took a sample size of 16 ?
    - Z 1−α = 1.645
       μa −30          −−
    -  σ/√ n
              = 2/(4/ √16 ) = 2
    - P (Z > 1.645 − 2) = P (Z > −0.355) = 64%
pnorm(-0.355, lower.tail = FALSE)
[1] 0.6387
                                                                                                5/12
 
 Note
· Consider H  0  : μ = μ0  and H      a : μ > μ0    with μ = μ under H .
                                                              a       a
                                          ˉ
· Under H the statistic Z
          0                     =
                                     √ n(X −μ 0 )
                                                  is N (0, 1)
                                          σ
· Under H   Z   is N (
                        √n (μ a −μ0 )
                                      , 1)
          a
                             σ
· We reject if Z   > Z 1−α
sigma <- 10; mu_0 = 0; mu_a = 2; n <- 100; alpha = .05
plot(c(-3, 6),c(0, dnorm(0)), type = ""n"", frame = false, xlab = ""Z value"", ylab = """")
xvals <- seq(-3, 6, length = 1000)
lines(xvals, dnorm(xvals), type = ""l"", lwd = 3)
lines(xvals, dnorm(xvals, mean = sqrt(n) * (mu_a - mu_0) / sigma), lwd =3)
abline(v = qnorm(1 - alpha))
                                                                                      6/12
 
 7/12 
 Question
· When testing H   a : μ > μ0 , notice if power is 1 − β , then
                                                    μ  − μ0
                                                     a
                        1 − β = P (Z > z 1−α −                | μ = μ ) = P (Z > z β )
                                                                     a
                                                     σ/ √n
· This yields the equation
                                                  √ n(μa − μ0 )
                                          z 1−α −                 = zβ
                                                        σ
· Unknowns: μ , σ , n, β
                a
· Knowns: μ , α
             0
· Specify any 3 of the unknowns and you can solve for the remainder
                                                                                       8/12
 
 Notes
· The calculation for H  a  : μ < μ0  is similar
· For Ha : μ ≠ μ
                 0
                    calculate the one sided power using              α/2 (this is only approximately right, it
  excludes the probability of getting a large TS in the opposite direction of the truth)
· Power goes up as α gets larger
· Power of a one sided test is greater than the power of the associated two sided test
· Power goes up as μ gets further away from μ
                        1                             0
· Power goes up as n goes up
                                                      √ n( μa −μ 0 )
· Power doesn't need μ , σ and n, instead only
                           a                                σ
                     μa −μ 0
    - The quantity      σ
                              is called the effect size, the difference in the means in standard deviation
      units.
    - Being unit free, it has some hope of interpretability across settings
                                                                                                         9/12
 
 T-test power
· Consider calculating power for a Gossett's T test for our example
· The power is
                                        ˉ
                                       X −μ
                                                0
                                   P(             > t1−α,n−1 | μ = μ )
                                                                    a
                                        S/ √n
· Calcuting this requires the non-central t distribution.
· power.t.testdoes this very well
    - Omit one of the arguments and it solves for it
                                                                       10/12
 
 Example
power.t.test(n = 16, delta = 2 / 4, sd=1, type = ""one.sample"", alt = ""one.sided"")$power
[1] 0.604
power.t.test(n = 16, delta = 2, sd=4, type = ""one.sample"", alt = ""one.sided"")$power
[1] 0.604
power.t.test(n = 16, delta = 100, sd=200, type = ""one.sample"", alt = ""one.sided"")$power
[1] 0.604
                                                                                        11/12
 
 Example
power.t.test(power = .8, delta = 2 / 4, sd=1, type = ""one.sample"", alt = ""one.sided"")$n
[1] 26.14
power.t.test(power = .8, delta = 2, sd=4, type = ""one.sample"", alt = ""one.sided"")$n
[1] 26.14
power.t.test(power = .8, delta = 100, sd=200, type = ""one.sample"", alt = ""one.sided"")$n
[1] 26.14
                                                                                        12/12
"
"./06_StatisticalInference/lectures/03_05_MultipleTesting.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./06_StatisticalInference/lectures/03_06_resampledInference.pdf","Resampled inference
Statistical Inference
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 The jackknife
· The jackknife is a tool for estimating standard errors and the bias of estimators
· As its name suggests, the jackknife is a small, handy tool; in contrast to the bootstrap, which is
  then the moral equivalent of a giant workshop full of tools
· Both the jackknife and the bootstrap involve resampling data; that is, repeatedly creating new data
  sets from the original data
                                                                                                  2/21
 
 The jackknife
· The jackknife deletes each observation and calculates an estimate based on the remaining        n − 1
  of them
· It uses this collection of estimates to do things like estimate the bias and the standard error
· Note that estimating the bias and having a standard error are not needed for things like sample
  means, which we know are unbiased estimates of population means and what their standard
  errors are
                                                                                                    3/21
 
 The jackknife
· We'll consider the jackknife for univariate data
· Let X  1, … , Xn   be a collection of data used to estimate a parameter θ
      ˆ
· Let θ  be the estimate based on the full data set
      ˆ
· Let θi  be the estimate of θ obtained by deleting observation i
      ˉ     1    n   ˆ
· Let θ =
            n
              ∑
                 i=1
                     θi
                                                                            4/21
 
 Continued
· Then, the jackknife estimate of the bias is
                                                      ˉ   ˆ
                                             (n − 1)( θ − θ )
  (how far the average delete-one estimate is from the actual estimate)
· The jackknife estimate of the standard error is
                                                              1/2
                                                  n
                                          n − 1             2
                                                    ˆ    ˉ
                                        [       ∑ (θ i − θ ) ]
                                            n
                                                i=1
  (the deviance of the delete-one estimates from the average delete-one estimate)
                                                                                  5/21
 
 Example
We want to estimate the bias and standard error of the median
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
theta <- median(x)
jk <- sapply(1 : n,
             function(i) median(x[-i])
             )
thetaBar <- mean(jk)
biasEst <- (n - 1) * (thetaBar - theta)
seEst <- sqrt((n - 1) * mean((jk - thetaBar)^2))
                                                              6/21
 
 Example
c(biasEst, seEst)
[1] 0.0000 0.1014
library(bootstrap)
temp <- jackknife(x, median)
c(temp$jack.bias, temp$jack.se)
[1] 0.0000 0.1014
                                7/21
 
 Example
· Both methods (of course) yield an estimated bias of 0 and a se of 0.1014
· Odd little fact: the jackknife estimate of the bias for the median is always  0 when the number of
  observations is even
· It has been shown that the jackknife is a linear approximation to the bootstrap
· Generally do not use the jackknife for sample quantiles like the median; as it has been shown to
  have some poor properties
                                                                                                8/21
 
 Pseudo observations
· Another interesting way to think about the jackknife uses pseudo observations
· Let
                                                      ˆ           ˆ
                                    Pseudo Obs = nθ − (n − 1) θ i
· Think of these as ``whatever observation i contributes to the estimate of θ''
              ˆ
· Note when   θ is the sample mean, the pseudo observations are the data themselves
· Then the sample standard error of these observations is the previous jackknife estimated standard
  error.
· The mean of these observations is a bias-corrected estimate of θ
                                                                                                9/21
 
 The bootstrap
· The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating
  standard errors for difficult statistics
· For example, how would one derive a confidence interval for the median?
· The bootstrap procedure follows from the so called bootstrap principle
                                                                                               10/21
 
 The bootstrap principle
· Suppose that I have a statistic that estimates some population parameter, but I don't know its
  sampling distribution
· The bootstrap principle suggests using the distribution defined by the data to approximate its
  sampling distribution
                                                                                           11/21
 
 The bootstrap in practice
· In practice, the bootstrap principle is always carried out using simulation
· We will cover only a few aspects of bootstrap resampling
· The general procedure follows by first simulating complete data sets from the observed data with
  replacement
     - This is approximately drawing from the sampling distribution of that statistic, at least as far as
       the data is able to approximate the true population distribution
· Calculate the statistic for each simulated data set
· Use the simulated statistics to either define a confidence interval or take the standard deviation to
  calculate a standard error
                                                                                                     12/21
 
 Nonparametric bootstrap algorithm example
· Bootstrap procedure for calculating confidence interval for the median from a data set of n
  observations
  i. Sample n observations with replacement from the observed data resulting in one simulated
  complete data set
  ii. Take the median of the simulated data set
  iii. Repeat these two steps B times, resulting in B simulated medians
  iv. These medians are approximately drawn from the sampling distribution of the median of n
  observations; therefore we can
      - Draw a histogram of them
      - Calculate their standard deviation to estimate the standard error of the median
                     th          th
      - Take the 2.5    and 97.5    percentiles as a confidence interval for the median
                                                                                         13/21
 
 Example code
B <- 1000
resamples <- matrix(sample(x,
                           n * B,
                           replace = TRUE),
                    B, n)
medians <- apply(resamples, 1, median)
sd(medians)
[1] 0.08546
quantile(medians, c(.025, .975))
 2.5% 97.5%
68.43 68.82
                                            14/21
 
 Histogram of bootstrap resamples
hist(medians)
                                 15/21
 
 Notes on the bootstrap
· The bootstrap is non-parametric
· Better percentile bootstrap confidence intervals correct for bias
· There are lots of variations on bootstrap procedures; the book ""An Introduction to the Bootstrap""""
  by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information
                                                                                                 16/21
 
 Group comparisons
· Consider comparing two independent groups.
· Example, comparing sprays B and C
data(InsectSprays)
boxplot(count ~ spray, data = InsectSprays)
                                             17/21
 
 Permutation tests
· Consider the null hypothesis that the distribution of the observations from each group is the same
· Then, the group labels are irrelevant
· We then discard the group levels and permute the combined data
· Split the permuted data into two groups with     nA   and nB   observations (say by always treating the
  first n observations as the first group)
         A
· Evaluate the probability of getting a statistic as large or large than the one observed
· An example statistic would be the difference in the averages between the two groups; one could
  also use a t-statistic
                                                                                                     18/21
 
 Variations on permutation testing
DATA TYPE             STATISTIC                             TEST NAME
Ranks                 rank sum                              rank sum test
Binary                hypergeometric prob                   Fisher's exact test
Raw data                                                    ordinary permutation test
· Also, so-called randomization tests are exactly permutation tests, with a different motivation.
· For matched data, one can randomize the signs
     - For ranks, this results in the signed rank test
· Permutation strategies work for regression as well
     - Permuting a regressor of interest
· Permutation tests work very well in multivariate settings
                                                                                                  19/21
 
 Permutation test for pesticide data
subdata <- InsectSprays[InsectSprays$spray %in% c(""B"", ""C""),]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == ""B""]) - mean(w[g == ""C""])
observedStat <- testStat(y, group)
permutations <- sapply(1 : 10000, function(i) testStat(y, sample(group)))
observedStat
[1] 13.25
mean(permutations > observedStat)
[1] 0
                                                                          20/21
 
 Histogram of permutations
                          21/21
"
"./06_StatisticalInference/old/004representingData/How do we represent data.pdf","7/22/13                                                                                    How do we represent data?
                          How do we represent data?
                          Jeffrey Leek, Assistant Professor of Biostatistics
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                           1/21
 
 7/22/13                                                                                    How do we represent data?
              How do we write about data?
                 · Each data point is usually represented by a capital letter.
                         -   H    for height, W for weight.
                 · If there are more than one data point of the same type we use subscripts.
                         -   H1    , H2 , H3 for three different people's heights.
                 · Sometimes it is more compact to write X1 for height and X2 for weight.
                 · Then we need another subscript for the individual data point
                         -   X 11    for the height of the first person.
                 ·  Y    representes general outcomes and X general covariates.
                 · In this course we will try to use informative letters when possible.
                                                                                                                     2/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                2/21
 
 7/22/13                                                                                    How do we represent data?
              Randomness
                 · Variables like X and Y are called random variables because we expect them to be random in some
                    way.
                 · In general, randomness is a hard thing to define
                 · In this class a variable may be random because
                         - It represents an incompletely measured variable
                         - It represents a sample drawn from a population using a random mechanism.
                 · Once we are talking about a specific value of a variable we have observed it isn't random
                    anymore, we write these values with lower case letters x, y , etc.
                 · We write X             = x   or X       = 1    to indicate we have observed a specific value x or 1.
                                                                                                                        3/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                   3/21
 
 7/22/13                                                                                    How do we represent data?
              Randomness and measurement
                 · A coin flip is commonly considered random
                 · But it can be modeled by deterministic equations
                         - Dynamical bias in the coin toss (Diaconis, Holmes and Montgomery SIAM Review 2007)
                         - Modeled the tossing as a dynamical system
                         - Showed that a coin is more likely to land on the side it started
                         - Did experiments that demonstrated it was a 51% chance
                 · Some have taken it a bit farther making predictable coin flipping machines based on physical
                    properties.
                                                                                                                     4/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                4/21
 
 7/22/13                                                                                          How do we represent data?
              Distributions
                 · In statistical modeling, random variables like X are assumed to be samples from a distribution
                 · A distribution tells us the possible values of X and the probabilities for each value.
                 · Probability is the chance something will happen and is abbreviated Pr
                 · The probabilities must all be between 0 and 1.
                 · The probabilities must add up to 1.
                 · An example:
                         - Let's flip a coin and allow X to represent whether it is heads or tails
                         -   X = 1       if it is heads and X              = 0      if it is tails
                         - We expect that about 50% of the time it will be heads.
                         - The distribution can then be written Pr(X                               = 1   )=0.5 and Pr(X    = 0 )=0.5
                                                                                                                                     5/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                                5/21
 
 7/22/13                                                                                    How do we represent data?
              Continuous versus discrete distributions
                 · discrete distributions specify probabilities for discrete values
                         - Qualitative variables are discrete
                         - So are variables that take on all values 0,1,2,3...
                 · continuous distributions specify probabilities for ranges of values
                         - Quantitative variables are often assumed to be continuous
                         - But we might only see specific values
                                                                                                                     6/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                6/21
 
 7/22/13                                                                                    How do we represent data?
              Parameters
                 · Distributions are defined by a set of fixed values called parameters.
                 · parameters are sometimes represented by Greek letters like + , 2 , 3.
                 · Distributions are written as letters with the parameters in parentheses like N(+ , 2 ) or Poisson(*) .
                 ·  X    t   N(  + , 2 ) means that X has the N(+ , 2 ) distribution.
                                                                                                                         7/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                    7/21
 
 7/22/13                                                                                       How do we represent data?
              The three most important parameters
                 · If X is a random variable, the mean of that random variable is written E[X]
                         - Stands for expected value
                         - Measures the ""center"" of a distribution
                 · The variance of that random variable is written V ar[X]
                         - Measures how ""spread out"" a distribution is
                                                                                   2
                         - Measurement is in (units of X)
                 · The standard deviation is written SD[X]                                 = R_______
                                                                                               V ar[X]
                         - Also measures how ""spread out"" a distribution is
                         - Measurement is in units of X
                                                                                                                        8/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                   8/21
 
 7/22/13                                                                                    How do we represent data?
              Conditioning
                 · The variables X are considered to be random
                 · The parameters are considered to be fixed values
                 · Sometimes we want to talk about a case where one of the random variables is fixed
                 · To indicate what is fixed, we condition using the symbol ""|""""
                         -   X| + means that X is a random variable with fixed parameter +
                         -   Y |X = 2      means Y is the random variable Y when X is fixed at 2.
                                                                                                                     9/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                9/21
 
 7/22/13                                                                                    How do we represent data?
              Example distribution: Binomial
              Binomial distribution: Bin(n, p)
                 ·  X    t   Bin(10, 0.5)
                                                                                                                     10/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                 10/21
 
 7/22/13                                                                                    How do we represent data?
              Example distribution: Normal
              Normal Distribution: N(+ , 2 )
                 ·  X    t   N(0, 1)
                                                                                                                     11/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                 11/21
 
 7/22/13                                                                                    How do we represent data?
              Example distribution: Uniform
              Uniform distribution: U (                        , !)
                 ·  X    t   U (0, 1)
                                                                                                                     12/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                 12/21
 
 7/22/13                                                                                         How do we represent data?
              Changing parameters
              Normal Distribution: N(+ , 2 )
                 ·  X    t   N(0, 1)      , E[X]     =     +  = 0    , V ar[X]        =   2 2
                                                                                              = 1
                                                                                                                          13/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                      13/21
 
 7/22/13                                                                                         How do we represent data?
              Changing parameters: the variance
              Normal Distribution: N(+ , 2 )
                 ·  X    t   N(0, 5)      , E[X]     =     +  = 0    , V ar[X]        =   2 2
                                                                                              = 25
                                                                                                                          14/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                      14/21
 
 7/22/13                                                                                         How do we represent data?
              Changing parameters: the mean
              Normal Distribution: N(+ , 2 )
                 ·  X    t   N(5, 1)      , E[X]     =     +  = 5    , V ar[X]        =   2 2
                                                                                              = 1
                                                                                                                          15/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                      15/21
 
 7/22/13                                                                                       How do we represent data?
              Example distribution: Binomial
              Binomial distribution: Bin(n, p)
                 ·  X    t   Bin(10, 0.5)        , E[X]        = n × p = 5           , V ar[X] = n × p × (1           J p) = 2.5
                                                                                                                                 16/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                             16/21
 
 7/22/13                                                                                       How do we represent data?
              Changing parameters: both mean and variance
              Binomial distribution: Bin(n, p)
                 ·  X    t   Bin(10, 0.8)        , E[X]        = n × p = 8           , V ar[X] = n × p × (1           J p) = 1.6
                                                                                                                                 17/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                             17/21
 
 7/22/13                                                                                         How do we represent data?
              Conditioning
                 · Suppose Y              t   N(X, 1)       and X        t   N(0, 1)      , then the distribution of Y |X = 5 is
                                                                                                                                 18/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                             18/21
 
 7/22/13                                                                                         How do we represent data?
              Conditioning
                 · Suppose Y              t   N(X, 1)       and X        t   N(0, 1)      , then the distribution of Y is
              http://en.wikipedia.org/wiki/Law_of_total_variance
              http://en.wikipedia.org/wiki/Law_of_total_expectation
                                                                                                                          19/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                      19/21
 
 7/22/13                                                                                    How do we represent data?
              Learning more about a specific distribution
              http://en.wikipedia.org/wiki/Poisson_distribution
                                                                                                                     20/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                 20/21
 
 7/22/13                                                                                    How do we represent data?
              Learning more about representing data
              http://www.openintro.org/stat/textbook.php
                                                                                                                     21/21
file://localhost/Users/sean/Developer/GitHub/modules/jeff/004representingData/index.html#1                                 21/21
"
"./06_StatisticalInference/old/005representingDataR/Representing data in R.pdf","7/22/13                                                                                     Representing data in R
                          Representing data in R
                          Jeffrey Leek, Assistant Professor of Biostatistics
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                        1/18
 
 7/22/13                                                                                     Representing data in R
              Important data types in R
              Classes
                 · Character, Numeric, Integer, Logical
              Objects
                 · Vectors, Matrices, Data frames, Lists, Factors, Missing values
              Operations
                 · Subsetting, Logical subsetting
              For more information:
                 · Data Types
                                                                                                                   2/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             2/18
 
 7/22/13                                                                                     Representing data in R
              Character
                 firstName = ""jeff""
                 class(firstName)
                 ## [1] ""character""
                 firstName
                 ## [1] ""jeff""
                                                                                                                   3/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             3/18
 
 7/22/13                                                                                     Representing data in R
              Numeric
                 heightCM = 188.2
                 class(heightCM)
                 ## [1] ""numeric""
                 heightCM
                 ## [1] 188.2
                                                                                                                   4/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             4/18
 
 7/22/13                                                                                     Representing data in R
              Integer
                 numberSons = 1L
                 class(numberSons)
                 ## [1] ""integer""
                 numberSons
                 ## [1] 1
                                                                                                                   5/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             5/18
 
 7/22/13                                                                                     Representing data in R
              Logical
                 teachingCoursera = TRUE
                 class(teachingCoursera)
                 ## [1] ""logical""
                 teachingCoursera
                 ## [1] TRUE
                                                                                                                   6/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             6/18
 
 7/22/13                                                                                     Representing data in R
              Vectors
              A set of values with the same class
                 heights = c(188.2, 181.3, 193.4)
                 heights
                 ## [1] 188.2 181.3 193.4
                 firstNames = c(""jeff"", ""roger"", ""andrew"", ""brian"")
                 firstNames
                 ## [1] ""jeff""                ""roger"" ""andrew"" ""brian""
                                                                                                                   7/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             7/18
 
 7/22/13                                                                                     Representing data in R
              Lists
              A vector of values of possibly different classes
                 vector1 = c(188.2, 181.3, 193.4)
                 vector2 = c(""jeff"", ""roger"", ""andrew"", ""brian"")
                 myList = list(heights = vector1, firstNames = vector2)
                 myList
                 ## $heights
                 ## [1] 188.2 181.3 193.4
                 ##
                 ## $firstNames
                 ## [1] ""jeff"" ""roger"" ""andrew"" ""brian""
                                                                                                                   8/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             8/18
 
 7/22/13                                                                                     Representing data in R
              Matrices
              Vectors with multiple dimensions
                 myMatrix = matrix(c(1, 2, 3, 4), byrow = T, nrow = 2)
                 myMatrix
                 ##            [,1] [,2]
                 ## [1,]             1        2
                 ## [2,]             3        4
                                                                                                                   9/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                             9/18
 
 7/22/13                                                                                     Representing data in R
              Data frames
              Multiple vectors of possibly different classes, of the same length
                 vector1 = c(188.2, 181.3, 193.4)
                 vector2 = c(""jeff"", ""roger"", ""andrew"", ""brian"")
                 myDataFrame = data.frame(heights = vector1, firstNames = vector2)
                 ## Error: arguments imply differing number of rows: 3, 4
                 myDataFrame
                 ## Error: object 'myDataFrame' not found
                                                                                                                   10/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              10/18
 
 7/22/13                                                                                     Representing data in R
              Data frames
                 vector1 = c(188.2, 181.3, 193.4, 192.3)
                 vector2 = c(""jeff"", ""roger"", ""andrew"", ""brian"")
                 myDataFrame = data.frame(heights = vector1, firstNames = vector2)
                 myDataFrame
                 ## heights firstNames
                 ## 1 188.2                        jeff
                 ## 2 181.3                      roger
                 ## 3 193.4                    andrew
                 ## 4 192.3                      brian
                                                                                                                   11/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              11/18
 
 7/22/13                                                                                     Representing data in R
              Factors
              Qualitative variables that can be included in models
                 smoker = c(""yes"", ""no"", ""yes"", ""yes"")
                 smokerFactor = as.factor(smoker)
                 smokerFactor
                 ## [1] yes no yes yes
                 ## Levels: no yes
                                                                                                                   12/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              12/18
 
 7/22/13                                                                                     Representing data in R
              Missing values
              In R they are usually coded NA
                 vector1 = c(188.2, 181.3, 193.4, NA)
                 vector1
                 ## [1] 188.2 181.3 193.4                            NA
                 is.na(vector1)
                 ## [1] FALSE FALSE FALSE TRUE
                                                                                                                   13/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              13/18
 
 7/22/13                                                                                     Representing data in R
              Subsetting
                 vector1 = c(188.2, 181.3, 193.4, 192.3)
                 vector2 = c(""jeff"", ""roger"", ""andrew"", ""brian"")
                 myDataFrame = data.frame(heights = vector1, firstNames = vector2)
                 vector1[1]
                 ## [1] 188.2
                 vector1[c(1, 2, 4)]
                 ## [1] 188.2 181.3 192.3
                                                                                                                   14/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              14/18
 
 7/22/13                                                                                     Representing data in R
              Subsetting
                 myDataFrame[1, 1:2]
                 ## heights firstNames
                 ## 1 188.2                        jeff
                 myDataFrame$firstNames
                 ## [1] jeff roger andrew brian
                 ## Levels: andrew brian jeff roger
                                                                                                                   15/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              15/18
 
 7/22/13                                                                                     Representing data in R
              Logical subsetting
                 myDataFrame[myDataFrame$firstNames == ""jeff"", ]
                 ## heights firstNames
                 ## 1 188.2                        jeff
                 myDataFrame[myDataFrame$heights < 190, ]
                 ## heights firstNames
                 ## 1 188.2                        jeff
                 ## 2 181.3                      roger
                                                                                                                   16/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              16/18
 
 7/22/13                                                                                     Representing data in R
              Variable naming conventions
              Variable names should be short, but descriptive. Here are some common styles
              Camel caps
                 myHeightCM = 188
              Underscore
                 my_height_cm = 188
              Dot separated
                 my.height.cm = 188
                                                                                                                   17/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              17/18
 
 7/22/13                                                                                     Representing data in R
              Style guides
                 · http://4dpiecharts.com/r-code-style-guide/
                 · http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html
                 · http://wiki.fhcrc.org/bioc/Coding_Standards
                                                                                                                   18/18
file://localhost/Users/sean/Developer/GitHub/modules/jeff/005representingDataR/index.html#1                              18/18
"
"./06_StatisticalInference/old/11. Plotting/Plotting.pdf","8/27/13                                                                              Plotting
                          Plotting
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1          1/33
 
 8/27/13                                                                              Plotting
              Table of contents
                   1. Histograms
                   2. Stem and leaf
                   3. Dotcharts
                   4. Boxplots
                   5. KDEs
                   6. QQ-plots
                   7. Mosaic plots
                                                                                              2/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1               2/33
 
 8/27/13                                                                              Plotting
              Histograms
                 · Histograms display a sample estimate of the density or mass function by plotting a bar graph of the
                    frequency or proportion of times that a variable takes specific values, or a range of values for
                    continuous data, within a sample
                                                                                                                   3/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                    3/33
 
 8/27/13                                                                              Plotting
              Example
                 · The data set islands in the R package datasets contains the areas of all land masses in
                    thousands of square miles
                 · Load the data set with the command data(islands)
                 · View the data by typing islands
                 · Create a histogram with the command hist(islands)
                 · Do ?histfor options
                                                                                                       4/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                        4/33
 
 8/27/13                                                                              Plotting
                                                                                              5/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1               5/33
 
 8/27/13                                                                              Plotting
              Pros and cons
                 · Histograms are useful and easy, apply to continuous, discrete and even unordered data
                 · They use a lot of ink and space to display very little information
                 · It's difficult to display several at the same time for comparisons
              Also, for this data it's probably preferable to consider log base 10 (orders of magnitude), since the raw
              histogram simply says that most islands are small
                                                                                                                    6/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                     6/33
 
 8/27/13                                                                              Plotting
                                                                                              7/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1               7/33
 
 8/27/13                                                                              Plotting
              Stem-and-leaf plots
                 · Stem-and-leaf plots are extremely useful for getting distribution information on the fly
                 · Read the text about creating them
                 · They display the complete data set and so waste very little ink
                 · Two data sets' stem and leaf plots can be shown back-to-back for comparisons
                 · Created by John Tukey, a leading figure in the development of the statistical sciences and signal
                    processing
                                                                                                                 8/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                  8/33
 
 8/27/13                                                                              Plotting
              Example
                 > stem(log10(islands))
                    The decimal point is at the |
                    1 | 1111112222233444
                    1 | 5555556666667899999
                    2 | 3344
                    2 | 59
                    3|
                    3 | 5678
                    4 | 012
                                                                                              9/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1               9/33
 
 8/27/13                                                                              Plotting
              Dotcharts
                 · Dotcharts simply display a data set, one dot per point
                 · Ordering of the dots and labeling of the axes can display additional information
                 · Dotcharts show a complete data set and so have high data density
                 · May be impossible to construct/difficult to interpret for data sets with lots of points
                                                                                                           10/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                             10/33
 
 8/27/13                                                                              Plotting
                                                                                              11/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                11/33
 
 8/27/13                                                                              Plotting
              Discussion
                 · Maybe ordering alphabetically isn't the best thing for this data set
                 · Perhaps grouped by continent, then nations by geography (grouping Pacific islands together)?
                                                                                                                12/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                  12/33
 
 8/27/13                                                                              Plotting
              Dotplots comparing grouped data
                 · For data sets in groups, you often want to display density information by group
                 · If the size of the data permits, displaying the whole data is preferable
                 · Add horizontal lines to depict means, medians
                 · Add vertical lines to depict variation, show confidence intervals interquartile ranges
                 · Jitter the points to avoid overplotting jitter
                                                                                                          13/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                            13/33
 
 8/27/13                                                                              Plotting
              Example
                 · The InsectSprays dataset contains counts of insect deaths by insecticide type (A, B, C, D, E, F)
                 · You can obtain the data set with the command
                 data(InsectSprays)
                                                                                                                  14/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                    14/33
 
 8/27/13                                                                              Plotting
              The gist of the code is below
                 attach(InsectSprays)
                 plot(c(.5, 6.5), range(count))
                 sprayTypes <- unique(spray)
                 for (i in 1 : length(sprayTypes)){
                    y <- count[spray == sprayTypes[i]]
                    n <- sum(spray == sprayTypes[i])
                    points(jitter(rep(i, n), amount = .1), y)
                    lines(i + c(.12, .28), rep(mean(y), 2), lwd = 3)
                    lines(rep(i + .2, 2),
                               mean(y) + c(-1.96, 1.96) * sd(y) / sqrt(n)
                              )
                 }
                                                                                              15/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                15/33
 
 8/27/13                                                                              Plotting
                                                                                              16/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                16/33
 
 8/27/13                                                                              Plotting
              Boxplots
                 · Boxplots are useful for the same sort of display as the dot chart, but in instances where displaying
                    the whole data set is not possible
                 · Centerline of the boxes represents the median while the box edges correspond to the quartiles
                 · Whiskers extend out to a constant times the IQR or the max value
                 · Sometimes potential outliers are denoted by points beyond the whiskers
                 · Also invented by Tukey
                 · Skewness indicated by centerline being near one of the box edges
                                                                                                                   17/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                     17/33
 
 8/27/13                                                                              Plotting
                                                                                              18/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                18/33
 
 8/27/13                                                                              Plotting
              Boxplots discussion
                 · Don't use boxplots for small numbers of observations, just plot the data!
                 · Try logging if some of the boxes are too squished relative to other ones; you can convert the axis
                    to unlogged units (though they will not be equally spaced anymore)
                 · For data with lots and lots of observations omit the outliers plotting if you get so many of them that
                    you can't see the points
                 · Example of a bad box plot
                 boxplot(rt(500, 2))
                                                                                                                    19/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                      19/33
 
 8/27/13                                                                              Plotting
                                                                                              20/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                20/33
 
 8/27/13                                                                              Plotting
              Kernel density estimates
                 · Kernel density estimates are essentially more modern versions of histograms providing density
                    estimates for continuous data
                 · Observations are weighted according to a ""kernel"", in most cases a Gaussian density
                 · ""Bandwidth"" of the kernel effectively plays the role of the bin size for the histogram
                    a. Too low of a bandwidth yields a too variable (jagged) measure of the density b. Too high of a
                    bandwidth oversmooths
                 · The R function densitycan be used to create KDEs
                                                                                                                21/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                  21/33
 
 8/27/13                                                                              Plotting
              Example
              Data is the waiting and eruption times in minutes between eruptions of the Old Faithful Geyser in
              Yellowstone National park
                 data(faithful)
                 d <- density(faithful$eruptions, bw = ""sj"")
                 plot(d)
                                                                                                          22/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                            22/33
 
 8/27/13                                                                              Plotting
                                                                                              23/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                23/33
 
 8/27/13                                                                              Plotting
              Imaging example
                 · Consider the following image slice (created in R) from a high resolution MRI of a brain
                 · This is a single (axial) slice of a three-dimensional image
                 · Consider discarding the location information and plotting a KDE of the intensities
                                                                                                           24/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                             24/33
 
 8/27/13                                                                              Plotting
                                                                                              25/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                25/33
 
 8/27/13                                                                              Plotting
                                                                                              26/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                26/33
 
 8/27/13                                                                              Plotting
              QQ-plots
                 · QQ-plots (for quantile-quantile) are extremely useful for comparing data to a theoretical distribution
                 · Plot the empirical quantiles against theoretical quantiles
                 · Most useful for diagnosing normality
                                                                                                                     27/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                       27/33
 
 8/27/13                                                                                           Plotting
                 · Let x be the p
                             p
                                              th
                                                   quantile from a N(+ , 2              2
                                                                                          )
                 · Then P(X            ½   xp) = p
                                                  J+
                 · Clearly P(Z            ½   xp
                                                 2    ) = p
                 · Therefore x           p  =   + + z 2 (this should not be news)
                                                          p
                 · Result, quantiles from a N(+ , 2                           2
                                                                                )  population should be linearly related to standard normal quantiles
                 · A normal qq-plot plots the empirical quantiles against the theoretical standard normal quantiles
                 · In R qqnormfor a normal QQ-plot and qqplotfor a qqplot against an arbitrary distribution
                                                                                                                                                28/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                                                                  28/33
 
 8/27/13                                                                              Plotting
                                                                                              29/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                29/33
 
 8/27/13                                                                              Plotting
                                                                                              30/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                30/33
 
 8/27/13                                                                              Plotting
                                                                                              31/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                31/33
 
 8/27/13                                                                              Plotting
              Mosaic plots
                 · Mosaic plots are useful for displaying contingency table data
                 · Consider Fisher's data regarding hair and eye color data for people from Caithness
                 library(MASS)
                 data(caith)
                 caith
                 mosaicplot(caith, color = topo.colors(4),
                                     main = ""Mosiac plot"")
                              fair red medium dark black
                 blue          326 38              241 110                     3
                 light 688 116                     584 188                     4
                 medium 343 84                     909 412                   26
                 dark             98 48            403 681                   85
                                                                                                      32/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                        32/33
 
 8/27/13                                                                              Plotting
                                                                                              33/33
file://localhost/Users/sean/Developer/GitHub/modules/brian/11. Plotting/index.html#1                33/33
"
"./06_StatisticalInference/old/12. Bootstrapping/Bootstrapping.pdf","8/27/13                                                                      Bootstrapping
                          Bootstrapping
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1               1/20
 
 8/27/13                                                                      Bootstrapping
              Table of contents
                   1. The jackknife
                   2. The bootstrap principle
                   3. The bootstrap
                                                                                           2/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    2/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · The jackknife is a tool for estimating standard errors and the bias of estimators
                 · As its name suggests, the jackknife is a small, handy tool; in contrast to the bootstrap, which is
                    then the moral equivalent of a giant workshop full of tools
                 · Both the jackknife and the bootstrap involve resampling data; that is, repeatedly creating new data
                    sets from the original data
                                                                                                                   3/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                            3/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · The jackknife deletes each observation and calculates an estimate based on the remaining        n J 1
                    of them
                 · It uses this collection of estimates to do things like estimate the bias and the standard error
                 · Note that estimating the bias and having a standard error are not needed for things like sample
                    means, which we know are unbiased estimates of population means and what their standard
                    errors are
                                                                                                                     4/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                              4/20
 
 8/27/13                                                                      Bootstrapping
              The jackknife
                 · We'll consider the jackknife for univariate data
                 · Let X , Q , X be a collection of data used to estimate a parameter '
                              1            n
                 · Let 'ė be the estimate based on the full data set
                 · Let 'ė be the estimate of ' obtained by deleting observation i
                              i
                 · Let 'Ě       =
                                    1
                                    n
                                       I   n
                                           i=1
                                               'ė i
                                                                                           5/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    5/20
 
 8/27/13                                                                                  Bootstrapping
              Continued
                 · Then, the jackknife estimate of the bias is
                                                                                 (n J  1)
                                                                                          (
                                                                                             'Ě  J 'ė  )
                    (how far the average delete-one estimate is from the actual estimate)
                 · The jackknife estimate of the standard error is
                                                                               n J 1
                                                                                       n
                                                                                          ( 'ė i J 'Ě  )
                                                                                                         2
                                                                                                             1/2
                                                                                 n   ∑
                                                                             [                             ]
                                                                                      i=1
                    (the deviance of the delete-one estimates from the average delete-one estimate)
                                                                                                                 6/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          6/20
 
 8/27/13                                                                                  Bootstrapping
              Example
                 · Consider the data set of                            630   measurements of gray matter volume for workers from a lead
                    manufacturing plant
                 · The median gray matter volume is around 589 cubic centimeters
                 · We want to estimate the bias and standard error of the median
                                                                                                                                    7/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                             7/20
 
 8/27/13                                                                      Bootstrapping
              Example
              The gist of the code
                 n <- length(gmVol)
                 theta <- median(gmVol)
                 jk <- sapply(1 : n,
                                         function(i) median(gmVol[-i])
                                         )
                 thetaBar <- mean(jk)
                 biasEst <- (n - 1) * (thetaBar - theta)
                 seEst <- sqrt((n - 1) * mean((jk - thetaBar)^2))
                                                                                           8/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    8/20
 
 8/27/13                                                                      Bootstrapping
              Example
              Or, using the bootstrappackage
                 library(bootstrap)
                 out <- jackknife(gmVol, median)
                 out$jack.se
                 out$jack.bias
                                                                                           9/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                    9/20
 
 8/27/13                                                                      Bootstrapping
              Example
                 · Both methods (of course) yield an estimated bias of 0 and a se of 9.94
                 · Odd little fact: the jackknife estimate of the bias for the median is always  0 when the number of
                    observations is even
                 · It has been shown that the jackknife is a linear approximation to the bootstrap
                 · Generally do not use the jackknife for sample quantiles like the median; as it has been shown to
                    have some poor properties
                                                                                                                10/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          10/20
 
 8/27/13                                                                                 Bootstrapping
              Pseudo observations
                 · Another interesting way to think about the jackknife uses pseudo observations
                 · Let
                                                                             Pseudo Obs = n   'ė J J (n 1)'ė i
                 · Think of these as ``whatever observation i contributes to the estimate of ' ''
                 · Note when 'ė is the sample mean, the pseudo observations are the data themselves
                 · Then the sample standard error of these observations is the previous jackknife estimated standard
                    error.
                 · The mean of these observations is a bias-corrected estimate of '
                                                                                                                11/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          11/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap
                 · The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating
                    standard errors for difficult statistics
                 · For example, how would one derive a confidence interval for the median?
                 · The bootstrap procedure follows from the so called bootstrap principle
                                                                                                                12/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                          12/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap principle
                 · Suppose that I have a statistic that estimates some population parameter, but I don't know its
                    sampling distribution
                 · The bootstrap principle suggests using the distribution defined by the data to approximate its
                    sampling distribution
                                                                                                            13/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                      13/20
 
 8/27/13                                                                      Bootstrapping
              The bootstrap in practice
                 · In practice, the bootstrap principle is always carried out using simulation
                 · We will cover only a few aspects of bootstrap resampling
                 · The general procedure follows by first simulating complete data sets from the observed data with
                    replacement
                         - This is approximately drawing from the sampling distribution of that statistic, at least as far as
                             the data is able to approximate the true population distribution
                 · Calculate the statistic for each simulated data set
                 · Use the simulated statistics to either define a confidence interval or take the standard deviation to
                    calculate a standard error
                                                                                                                         14/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                   14/20
 
 8/27/13                                                                                  Bootstrapping
              Example
                 · Consider again, the data set of                           630 measurements of gray matter volume for workers from a lead
                    manufacturing plant
                 · The median gray matter volume is around 589 cubic centimeters
                 · We want a confidence interval for the median of these measurements
                                                                                                                                       15/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                                 15/20
 
 8/27/13                                                                                        Bootstrapping
                 · Bootstrap procedure for calculating confidence interval for the median from a data set of                        n
                    observations
                    i. Sample n observations with replacement from the observed data resulting in one simulated
                    complete data set
                    ii. Take the median of the simulated data set
                    iii. Repeat these two steps B times, resulting in B simulated medians
                    iv. These medians are approximately drawn from the sampling distribution of the median of                       n
                    observations; therefore we can
                         - Draw a histogram of them
                         - Calculate their standard deviation to estimate the standard error of the median
                                                  th                     th
                         - Take the 2.5                and 97.5              percentiles as a confidence interval for the median
                                                                                                                                 16/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                                           16/20
 
 8/27/13                                                                              Bootstrapping
              Example code
                 B <- 1000
                 n <- length(gmVol)
                 resamples <- matrix(sample(gmVol,
                                                                    n * B,
                                                                    replace = TRUE),
                                                      B, n)
                 medians <- apply(resamples, 1, median)
                 sd(medians)
                 [1] 3.148706
                 quantile(medians, c(.025, .975))
                         2.5%          97.5%
                 582.6384 595.3553
                                                                                                   17/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                             17/20
 
 8/27/13                                                                      Bootstrapping
                                                                                           18/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                     18/20
 
 8/27/13                                                                      Bootstrapping
              Notes on the bootstrap
                 · The bootstrap is non-parametric
                 · However, the theoretical arguments proving the validity of the bootstrap rely on large samples
                 · Better percentile bootstrap confidence intervals correct for bias
                 · There are lots of variations on bootstrap procedures; the book ""An Introduction to the Bootstrap""""
                    by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information
                                                                                                                   19/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                                             19/20
 
 8/27/13                                                                          Bootstrapping
                 library(boot)
                 stat <- function(x, i) {median(x[i])}
                 boot.out <- boot(data = gmVol,
                                               statistic = stat,
                                               R = 1000)
                 boot.ci(boot.out)
                 Level             Percentile                                BCa
                 95% (583.1, 595.2 ) (583.2, 595.3 )
                                                                                               20/20
file://localhost/Users/sean/Developer/GitHub/modules/brian/12. /index.html#1                         20/20
"
"./06_StatisticalInference/old/13. Binomial Proportions/Binomial Proportions.pdf","8/27/13                                                                                          Binomial Proportions
                          Binomial Proportions
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                      1/27
 
 8/27/13                                                                                          Binomial Proportions
              Table of contents
                   1. Intervals for binomial proportions
                   2. Agresti- Coull interval
                   3. Bayesian analysis
                          · Prior specification
                          · Posterior
                          · Credible intervals
                   4. Summary
                                                                                                                      2/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                           2/27
 
 8/27/13                                                                                              Binomial Proportions
              Intervals for binomial parameters
                 · When X           t    Binomial(n, p)             we know that
                    a. pė    = X/n       is the MLE for p b. E[pė ]                   = p     c. V ar(pė )  = p(1       J  p)/n d.
                                                                                                       ė Jp
                                                                                                       p
                                                                                                 R_pė_(1__J__p_ė _)/n__
                    follows a normal distribution for large n
                 · The latter fact leads to the Wald interval for p
                                                                                           ė
                                                                                           p ± Z1  J  /2 R_ė___J___ė ___
                                                                                                            p (1        p )/n
                                                                                                                                   3/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                        3/27
 
 8/27/13                                                                                             Binomial Proportions
              Some discussion
                 · The Wald interval performs terribly
                 · Coverage probability varies wildly, sometimes being quite low for certain values of                                     n even when    p
                    is not near the boundaries
                         - Example, when p                   = .5     and n        = 40       the actual coverage of a 95% interval is only 92%
                 · When p is small or large, coverage can be quite poor even for extremely large values of n
                         - Example, when                  p = .005           and      n = 1, 876      the actual coverage rate of a  95%  interval is only
                             90%
                                                                                                                                                       4/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                            4/27
 
 8/27/13                                                                                             Binomial Proportions
              Simple fix
                 · A simple fix for the problem is to add two successes and two failures
                 · That is let pĭ         = (X + 2)/(n + 4)
                 · The (Agresti- Coull) interval is
                                                                                           ĭ
                                                                                           p ± Z1 J  /2 R_ĭ___J___ĭ __ĭ_
                                                                                                           p (1        p )/n
                 · Motivation: when p is large or small, the distribution of                                          ė
                                                                                                                      p  is skewed and it does not make sense to
                    center the interval at the MLE; adding the pseudo observations pulls the center of the interval
                    toward .5
                 · Later we will show that this interval is the inversion of a hypothesis testing technique
                                                                                                                                                            5/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                                 5/27
 
 8/27/13                                                                                          Binomial Proportions
              Example
              Suppose that in a random sample of an at-risk population                                                13 of 20 subjects had hypertension.
              Estimate the prevalence of hypertension in this population.
                 ·   ė
                    p = .65       ,n    = 20
                 ·   ĭ
                    p = .63       , nĭ  = 24
                 ·  Z.975 = 1.96
                 · Wald interval [.44, .86]
                 · Agresti-Coull interval [.44, .82]
                 ·  1/8     likelihood interval [.42, .84]
                                                                                                                                                      6/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                           6/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      7/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                           7/27
 
 8/27/13                                                                                            Binomial Proportions
              Bayesian analysis
                 · Bayesian statistics posits a prior on the parameter of interest
                 · All inferences are then performed on the distribution of the parameter given the data, called the
                    posterior
                 · In general,
                                                                                   Posterior     U Likelihood × Prior
                 · Therefore (as we saw in diagnostic testing) the likelihood is the factor by which our prior beliefs are
                    updated to produce conclusions in the light of the data
                                                                                                                        8/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                             8/27
 
 8/27/13                                                                                                 Binomial Proportions
              Beta priors
                 · The beta distribution is the default prior for parameters between                                                0 and 1. \item The beta density
                    depends on two parameters and !
                                                                         t
                                                                          t (
                                                                            (    )
                                                                                  +
                                                                                   t
                                                                                       !)
                                                                                      (! )
                                                                                            p
                                                                                                 J 1
                                                                                                     (1 J   p)
                                                                                                               ! J  1
                                                                                                                           for 0 ½ ½
                                                                                                                                  p   1
                 · The mean of the beta density is                                 /(   +    !)
                 · The variance of the beta density is \
                                                                                                              !
                                                                                             (     +  !) 2
                                                                                                           (    +   !  + 1)
                 · The uniform density is the special case where                                             =   !   = 1
                                                                                                                                                               9/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                                    9/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      10/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            10/27
 
 8/27/13                                                                                               Binomial Proportions
              Posterior
                 · Suppose that we chose values of and ! so that the beta prior is indicative of our degree of belief
                    regarding p in the absence of data \item Then using the rule that
                                                                                   Posterior       U Likelihood × Prior
                    and throwing out anything that doesn't depend on p , we have that
                                                                         Posterior         U     x
                                                                                               p (1  J   p)
                                                                                                            n  J x
                                                                                                                   × p
                                                                                                                            J 1
                                                                                                                                (1 J p)
                                                                                                                                        ! J 1
                                                                                           = p
                                                                                                 x+ J 1
                                                                                                        (1  J    p)
                                                                                                                    n  J !J
                                                                                                                         x+     1
                 · This density is just another beta density with parameters                                               ĭ  = x +       and !ĭ = n J x + !
                                                                                                                                                             11/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                                   11/27
 
 8/27/13                                                                                               Binomial Proportions
              Posterior mean
                                                                                         ĭ
                                                                                      ĭ + !ĭ
                                                               E[p | X] =
                                                                                                 x +
                                                                               =
                                                                                     x +       + n   J x +    !
                                                                                         x +
                                                                               =
                                                                                     n +        +  !
                                                                                     x              n                              + !
                                                                               =         ×                   +               ×
                                                                                     n        n +     +  !             +   !   n +   + !
                                                                               = MLE ×            / + Prior Mean × (1          J/  )
                                                                                                                                         12/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                               12/27
 
 8/27/13                                                                                          Binomial Proportions
                 · The posterior mean is a mixture of the MLE (pė ) and the prior mean
                 · / goes to 1 as n gets large; for large n the data swamps the prior
                 · For small n , the prior mean dominates
                 · Generalizes how science should ideally work; as data becomes increasingly available, prior beliefs
                    should matter less and less
                 · With a prior that is degenerate at a value, no amount of data can overcome the prior
                                                                                                                      13/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            13/27
 
 8/27/13                                                                                               Binomial Proportions
              Posterior variance
                 · The posterior variance is
                                                                                 ĭ !ĭ
                                                       V ar(p | x) =                   ( ĭ + !ĭ ) ( ĭ + !ĭ + 1)
                                                                                                  2
                                                                           =
                                                                                (x +        )(n  J  x +   !)
                                                                                                                 (n +       + !) 2
                                                                                                                                   (n + + ! + 1)
                 · Let pĭ      = (x +        )/(n +         +    ! ) and nĭ         = n +        +   ! then we have
                                                                                                                 ĭ
                                                                                                                p (1    Jĭ p)
                                                                                            V ar(p | x) =
                                                                                                                   ĭ
                                                                                                                   n + 1
                                                                                                                                                 14/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                       14/27
 
 8/27/13                                                                                               Binomial Proportions
              Discussion
                 · If       =   !  = 2     then the posterior mean is
                                                                                              ĭ
                                                                                              p = (x + 2)/(n + 4)
                    and the posterior variance is
                                                                                                 ĭ
                                                                                                 p (1 Jĭ        ĭ
                                                                                                         p )/( n + 1)
                 · This is almost exactly the mean and variance we used for the Agresti-Coull interval
                                                                                                                           15/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                 15/27
 
 8/27/13                                                                                                Binomial Proportions
              Example
                 · Consider the previous example where x                                       = 13     and n      = 20
                 · Consider a uniform prior,                          =    !   = 1
                 · The posterior is proportional to (see formula above)
                                                                             p
                                                                                x+     J 1
                                                                                           (1  J p)
                                                                                                     n J !J
                                                                                                        x+     1
                                                                                                                   = p (1
                                                                                                                          x
                                                                                                                               J p)
                                                                                                                                    n J x
                    that is, for the uniform prior, the posterior is the likelihood
                 · Consider the instance where                                    =     !  = 2     (recall this prior is humped around the point .5 ) the
                    posterior is
                                                                          p
                                                                            x+    J  1
                                                                                       (1   J  p)
                                                                                                  n J !J
                                                                                                     x+    1
                                                                                                               = p
                                                                                                                      x+1
                                                                                                                            (1 J p)
                                                                                                                                    n J x+1
                 · The ``Jeffrey's prior'' which has some theoretical benefits puts                                                 =    !  = .5
                                                                                                                                                     16/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                                           16/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      17/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            17/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      18/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            18/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      19/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            19/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      20/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            20/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      21/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            21/27
 
 8/27/13                                                                                             Binomial Proportions
              Bayesian credible intervals
                 · A Bayesian credible interval is the Bayesian analog of a confidence interval
                 · A 95% credible interval, [a, b] would satisfy
                                                                                            P(p  @ [a, b] | x) = .95
                 · The best credible intervals chop off the posterior with a horizontal line in the same way we did for
                    likelihoods
                 · These are called highest posterior density (HPD) intervals
                                                                                                                         22/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                               22/27
 
 8/27/13                                                                                          Binomial Proportions
                                                                                                                      23/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            23/27
 
 8/27/13                                                                                          Binomial Proportions
              R code
              Install the binompackage, then the command
                 library(binom)
                 binom.bayes(13, 20, type = ""highest"")
              gives the HPD interval. The default credible level is 95% and the default prior is the Jeffrey's prior.
                                                                                                                      24/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            24/27
 
 8/27/13                                                                                          Binomial Proportions
              Interpretation of confidence intervals
                 · Confidence interval: (Wald) [.44, .86]
                 · Fuzzy interpretation:
                    We are 95% confident that p lies between .44 to .86
                 · Actual interpretation:
                    The interval .44 to .86 was constructed such that in repeated independent experiments,            95% of the
                    intervals obtained would contain p .
                 · Yikes!
                                                                                                                           25/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                                 25/27
 
 8/27/13                                                                                          Binomial Proportions
              Likelihood intervals
                 · Recall the 1/8 likelihood interval was [.42, .84]
                 · Fuzzy interpretation:
                    The interval [.42, .84] represents plausible values for p.
                 · Actual interpretation
                    The interval [.42, .84] represents plausible values for p in the sense that for each point in this
                    interval, there is no other point that is more than 8 times better supported given the data.
                 · Yikes!
                                                                                                                      26/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            26/27
 
 8/27/13                                                                                          Binomial Proportions
              Credible intervals
                 · Recall that Jeffrey's prior 95% credible interval was [.44, .84]
                 · Actual interpretation
                    The probability that p is between .44 and .84 is 95%.
                                                                                                                      27/27
file://localhost/Users/sean/Developer/GitHub/modules/brian/13. Binomial Proportions/index.html#1                            27/27
"
"./06_StatisticalInference/old/14. Logs/Logs.pdf","8/27/13                                                                          Logs
                          Logs
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1      1/16
 
 8/27/13                                                                          Logs
              Table of contents
                   1. Logs
                   2. The geometric mean
                   3. GM and the CLT
                   4. Comparisons
                   5. The log-normal distribution
                                                                                      2/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1           2/16
 
 8/27/13                                                                                         Logs
              Logs
                                                                                         y
                 · Recall that log            B
                                                (x)    is the number y so that B            = x
                 · Note that you can not take the log of a negative number; log                           B
                                                                                                            (1) is always 0 and log B
                                                                                                                                      (0)  is JV
                 · When the base is B                    = e     we write log as just log or ln
                                                                                 e
                 · Other useful bases are 10 (orders of magnitude) or 2
                 · Recall that            log(ab) = log(a) + log(b)                ,       b
                                                                                     log(a ) = b log(a) , log(a/b) = log(a)  J log(b)     (log turns
                    multiplication into addition, division into subtraction, powers into multiplication)
                                                                                                                                                 3/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                                      3/16
 
 8/27/13                                                                          Logs
              Some reasons for ""logging"" data
                 · To correct for right skewness
                 · When considering ratios
                 · In settings where errors are feasibly multiplicative, such as when dealing with concentrations or
                    rates
                 · To consider orders of magnitude (using log base 10); for example when considering astronomical
                    distances
                 · Counts are often logged (though note the problem with zero counts)
                                                                                                                 4/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                      4/16
 
 8/27/13                                                                                 Logs
              The geometric mean
                 · The (sample) {\bf geometric mean} of a data set X , Q , X is         1        n
                                                                                             1/n
                                                                                    n
                                                                                       Xi
                                                                                   ∏
                                                                                 (        )
                                                                                   i=1
                 · Note that (provided that the X are positive) the log of the geometric mean is
                                                                          i
                                                                                    n
                                                                                 1
                                                                                       log(Xi )
                                                                                 n ∑
                                                                                   i=1
                 · As the log of the geometric mean is an average, the LLN and clt apply (under what assumptions?)
                 · The geometric mean is always less than or equal to the sample (arithmetic) mean
                                                                                                                5/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                     5/16
 
 8/27/13                                                                                         Logs
              The geometric mean
                 · The geometric mean is often used when the X are all multiplicative       i
                 · Suppose that in a population of interest, the prevalence of a disease rose                             2  one year, then fell 1
                    the next, then rose 2 , then rose                            1 ; since these factors act multiplicatively it makes sense to
                    consider the geometric mean
                                                                                                        1/4
                                                                             (1.02 × .99 × 1.02 × 1.01)     = 1.01
                    for a 1 geometric mean increase in disease prevalence
                                                                                                                                              6/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                                   6/16
 
 8/27/13                                                                          Logs
                                                                                    4
                 · Notice that multiplying the initial prevalence by 1.01 is the same as multiplying by the original four
                    numbers in sequence
                 · Hence 1.01 is constant factor by which you would need to multiply the initial prevalence each year
                    to achieve the same overall increase in prevalence over a four year period
                 · The arithmetic mean, in contrast, is the constant factor by which your would need to add each year
                    to achieve the same total increase (1.02 + .99 + 1.02 + 1.01 )
                 · In this case the product and hence the geometric mean make more sense than the arithmetic
                    mean
                                                                                                                      7/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                           7/16
 
 8/27/13                                                                                             Logs
              Nifty fact
                 · The question corner (google) at the University of Toronto's web site (where I got much of this) has
                    a fun interpretation of the geometric mean
                 · If a and b are the lengths of the sides of a rectangle then
                         - The arithmetic mean                        (a + b)/2       is the length of the sides of the square that has the same
                             perimeter
                                                                           1/2
                         - The geometric mean (ab)                               is the length of the sides of the square that has the same area
                 · So if you're interested in perimeters (adding) use the arithmetic mean; if you're interested in areas
                    (multiplying) use the geometric mean
                                                                                                                                                 8/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                                      8/16
 
 8/27/13                                                                                          Logs
              Asymptotics
                 · Note, by the LLN the log of the geometric mean converges to +                               = E[log(X)]
                                                                                                                  +
                 · Therefore the geometric mean converges to                                   exp{E[log(X)]} = e   , which is not the population
                    mean on the natural scale; we call this the population geometric mean (but no one else seems to)
                 · To reiterate
                                                                        exp{E[log(x)]}     ¹  E[exp{log(X)}] = E[X]
                 · Note if the distribution of log(X) is symmetric then
                                                                                 .5 = P(log X  ½+ ) = P(X ½  +
                                                                                                            e )
                 · Therefore, for log-symmetric distributions the geometric mean is estimating the median
                                                                                                                                              9/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                                   9/16
 
 8/27/13                                                                          Logs
              GM and the CLT
                 · If you use the CLT to create a confidence interval for the log measurements, your interval is
                    estimating + , the expected value of the log measurements
                                                                                               +
                 · If you exponentiate the endpoints of the interval, you are estimating     e   , the population geometric
                    mean
                 · Recall, e + is the population median when the distribution of the logged data is symmetric
                 · This is especially useful for paired data when their ratio, rather than their difference, is of interest
                                                                                                                         10/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                               10/16
 
 8/27/13                                                                          Logs
              Example
              Rosner, Fundamentals of Biostatistics page 298 gives a paired design comparing SBP for matched
              oral contraceptive users and controls.
                 · The geometric mean ratio is 1.04 (4% increase in SBP for the OC users)
                 · The T interval on the difference of the log scale measurements is [0.010, 0.067] log(mm Hg)
                 · Exponentiating yields [1.010, 1.069] mmHg .
                                                                                                               11/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                     11/16
 
 8/27/13                                                                               Logs
              Comparisons
                 · Consider when you have two independent groups, logging the individual data points and creating a
                    confidence interval for the difference in the log means
                 · Prove to yourself that exponentiating the endpoints of this interval is then an interval for the ratio of
                                                                                  + 1
                                                                                 e
                    the population geometric means, e+                              2
                                                                                                                       12/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                             12/16
 
 8/27/13                                                                                        Logs
              The log-normal distribution
                 · A random variable is log-normally distributed if its log is a normally distributed random variable
                 · ""I am log-normal"" means ""take logs of me and then I'll then be normal""
                 · Note log-normal random variables are not logs of normal random variables!!!!!! (You can't even
                    take the log of a normal random variable)
                 · Formally, X is lognormal(+ , 2 2 ) if log(X)                         t  N( +, 2  2
                                                                                                      )
                 · If Y      t   N(   +, 2    2
                                                )  then X        = e
                                                                         Y
                                                                             is log-normal
                                                                                                                    13/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                          13/16
 
 8/27/13                                                                                                Logs
              The log-normal distribution
                 · The log-normal density is
                                                                  1
                                                                         ×
                                                                              exp[ J {log(x)      J+   2
                                                                                                      } /(2 2 2
                                                                                                                )]
                                                                                                                   for 0 ½ ½V
                                                                                                                          x
                                                               √2 ___/                           x
                 · Its mean is e +          +( 2 2
                                                   /2)
                                                        and variance is e           2 + +2 2
                                                                                             (e
                                                                                                2 2
                                                                                                    J 1)
                 · Its median is e +
                                                                                                                              14/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                    14/16
 
 8/27/13                                                                              Logs
              The log-normal distribution
                 · Notice that if we assume that X , Q , X are log-normal(+ , 2  1 n
                                                                                            2
                                                                                              ) then Y1 = log X1 , Q, Y n = log X n
                    are normally distributed with mean + and variance 2                 2
                 · Creating a Gosset's t confidence interval on using the Y is a confidence interval for + the log of the
                                                                                          i
                    median of the X             i
                 · Exponentiate the endpoints of the interval to obtain a confidence interval for e + , the median on the
                    original scale
                 · Assuming log-normality, exponentiating t confidence intervals for the difference in two log means
                    again estimates ratios of geometric means
                                                                                                                              15/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                                    15/16
 
 8/27/13                                                                          Logs
              Example
                 · Took GM volumes for the young and old groups, logged them
                 · Did two independent group intervals, got old [13.24, 13.27] log(cubic cm) and young [13.29, 13.31]
                    log(cubic cm).
                 · Exponentiating yields [564.4, 577.5] cc, [592.0, 606.9] cc.
                 · Doing a two group T interval on the logged measurements yields [0.032, 0.066] log(cubic cm)
                 · exponentiating this interval yields [1.032, 1.068]
                                                                                                                 16/16
file://localhost/Users/sean/Developer/GitHub/modules/brian/14. Logs/index.html#1                                       16/16
"
"./06_StatisticalInference/Random Formulae/Random Formulae.pdf","8/24/13                                                                                 Random Formulae
                          Random Formulae
                          Mathematical Biostatistics Boot Camp
                          Brian Caffo, PhD
                          Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                 1/13
 
 8/24/13                                                                                 Random Formulae
              About this document
              This document contains random formulae images I used in the notes.
                                                                                                        2/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                      2/13
 
 8/24/13                                                                                     Random Formulae
                                                                                         A = {1, 2}
                                                                                        B = {1, 2, 3}
                                                                                                            3/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                          3/13
 
 8/24/13                                                                                           Random Formulae
                                                                                                        1
                                                                                             2               2
                                                                                        E[ X   ] =         x   dx
                                                                                                   ∫
                                                                                                      0
                                                                                                    x
                                                                                                      3
                                                                                                        [[1
                                                                                                                 1
                                                                                                 =
                                                                                                     3   [0
                                                                                                            =
                                                                                                                 3
                                                                                                                   4/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                 4/13
 
 8/24/13                                                                                            Random Formulae
                                                                                             |x J+    |
                                                                                                         > 1
                                                                                                k 2
              Over the set {x              : |x J+     | > k   2}
                                                                                            (x J+    )
                                                                                                       2
                                                                                                          > 1
                                                                                              k
                                                                                                2
                                                                                                  2 2
                                                                                                V
                                                                                   k
                                                                                      2
                                                                                       1
                                                                                        2 2 ∫
                                                                                              JV
                                                                                                   (x  J+    2
                                                                                                           ) f (x)dx
                                                                           k
                                                                              1
                                                                              2
                                                                                2 2
                                                                                     E[(X    J+    2
                                                                                                  ) ] =
                                                                                                           k
                                                                                                             2
                                                                                                               1
                                                                                                                2 2
                                                                                                                    V ar(X)
                                                                                                                            5/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                          5/13
 
 8/24/13                                                                                          Random Formulae
                                                    P(A1      b b
                                                                A2       A3 ) = P{A1       b  ( A2 b  A3 )} = P(A1 ) + P(A2  b A3 )
                                                                 P(A1 ) + P( A2         b A3 ) = P( A1 ) + P( A2 ) + P( A3 )
                                                                                                                                    6/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                                  6/13
 
 8/24/13                                                                                         Random Formulae
                                                                             P(  b  n
                                                                                    i=1
                                                                                        Ei ) = P{E n  b bJ(
                                                                                                              n
                                                                                                              i=1
                                                                                                                 1
                                                                                                                   E i )}
                                                                                                                          7/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                        7/13
 
 8/24/13                                                                                               Random Formulae
                                                                                 (x 1 , x 2 , x 3 , x 4 ) = (1, 0, 1, 1)
                                                                   p
                                                                     (1+0+1+1)
                                                                                    (1   J  p)
                                                                                                {4 J (1+0+1+1)}          3
                                                                                                                     = p (1 J  p)
                                                                                                                                  1
                                                                                             SD(X)SD(Y )
                                                                                                  V ar(X)
                                               V ar(X) = E[ X
                                                                         2
                                                                           ] J   E[X ]
                                                                                          2
                                                                                            ó    E[X
                                                                                                         2
                                                                                                           ] = V ar(X) + E[X ]
                                                                                                                                  2
                                                                                                                                    =  2 2
                                                                                                                                           + + 2
                                                        Ě
                                              V ar(X ) = E[ X ]
                                                                     Ě 2
                                                                           J        Ě
                                                                               E[ X ]
                                                                                        2
                                                                                           ó        Ě 2
                                                                                                                       Ě
                                                                                               E[X ] = V ar( X ) + E[X ]
                                                                                                                             Ě  2
                                                                                                                                   = 2 2
                                                                                                                                         /n + + 2
                                                                                                            f xy (x, 5)
                                                                                      f (x|y = 5) =
                                                                                                               f y (5)
                                                                                                                                                  8/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                                                8/13
 
 8/24/13                                                                                    Random Formulae
                                                                                        P(A  a   B)
                                                                                          P(A)
                                                                                        P(A a     c
                                                                                                B )
                                                                                                           9/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                         9/13
 
 8/24/13                                                                                         Random Formulae
                                                                       10!              10 × 9 × 8 ×      Q× 1
                                                                                  =                                = 10
                                                                      1!9!                 9 × 8 × Q× 1
                                                                     10!                10 × 9 × 8 × Q × 1
                                                                               =                                     = 45
                                                                    2!8!             2 × 1 × 8 × 7 × Q× 1
              In general
                   n
                            =
                                  n×(n   J 1)
                                       2
              (2)
                                                                                                 +
                                                                                                2 2
                                                                     E[Z] = E
                                                                                          X J+      =
                                                                                                          E[X]    J+ = 0
                                                                                        [   2    ]              2
                                                                                                                          10/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                         10/13
 
 8/24/13                                                                                       Random Formulae
                                           V ar(Z) = V ar
                                                                             X   J+       =
                                                                                             1
                                                                                                 V ar(X       J+ ) =
                                                                                                                      1
                                                                                                                         V ar(X) = 1
                                                                       (         2      )   2 2
                                                                                                                     2 2
                                                                                                                                     11/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                                    11/13
 
 8/24/13                                                                                            Random Formulae
                                                                                   E[ X
                                                                                        2
                                                                                        i
                                                                                          ] = E[ Yi ] =      2 2
                                                                                                                  + + 2
                                                                                          J XĚ )                  J
                                                                                 n                      2
                                                                                     (Xi
                                                                                                 2
                                                                                                   =        X
                                                                                                               2
                                                                                                               i
                                                                                                                    nX
                                                                                                                       Ě 2
                                                                               ∑                      ∑
                                                                                i=1                    i=1
                                                                                                                           12/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                          12/13
 
 8/24/13                                                                                           Random Formulae
                                                                                            E[ 6 2
                                                                                                 df
                                                                                                    ] = df
                                                                     E[S
                                                                            2
                                                                              ] =    2 ó2
                                                                                             E
                                                                                                 (n   J  1)S
                                                                                                              2
                                                                                                                    = (n J 1)
                                                                                               [       2 2
                                                                                                                  ]
                                                                                          V ar( 6 2
                                                                                                 df
                                                                                                    ) = 2df
                                                                                                                              13/13
file://localhost/Users/sean/Developer/GitHub/modules/brian/Random Formulae/index.html#1                                             13/13
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-1.pdf","                                                                     Standardized residuals
                                Residuals vs Fitted                                                        Normal Q−Q
                                                    Sierre ●                                                                    Sierre ●
                                                    ●                                         2
                         0 10
                                           ●               ●                                                                      ●●●
Residuals
                                                     ● ●                                                                         ●
                                              ● ● ● ●● ●      ●
                                                                                                                              ●●●
                                          ●     ●●        ● ●                                                              ●
                                                                                                                           ●●
                                                                                                                            ●●
                                                                                                                             ●
                                                                                                                         ●
                                                                                                                         ●●
                                                                                                                          ●
                                ●           ● ●● ● ●●●●●●●                                    0                      ●●
                                                                                                                      ●
                                                                                                                      ●●
                                                                                                                       ●●
                                                                                                                        ●
                                                                                                                        ●
                                                                                                                    ●
                                                                                                                    ●
                                                                                                                    ●●
                                                ●●● ●● ● ●                                                        ●●
                                                                                                                   ●
                                                 ●    ●●●                                                       ●●
                                                                                                                 ●●
                                            ●                                                                  ●
                                                                                                            ●●●●
                                                        ●
                         −15
                                                                                                          ●●
                                                      Porrentruy ●
                                              ● Rive Gauche                                   −2       ● Porrentruy
                                                                                                    ● Rive Gauche
                                    40         60          80                                        −2 −1            0     1       2
                                         Fitted values                                               Theoretical Quantiles
Standardized residuals                                               Standardized residuals
                                    Scale−Location                                                 Residuals vs Leverage
                                                   Sierre
                                              ● Rive      ●
                                                     Gauche
                                                      Porrentruy ●                                                                          1
                                                                                                              ● Sierre
                                           ●      ●                                           2
                                            ●            ● ●                                              ●
                                                                                                         Neuchatel
                                                                                                              ● ●                           0.5
                                                      ●● ●                                                 ●
                         1.0                     ● ●
                                              ●●●● ●
                                                       ● ●
                                                       ●●●
                                                       ●
                                                               ●                                           ●●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ● ●● ●●
                                                                                                                ●
                                                                                                                  ●
                                                                                                                              ●
                                          ●         ●       ●●                                0            ●
                                                                                                           ●●
                                                                                                          ●● ● ● ●
                                                                                                              ●●                        ●
                                                ●●●        ● ●                                               ●    ●
                                                                                                         ●●● ● ●
                                                ●●     ● ●                                                 ●●● ● ●
                                                                                                             ●
                                                       ●● ●
                                                   ● ●       ●                                             ●         ●
                                ●           ●         ●                                       −2              ●
                                                                                                                Cook's distance
                                                                                                                    ●
                                                                                                                                            0.5
                         0.0                                                                             Porrentruy                         1
                                    40         60          80                                      0.0            0.2             0.4
                                         Fitted values                                                         Leverage
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-10.pdf","                                                                     Standardized residuals
                                Residuals vs Fitted                                                        Normal Q−Q
                                                    Sierre ●                                                                    Sierre ●
                                                    ●                                         2
                         0 10
                                           ●               ●                                                                      ●●●
Residuals
                                                     ● ●                                                                         ●
                                              ● ● ● ●● ●      ●
                                                                                                                              ●●●
                                          ●     ●●        ● ●                                                              ●
                                                                                                                           ●●
                                                                                                                            ●●
                                                                                                                             ●
                                                                                                                         ●
                                                                                                                         ●●
                                                                                                                          ●
                                ●           ● ●● ● ●●●●●●●                                    0                      ●●
                                                                                                                      ●
                                                                                                                      ●●
                                                                                                                       ●●
                                                                                                                        ●
                                                                                                                        ●
                                                                                                                    ●
                                                                                                                    ●
                                                                                                                    ●●
                                                ●●● ●● ● ●                                                        ●●
                                                                                                                   ●
                                                 ●    ●●●                                                       ●●
                                                                                                                 ●●
                                            ●                                                                  ●
                                                                                                            ●●●●
                                                        ●
                         −15
                                                                                                          ●●
                                                      Porrentruy ●
                                              ● Rive Gauche                                   −2       ● Porrentruy
                                                                                                    ● Rive Gauche
                                    40         60          80                                        −2 −1            0     1       2
                                         Fitted values                                               Theoretical Quantiles
Standardized residuals                                               Standardized residuals
                                    Scale−Location                                                 Residuals vs Leverage
                                                   Sierre
                                              ● Rive      ●
                                                     Gauche
                                                      Porrentruy ●                                                                          1
                                                                                                              ● Sierre
                                           ●      ●                                           2
                                            ●            ● ●                                              ●
                                                                                                         Neuchatel
                                                                                                              ● ●                           0.5
                                                      ●● ●                                                 ●
                         1.0                     ● ●
                                              ●●●● ●
                                                       ● ●
                                                       ●●●
                                                       ●
                                                               ●                                           ●●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ● ●● ●●
                                                                                                                ●
                                                                                                                  ●
                                                                                                                              ●
                                          ●         ●       ●●                                0            ●
                                                                                                           ●●
                                                                                                          ●● ● ● ●
                                                                                                              ●●                        ●
                                                ●●●        ● ●                                               ●    ●
                                                                                                         ●●● ● ●
                                                ●●     ● ●                                                 ●●● ● ●
                                                                                                             ●
                                                       ●● ●
                                                   ● ●       ●                                             ●         ●
                                ●           ●         ●                                       −2              ●
                                                                                                                Cook's distance
                                                                                                                    ●
                                                                                                                                            0.5
                         0.0                                                                             Porrentruy                         1
                                    40         60          80                                      0.0            0.2             0.4
                                         Fitted values                                                         Leverage
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-2.pdf","    6
                     ●                   ●
    4
    2                         ●
                              ●●●●
                           ●●●●
                          ●●
Y
                          ●● ●
                             ●
                             ●
                        ●●
                         ●●
                          ●
                          ●●
                     ●●● ●
                         ●●
                     ●●
                      ●●
                       ●
                        ●
                        ●
                        ●
                        ●●●              ●
    0
                  ● ●
                    ●
                    ●
                    ●●
                     ●
                     ●●
                 ●●●
                   ●
                   ●●
                    ●●
                     ●
                     ●
                ●●●
                  ●●
               ●●
                ●●●
    −2       ●
             ●  ●
             ●
         ●
             −2      0         2     4       6
                           X
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-3.pdf","    10                                    ●
    8
    6
y   4
    2         ●●
               ●●●●●●
          ●●●●●● ●●
                  ●
                  ●●●●●●
           ●●●
                ●
                ●●
                 ●●●
                  ●
                  ●
                  ●●●
                    ●
                    ●
                     ●●
                     ●●●● ●
    0
               ●
               ●●● ●
                   ●
                  ●● ●
            ●●●●
               ●●
                ●
                ●
                ●●●
                  ●●●
                    ●
                     ●
                     ●
                       ●
         ●●● ● ●● ●●
    −2        ●● ●
              ●       ●
         −2     0      2      4   6   8   10
                              x
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-5.pdf","                                           ●
    4
                                   ●
                                   ●
    2                       ● ●
                           ●●●
                           ●  ●
y
                        ●  ●●●
                     ●●
                     ● ●●
                        ●●
                         ●●
                    ●●●
                      ●●●
                        ●●
                         ●
                         ●
                   ●
                   ●●●
                     ●
                     ●●●●●
                      ●
                  ●●●
                    ● ●
                      ●
                    ●●
    0
               ●● ●
                  ●
                  ●●●●●
                ● ●
                  ●
             ●●
             ● ●
               ●●
                ●●●●
             ●   ●●
             ●●
            ●●
    −2    ●● ●
         ●●
          −2        0          2       4
                           x
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-7.pdf","                               −0.5 0.5                                −0.6 0.4
                                ● ●●
                                  ●●●
                                    ●●
                                    ●
                                    ●●●●
                                       ●●
                                        ●●
                                         ●●
                                          ●●
                                           ●
                                           ●
                                           ●●         ●●●●
                                                         ●
                                                         ●●
                                                         ●●
                                                          ●
                                                          ●●
                                                           ●●
                                                           ●●
                                                           ●●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●●
                                                               ●●
                                                                ●●
                                                                 ●●      ●●
                                                                         ●●●
                                                                           ●●
                                                                            ●●
                                                                             ●●
                                                                             ●●
                                                                             ●●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●●
                                                                                   ●●● ●         ●●●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                       ●●
                                                                                                        ●●●
                                                                                                          ●
                                                                                                          ●●
                                                                                                           ●●     2
                                ●
                                ●● ●●
                                    ●
                                    ●●
                                     ●●
                                      ●
                                      ●
                                      ●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●
                                          ●●
                                           ●●
                                            ● ●
                                              ●        ●●●●
                                                        ●●
                                                         ●
                                                         ●●
                                                         ●●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●●
                                                               ●●
                                                                ●  ●
                                                                   ●     ●● ●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●● ●       ●● ●●
                                                                                                 ●●●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●●
                                                                                                          ●
                                ●●●
                                  ●●
                                   ●
                                   ●
                                   ●●
                                    ●●
                                     ●●
                                      ●●
                                       ●
                                      ●●
                                      ●●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                        ●●
                                       ●● ●
                                          ●
                                          ●
                                          ●●
                                           ●
                                           ●
                                           ●●
                                            ●●
                                             ●
                                             ●●●   ●●  ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                           ●
                                                           ● ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●●●
                                                                 ●
                                                                 ●      ●●
                                                                         ●●
                                                                          ●●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                              ●●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ● ●
                                                                                    ●●          ●●
                                                                                                 ●
                                                                                                 ●●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                   ● ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●●
                                                                                                          ●●
                                                                                                           ●●
                                ●
                                ●●●
                                  ●
                                  ●●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                    ●●
                                     ●
                                    ●●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                     ●●
                                      ●
                                      ●
                                       ●
                                      ●●
                                       ●
                                     ●●●
                                     ●●
                                     ●●
                                      ●
                                      ●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                      ●●●
                                       ●●
                                       ●
                                      ●●
                                      ●
                                      ●●●
                                        ●
                                       ●●
                                      ●●
                                       ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●●
                                          ●
                                          ●
                                         ●●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●
                                           ●
                                          ●●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                           ●●●●●   ●●
                                                   ●
                                                      ●
                                                      ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                          ●●
                                                         ●●
                                                          ●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                        ●●●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●●●
                                                             ●
                                                            ●●
                                                            ●
                                                            ●●
                                                            ●
                                                            ●●
                                                           ●●●
                                                            ●●
                                                            ●
                                                            ●●
                                                             ●
                                                            ●●
                                                             ●
                                                            ●●
                                                            ●●
                                                              ●
                                                              ●
                                                              ●
                                                             ●●
                                                              ●
                                                              ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●●
                                                                 ●●●
                                                                  ●     ●
                                                                        ●
                                                                        ● ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                           ●●
                                                                           ●●
                                                                            ●
                                                                           ●●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ●●
                                                                            ●
                                                                              ●
                                                                             ●●
                                                                            ●●
                                                                            ● ●
                                                                             ●●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                             ●●
                                                                             ●●
                                                                             ●●
                                                                               ●●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                             ●●
                                                                             ●  ●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                 ●●
                                                                                ●●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                 ●●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●●
                                                                                      ●●      ●●
                                                                                               ●
                                                                                               ●
                                                                                              ●●●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                ●●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                  ●●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                   ●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                    ●●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                    ●●●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                       ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                      ●●
                                                                                                      ●
                                                                                                     ●●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●●●
                                                                                                           ●
                                                                                                           ●
                                ●●●
                                  ●●
                                  ●● ●
                                    ●●●●
                                      ●
                                     ●●
                                      ●●
                                       ●●
                                      ●●
                                       ● ●
                                         ●●
                                         ●●
                                        ●●●
                                         ●
                                        ●● ●●  ●     ●   ●
                                                        ●●●
                                                         ●●
                                                          ●
                                                         ●●
                                                         ● ●
                                                           ●●
                                                           ●
                                                           ●●
                                                            ●●
                                                             ●●
                                                              ● ●
                                                                ●
                                                                ●
                                                                ●      ●●●●
                                                                          ●●
                                                                           ●●●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ● ●
                                                                             ●●
                                                                             ●●
                                                                              ●●●
                                                                               ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                              ●●●
                                                                                ●●
                                                                                 ●●
                                                                                 ●●
                                                                                  ●●●●●      ●●●●●
                                                                                                ●●●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                  ●●
                                                                                                  ●  ●
                                                                                                     ●
                                                                                                    ●●●
                                                                                                      ●
                                                                                                      ●●●
                                                                                                        ●●
                                                                                                         ●●●
                                                                                                         ●●●
                                                                                                           ●
              V1               ●●
                                 ●
                                 ●
                                 ●
                                 ●●
                                  ●
                                 ●●
                                  ●
                                  ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●●
                                    ●●
                                    ●
                                    ●
                                   ●●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                     ●●
                                     ●
                                     ●
                                    ●●
                                     ●●
                                      ●
                                      ●
                                     ●●
                                    ●●
                                     ●
                                     ●●
                                      ●
                                       ●●●
                                       ●
                                       ●
                                      ●●
                                      ●●
                                      ●●
                                     ●●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                        ●●
                                        ●●
                                        ●●
                                       ●●
                                       ●●●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●●
                                            ●
                                            ●
                                            ●
                                            ●
                                             ●
                                             ●●      ●●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                        ●
                                                       ●●
                                                        ●●
                                                       ●●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●●●
                                                          ●
                                                          ●
                                                          ●
                                                         ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                         ●●
                                                         ●●
                                                        ●●
                                                        ●●
                                                        ●●●●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                          ●
                                                         ●●●
                                                          ●●
                                                           ●
                                                            ●●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●
                                                           ●●
                                                            ●
                                                           ●●●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                            ●
                                                          ●●●●
                                                            ●●
                                                           ●●
                                                           ●●
                                                            ●
                                                            ●●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                             ●●●
                                                               ●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●
                                                                 ●
                                                                 ●
                                                                 ●●
                                                                  ●●   ●●
                                                                        ●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                          ●●●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                              ●
                                                                             ●●
                                                                              ●
                                                                             ●●
                                                                             ●
                                                                            ●●
                                                                            ●
                                                                            ●●●
                                                                              ●
                                                                             ●●
                                                                             ●●
                                                                             ●●
                                                                             ●●
                                                                             ●●
                                                                             ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                               ●●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                               ●●
                                                                              ●●●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                 ●●
                                                                                 ●●
                                                                                  ●●
                                                                                   ●
                                                                                   ●●
                                                                                    ●
                                                                                    ●
                                                                                   ●●
                                                                                    ●
                                                                                     ●
                                                                                     ●●
                                                                                      ●●
                                                                                              ●●
                                                                                               ●●●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                    ●●
                                                                                                    ●●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                     ● ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                    ●●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                      ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●●
                                                                                                           ●●
                                                                                                            ●●
                                                                                                                  −2 0
                                 ●
                                 ●
                                 ●●
                                  ●
                                  ●●
                                   ●●
                                    ●
                                    ●
                                   ●●
                                    ●●●
                                      ●
                                      ●●
                                      ●●
                                      ●●
                                     ●●●
                                      ●●
                                       ●●
                                       ●●
                                        ●
                                        ●
                                       ●●
                                       ●●
                                        ●
                                        ●●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●●
                                           ●●
                                            ●
                                            ●
                                            ●●        ●
                                                      ●●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                          ●●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                             ●●
                                                              ●
                                                              ●●●
                                                               ●●
                                                               ●●
                                                                ●●
                                                                 ●●
                                                                  ●
                                                                  ●     ●●●
                                                                         ●●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                              ●●
                                                                               ●●
                                                                               ●
                                                                               ●
                                                                               ●●●
                                                                                ●
                                                                              ●●●
                                                                              ●●●
                                                                                ●
                                                                               ●●
                                                                               ●
                                                                              ●●
                                                                              ● ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                   ●●
                                                                                    ●
                                                                                    ●
                                                                                    ●●
                                                                                     ●       ●●●●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                    ●●
                                                                                                    ●●●●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●●
                                                                                                          ●
                                   ●●
                                    ●●
                                    ●●●
                                      ●
                                      ● ●●●
                                          ●●
                                          ● ●          ●●●
                                                        ● ●●
                                                           ●
                                                          ●●
                                                           ●●
                                                            ●●
                                                             ●●
                                                              ●●●●         ●
                                                                           ●●
                                                                            ●●
                                                                             ●●● ●
                                                                                ●●
                                                                                ● ●● ●●●      ●  ●●
                                                                                                  ●●
                                                                                                  ●●
                                                                                                   ●●
                                                                                                   ●●●●●
                                                                                                       ●
                                                                                                      ●●●  ●
                                 ●
                               ●●●●
                                  ●
                                  ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●
                                      ●●
                                      ●
                                     ●●
                                     ●●
                                      ●
                                      ●
                                     ●●
                                      ●
                                     ●●
                                    ●●
                                     ●●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                       ●●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                       ●●
                                       ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                         ●●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                          ●●
                                          ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●●
                                             ●       ●●
                                                      ●●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●●
                                                         ●
                                                         ●
                                                         ●
                                                        ●●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                        ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                          ●●
                                                          ●●
                                                          ●
                                                        ●●●
                                                          ●
                                                         ●●●
                                                          ●●
                                                          ●●
                                                           ●
                                                          ●●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●
                                                                ●●●
                                                                 ●●    ●●●
                                                                        ●●●
                                                                          ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                              ●●
                                                                             ●●
                                                                              ●●
                                                                              ●●
                                                                              ●
                                                                             ●●
                                                                            ●● ●
                                                                               ●
                                                                              ●●
                                                                              ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●●
                                                                                     ● ●●    ●● ●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                       ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                       ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                           ●
                                                                                                           ●●
                                                                                                           ●
                                 ●●
                                  ●●●
                                   ●
                                   ●●●●
                                     ●●
                                      ●
                                      ●
                                     ●●●●
                                       ●
                                       ●
                                      ●●
                                      ●●
                                       ●●
                                        ●
                                       ●●
                                        ●●
                                         ●●
                                          ●●
                                           ●
                                           ●●
                                            ●
                                            ●●
                                             ●         ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●●
                                                          ●
                                                          ●●
                                                           ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                            ●●
                                                           ●●●●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●●
                                                                ●
                                                                ● ●●
                                                                  ●     ●
                                                                        ●●●●●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ●●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                             ●●●
                                                                               ●
                                                                              ●●
                                                                              ●●
                                                                               ●●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ● ●
                                                                                    ●
                                                                                    ●●●
                                                                                      ●●       ●● ●
                                                                                                  ●●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                     ●●●
                                                                                                      ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●●●
                                                                                                        ●
                                                                                                        ●●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●●
                                                                                                          ●●
                                ●
                                ●●●●
                                  ●
                                  ●
                                  ●●
                                   ●
                                   ●
                                  ●●
                                   ●●●
                                    ●
                                    ●
                                    ●
                                    ●
                                   ●●●
                                     ●
                                     ●
                                     ●
                                     ●●●●
                                       ●
                                      ●●
                                      ●
                                      ●●
                                     ●●●
                                       ●
                                       ●
                                       ●
                                      ●●
                                      ●●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                         ●●
                                         ●
                                         ●●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●●
                                            ●
                                            ●●
                                             ●
                                             ●
                                             ●●●     ●●●
                                                       ●
                                                       ●●●
                                                         ●
                                                        ●●
                                                        ●●
                                                         ●
                                                        ●●
                                                        ●
                                                        ●
                                                        ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                          ●●●
                                                            ●
                                                            ●
                                                           ●●
                                                          ●●
                                                           ●●
                                                           ●●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                         ●●
                                                          ●●●
                                                            ●
                                                            ●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                             ●●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●●
                                                                 ●
                                                                 ●●    ●●
                                                                         ●●
                                                                          ●
                                                                          ●●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●●
                                                                            ●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                            ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                             ●●
                                                                             ●
                                                                            ●●●
                                                                            ● ●
                                                                              ●
                                                                             ●●●
                                                                              ●●
                                                                              ●●
                                                                               ●●
                                                                                ●
                                                                               ●●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●●
                                                                              ●●●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                   ●
                                                                                   ●
                                                                                   ●●
                                                                                    ●
                                                                                   ●●
                                                                                     ●●
                                                                                     ●●      ●
                                                                                             ●●
                                                                                              ● ●
                                                                                                ●
                                                                                                ●
                                                                                               ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●●
                                                                                                 ●●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                       ●●
                                                                                                       ●●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●●●
                                  ●
                                  ●●●
                                    ●
                                    ●
                                    ● ●
                                     ●●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                       ●●
                                       ●●●
                                         ●
                                         ●
                                         ●●●
                                          ●
                                          ●●
                                           ●
                                            ●
                                            ●●●        ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●●
                                                            ●●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●●  ●     ●●
                                                                         ●●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●●
                                                                              ●
                                                                              ●●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●●●●       ●● ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                   ●●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                  ● ●●●●
                                                                                                       ●●
                                                                                                        ●●
            ●●● ●●  ●●●●●                              ●  ●  ●●●           ●●   ●
                                                                                ● ●
                                                                                 ●●          ●●  ●●
−0.5 0.5
             ●
             ●●    ●●●●  ●                            ● ●
                                                        ● ●
                                                         ●●●
                                                           ●●
                                                            ●  ●●            ●
                                                                             ●●
                                                                              ●●
                                                                               ●
                                                                               ●● ●
                                                                                  ●
                                                                                  ●●●          ●●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●●
                                                                                                   ●
           ●●
            ●
            ●●
             ●●
              ●
              ●
              ●●
               ●
               ●●
                ●●●
                  ●
                  ●●
                   ●
                   ●●
                    ●
                    ●●
                     ●
                     ●
                     ●●
                      ●●
                       ●
                       ●●
                        ●●
                         ●●
                          ●                          ●●●
                                                       ●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●●
                                                              ●
                                                              ●●●
                                                                ●  ●    ●
                                                                       ●● ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●●●
                                                                                  ●
                                                                                  ●●●●        ●
                                                                                              ●●
                                                                                               ●●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●●●
           ●
           ●
           ●●●
             ●
             ●
             ●●
              ●
              ●●
               ●
               ●
               ●●
                ●
                ●
                ●●
                 ●●
                  ●
                  ●
                  ●●
                   ●
                   ●●
                    ●
                    ●
                    ●
                    ●●
                     ●
                     ●●
                      ●
                      ●
                      ●●
                       ●
                       ●●
                        ●
                        ●●
                         ●
                         ●●
                          ●                          ●
                                                     ●●
                                                      ●●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●●
                                                                 ●●     ●●●
                                                                          ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●●●
                                                                                    ●●
                                                                                     ●
                                                                                     ●        ●
                                                                                              ●
                                                                                              ●
                                                                                              ●●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
           ●
           ●●
            ●
            ●
            ●
            ●
            ●●
             ●
             ●
             ●●
              ●
              ●
              ●
              ●
              ●●
               ●
               ●
               ●
               ●●
                ●●
                 ●
                 ●
                 ●
                 ●●
                  ●
                  ●●
                   ●
                   ●
                   ●
                   ●
                   ●●
                    ●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●
                     ●●
                      ●
                      ●●
                       ●
                       ●●
                        ●
                        ●●
                         ●
                         ●
                         ●●
                          ●                          ●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●
                                                               ●●
                                                                ●
                                                                ●
                                                                ●●●●    ●
                                                                        ●
                                                                        ●
                                                                        ●●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●●
                                                                                   ●
                                                                                   ●
                                                                                   ●●
                                                                                    ●●
                                                                                     ●●
                                                                                      ●
                                                                                      ●●     ●●
                                                                                              ●●●
                                                                                                ●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
             ●
             ●
             ●●
              ●
              ●●●
                ●
                ●●●
                  ●●●
                    ●
                    ●
                    ●●
                     ●
                     ●●
                      ●●
                       ●●
                        ●●●                        ●● ●
                                                      ●●●
                                                        ●
                                                        ●
                                                        ●●●
                                                          ●●
                                                           ●●●●
                                                              ●●●●       ●●●
                                                                           ●●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                             ●●●
                                                                              ●●●
                                                                                ●●
                                                                                 ●●
                                                                                  ●●●
                                                                                    ●  ●       ●
                                                                                               ●●
                                                                                                ● ●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
           ●●
           ●●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
           ●●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                ●●
                ●●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                   ●●
                   ●
                     ●
                     ●
                    ●●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                        ●●
                         ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                           ●
                                  V2                ●
                                                   ●●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                   ●●●
                                                     ●
                                                     ●
                                                     ●
                                                     ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                       ●●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                         ●
                                                        ●●
                                                        ●
                                                        ●●
                                                         ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                           ●●
                                                            ●
                                                            ●
                                                           ●●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                           ●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                ●●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                 ●●
                                                                  ●
                                                                   ●
                                                                   ●
                                                                       ●
                                                                       ●●●
                                                                        ●
                                                                        ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                        ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                           ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                ●●●●
                                                                                ●  ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                    ●●
                                                                                    ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                       ●     ●●●
                                                                                              ●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                               ●●●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                   ●●
                                                                                                   ●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                          ●●
                                                                                                          ●
             ●●
             ● ●  ●●●●
                     ●   ●●                            ●●
                                                        ●●●
                                                          ●
                                                          ●●●●●●●  ●     ● ●●●●
                                                                              ●●
                                                                               ●                    ●●●●
                                                                                                       ●● ●
                                                                                                          ●
               ● ● ●                                     ●    ●             ●● ●                        ●●
              ●        ● ●●      ●●●  ●●    ●                                 ●●● ●●          ●●●●     ●
                                                                                                                  −0.5 0.5
           ●● ●●● ●●●
                    ●● ●
                       ● ●         ●●●● ●●●●                            ● ●●●
                                                                            ●●●
                                                                              ● ●●●           ●
                                                                                              ●   ●●●●●
           ●●●
             ●
             ●
              ●
              ●
              ●●
               ●
               ●●●
                 ●●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●●
                    ●
                    ●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●●
                       ●
                       ●●●●
                          ●●      ●●
                                  ●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●●
                                          ●
                                          ●●
                                           ●
                                           ●
                                           ●
                                           ●●
                                            ●                           ●
                                                                        ● ●
                                                                          ●●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●●
                                                                                  ●
                                                                                  ●
                                                                                  ● ●         ●
                                                                                              ●
                                                                                              ●●
                                                                                               ●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●●
                                                                                                      ●●
            ●
            ●
            ●●
             ●●●
               ●
               ●● ●
                  ●●
                   ●●●
                     ●●
                      ●
                      ●●
                       ●●●●       ●
                                  ● ●●●
                                      ●
                                      ●
                                      ●●
                                       ●●
                                        ●
                                        ●●
                                         ●
                                         ●● ●●●
                                              ●                         ●●●●
                                                                           ●●●
                                                                             ●
                                                                             ●●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●● ●
                                                                                   ● ●
                                                                                     ●       ●●●●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●●
                                                                                                       ●●
                                                                                                        ●
           ●
           ●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●
                 ●
                 ●
                 ●
                 ●
                 ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●●
                        ●
                        ●
                        ●
                        ●
                         ●
                        ●●
                         ●
                         ●
                         ●
                          ●
                          ●
                          ●
                          ●
                          ●    ●●
                                ●●
                                 ●●
                                  ●
                                  ●●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                        ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●●
                                             ●
                                               ●
                                               ●                       ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●●
                                                                               ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                              ●
                                                                                              ●●
                                                                                               ●
                                                                                               ●
                                                                                               ●
                                                                                               ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                      ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●●
                                                                                                          ●
                                                                                                          ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
            ●●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                 ●●
                  ●
                  ●
                  ●
                ●●●
                  ●
                ● ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                      ●●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                        ●●
                        ●
                        ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                         ●●
                          ●
                           ●
                           ●
                           ●
                                ●
                                ●
                                ●
                                ●●
                                 ●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                 ●●
                                 ●●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                     ●●
                                     ●
                                    ●●●
                                      ●
                                      ●
                                      ●
                                      ●
                                     ●●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                        ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                         ●●
                                         ●●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                            ●●
                                            ●
                                              ●
                                              ●
                                              ●
                                              ●
                                              ●●      V3               ●
                                                                       ●
                                                                       ●
                                                                       ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                         ●●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                               ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                 ●●
                                                                                  ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                   ●●●●
                                                                                      ●
                                                                                       ●
                                                                                               ●
                                                                                               ●
                                                                                               ●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                  ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                   ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                    ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                       ●
                                                                                                       ●
                                                                                                      ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                       ●●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                       ● ●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                          ●●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                          ●●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                     ●●●             ●● ●                                    ●● ●                       ●●●
             ●●●●
               ● ●  ●●    ●      ●●●●●●  ●
                                         ●●
                                          ●           ●●●●●●
                                                          ●  ●
                                                            ●● ●                               ●●●● ●● ●
                                                                                                       ●●
             ●  ●● ●
                   ●●●
                     ●●              ●● ●
                                        ●●  ●             ●●● ●
                                                              ●
                                                              ●●
                                                               ●                              ● ● ●
                                                                                                  ●●
                                                                                                   ●●
                                                                                                    ●●●
−0.6 0.4
           ●●●●
              ●●
               ●●
                ●
                ●●●
                  ●
                  ●●
                   ●
                   ●●
                    ●●
                     ●
                     ●●●
                       ●
                       ●●●●
                          ●        ●
                                   ●
                                  ●●●●
                                     ●●
                                      ●●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●●
                                          ●
                                          ●
                                          ●●
                                           ●
                                           ●          ●●●●
                                                         ●●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●●
                                                               ●
                                                               ●
                                                               ●●●●                           ●
                                                                                              ●
                                                                                              ● ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●●●●●
             ●
             ●●
              ●
              ●●●●
                 ●●
                  ●
                  ●●
                   ●●
                    ●
                    ●●
                     ●
                     ●● ●        ●●●●
                                    ●●
                                     ●
                                     ●●
                                      ●
                                      ●●
                                       ●
                                       ●●
                                        ●
                                        ●●
                                         ●●●  ●       ●●
                                                       ●●●
                                                         ●●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●●
                                                               ●                                ●
                                                                                                ●
                                                                                                ●●
                                                                                                 ●
                                                                                                 ●●
                                                                                                  ●
                                                                                                  ●●
                                                                                                   ●
                                                                                                   ●●
                                                                                                    ●●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●●
                                                                                                       ●●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●●
                 ●
                 ●
                 ●
                 ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                      ●●
                        ●
                        ●
                        ●
                        ●
                        ●●
                         ●
                         ●●
                          ●
                          ●
                          ●
                          ●●
                           ●    ●●●
                                  ●
                                  ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                             ●
                                             ●●
                                              ●
                                              ●      ●●●
                                                       ●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                         ●●
                                                         ●●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                           ●
                                                           ●●
                                                            ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                   ●                          ●
                                                                                              ●
                                                                                              ●
                                                                                              ●
                                                                                              ●●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●●●●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                ●●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                      ●●●
                      ●
                        ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                           ●
                           ●
                           ●
                                ●
                                ●
                                ●
                               ●●
                                 ●
                                 ●
                                 ●
                                 ●
                                 ●
                                 ●
                                ●●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                  ●
                                 ●●
                                  ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                       ●●
                                        ●
                                        ●
                                       ●●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                           ●●
                                           ●
                                           ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                              ●
                                              ●
                                              ●
                                              ●
                                              ●
                                             ●●
                                                   ●●
                                                   ●●
                                                   ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                        ●●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●
                                                            ●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                  ●
                                                                   ●
                                                                   ●
                                                                          V4                 ●●
                                                                                              ●●●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                 ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                 ●●
                                                                                                 ●●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                   ●
                                                                                                    ●
                                                                                                    ●
                                                                                                   ●●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                    ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                     ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                     ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                     ●●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                      ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                       ●●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                        ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                          ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                           ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                            ●
                                                                                                             ●
                                                                                                             ●
                                                                                                             ●
                                                                                                             ●
                                                                                                             ●
                                                                                                             ●●
              ●●●●
            ● ●●● ●
                   ●●
                    ●●
                     ●●
                      ●●●
                       ●  ●        ●●
                                   ●  ●● ●
                                         ●●
                                         ●●●
                                        ●●●●
                                        ●
                                        ●   ●            ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●●●
                                                            ●●● ●
                                                               ●●                                   ● ●
                                                                                                    ●●●
                                                                                                      ●●●
                                                                                                       ●●
                                                                                                       ●
                                                                                                       ●  ●
                                                                                                          ●●●
                                                                                                           ●●●
                   ●   ●●●         ●
                                   ●                   ●●
                                                        ●                 ●
                                                                       ●●
                                                                                                                  1.0
               ●●● ●
                   ●●●
                     ●●
                      ●●  ●     ●  ●●
                                    ●
                                    ●●
                                     ●
                                     ●
                                     ●●●●
                                        ●          ●●●
                                                     ●
                                                     ●●
                                                      ● ●
                                                        ●●
                                                         ●●              ●●
                                                                          ●●
                                                                           ●
                                                                           ●●●●●
                                                                               ●●
           ●
           ●● ●
              ●●
               ●
               ●●
                ●●
                 ●●
                  ●
                  ●
                  ●●
                   ●
                   ●
                   ●●
                    ●
                    ●●
                     ●●
                      ● ●
                        ●●
                         ●●    ●● ●
                                 ●●●
                                   ●
                                   ●●
                                    ●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●
                                     ●●
                                      ●●
                                       ●
                                       ●●
                                        ●           ●●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●●
                                                        ●
                                                        ●●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●●
                                                            ●●         ●●
                                                                        ●●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                              ●●
            ●●
             ●
             ●
             ●●
              ●
              ●●
               ●
               ●
               ●
               ●●
                ●●
                 ●
                 ●●
                  ●
                  ●
                  ●
                  ●●
                   ●●
                    ●
                    ●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●
                     ●●
                      ●
                      ●●
                       ●
                       ●●
                        ●
                        ●●
                         ●
                         ●
                         ●
                         ●●
                          ●      ●
                                ●●●
                                  ●
                                  ●●
                                   ●
                                   ●
                                   ●
                                   ●●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●●
                                          ●●       ●●
                                                    ●●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●
                                                      ●●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●●●
                                                             ●●        ●●
                                                                        ●●
                                                                         ●●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●●●●●
           ●
           ●●
            ●●
             ●●
              ●
              ●
              ●●
               ●
               ●
               ●
               ●●
                ●
                ●●
                 ●
                 ●
                 ●●
                  ●●
                   ●
                   ●
                   ●
                   ●●
                    ●
                    ●
                    ●●
                     ●
                     ●
                     ●
                     ●●
                      ●
                      ●
                      ●
                      ●
                      ●●
                       ●
                       ●●
                        ●●
                         ●
                         ●
                         ●
                         ●●
                          ●
                          ●      ●
                                 ●●
                                  ●●
                                   ●
                                   ●
                                   ●●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●
                                     ●●
                                      ●
                                      ●
                                     ●●
                                      ●
                                      ●
                                      ●●
                                       ●
                                       ●
                                       ●
                                       ●●
                                       ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●●
                                          ●
                                          ●●●
                                           ●
                                           ●
                                           ●       ●●
                                                    ●●
                                                     ●
                                                     ●
                                                     ●●
                                                      ●
                                                      ●●
                                                       ●
                                                      ●●
                                                       ●
                                                       ●●
                                                        ●
                                                        ●
                                                        ●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●●
                                                             ●●
                                                              ●          ●
                                                                         ●●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●●
                                                                           ●
                                                                           ●
                                                                           ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                            ●●
                                                                            ●
                                                                            ●●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●●
            ●
            ●
            ●●
             ●●
              ●
              ●●●●
                 ●●●
                   ●
                   ●
                   ●●
                    ●●
                     ●
                     ●●
                      ●
                      ●●
                       ●●
                        ● ●●      ●●
                                   ●●
                                    ●
                                    ●
                                    ●●
                                     ●
                                     ●●●
                                       ●
                                       ●
                                       ●●
                                        ●
                                        ●●
                                         ●●●
                                           ●●       ●●●●
                                                       ●●
                                                        ●●
                                                         ●
                                                         ●●
                                                          ●
                                                         ●●
                                                          ●
                                                         ●●●
                                                           ●●
                                                            ●
                                                            ●●
                                                             ●●
                                                              ●●  ●    ●●
                                                                        ●  ●
                                                                           ●●
                                                                            ●●
                                                                             ●
                                                                             ●●
                                                                              ●
                                                                              ●●
                                                                               ●●
                                                                               ●
                                                                               ●
                                                                               ●●●● ● ●●
                                                                                     ●●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
           ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
            ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
             ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
             ●●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
              ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
               ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                 ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                  ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                   ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                    ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                        ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                        ●●
                        ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                          ●
                           ●
                                 ●●
                                  ●
                                  ●
                                 ●●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                   ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                    ●
                                   ●●
                                     ●●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                       ●
                                       ●
                                       ●
                                      ●●
                                       ●
                                       ●
                                       ●
                                      ●●
                                      ●
                                     ●●
                                     ●
                                     ●
                                     ●●
                                      ●●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                       ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                       ●●
                                       ●
                                       ●●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                        ●
                                         ●
                                         ●
                                        ●●
                                        ●
                                        ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                        ●●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                         ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                         ●●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                            ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                              ●●
                                              ●
                                              ●
                                              ●●
                                                    ●
                                                    ●
                                                    ●●
                                                     ●
                                                    ●●●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                      ●
                                                     ●●
                                                      ●
                                                      ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                        ●
                                                       ●●
                                                        ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                        ●●
                                                        ●●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                         ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                         ●●
                                                          ●
                                                          ●
                                                          ●●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                          ●●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                           ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                             ●
                                                             ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                            ●
                                                           ●●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                            ●●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                             ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                 ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                         ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                          ●
                                                                        ●●●
                                                                          ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                           ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                           ●●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                            ●
                                                                             ●
                                                                            ●●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                            ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                              ●
                                                                             ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                             ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                            ●●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                             ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                              ●●
                                                                              ●
                                                                              ●
                                                                              ●
                                                                              ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                               ●●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                ●●
                                                                               ●●
                                                                                ●
                                                                                ●●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                 ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                  ●
                                                                                 ●●
                                                                                 ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                   ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                    ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                     ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                      ●
                                                                                       ●        V5
                                                                                                                  −1.5
            ●●
             ●●
              ●●●
               ●●● ●
                  ●●●●
                     ●●  ●            ● ●
                                       ●●●
                                         ●
                                         ●●
                                          ●●●
                                            ●●●          ●●
                                                          ●●●
                                                            ●
                                                            ●●
                                                             ●●●
                                                               ●
                                                               ●● ●          ●●
                                                                              ●●●●
                                                                                 ●●
                                                                                  ●●●
                                                                                  ●●●●●●
           ●
           ●
            ●●
             ●●
              ●●
               ●●●●
                  ●●●
                     ●●
                     ●
                     ● ●●          ●●  ●
                                        ●●●
                                          ●
                                          ●
                                          ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●●
                                            ●● ●          ●●●●
                                                             ●
                                                             ●
                                                             ●●
                                                              ●
                                                              ●
                                                               ●
                                                               ●
                                                               ●
                                                               ●●●
                                                                 ●
                                                                  ●
                                                                  ●            ●●●
                                                                                ●
                                                                                ●
                                                                                 ●
                                                                                 ●
                                                                                 ●●
                                                                                  ● ●
                                                                                    ●●●
                                                                                      ●
                         ●                     ●               ●                ●
           −2 0          2                         −0.5 0.5                                 −1.5        1.0
"
"./07_RegressionModels/02_04_residuals_variation_diagnostics/fig/unnamed-chunk-9.pdf","             2
             1
resid(fit)
             0
             −1
             −2
                  −0.4   −0.2   0.0        0.2   0.4
                            predict(fit)
"
"./07_RegressionModels/03_03_countOutcomes/Count outcomes.pdf","9/3/13                                                                                                Count outcomes
                            Count outcomes
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                1/19
 
 9/3/13                                                                                                Count outcomes
              Key ideas
                 · Many data take the form of counts
                          - Calls to a call center
                          - Number of flu cases in an area
                          - Number of cars that cross a bridge
                 · Data may also be in the form of rates
                          - Percent of children passing a test
                          - Percent of hits to a website from a country
                 · Linear regression with transformation is an option
                                                                                                                     2/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     2/19
 
 9/3/13                                                                                                Count outcomes
              Poisson distribution
                 set.seed(3433); par(mfrow=c(1,2))
                 poisData2 <- rpois(100,lambda=100); poisData1 <- rpois(100,lambda=50)
                 hist(poisData1,col=""blue"",xlim=c(0,150)); hist(poisData2,col=""blue"",xlim=c(0,150))
                                                                                                                     3/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     3/19
 
 9/3/13                                                                                                Count outcomes
              Poisson distribution
                 c(mean(poisData1),var(poisData1))
                 [1] 49.85 49.38
                 c(mean(poisData2),var(poisData2))
                 [1] 100.12 95.26
                                                                                                                     4/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     4/19
 
 9/3/13                                                                                                Count outcomes
              Example: Leek Group Website Traffic
              http://biostat.jhsph.edu/~jleek/
                                                                                                                     5/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     5/19
 
 9/3/13                                                                                                Count outcomes
              Website data
                 download.file(""https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda"",destfile=""./data/gaData.rda""
                 load(""./data/gaData.rda"")
                 gaData$julian <- julian(gaData$date)
                 head(gaData)
                                 date visits simplystats julian
                 1 2011-01-01                        0                    0 14975
                 2 2011-01-02                        0                    0 14976
                 3 2011-01-03                        0                    0 14977
                 4 2011-01-04                        0                    0 14978
                 5 2011-01-05                        0                    0 14979
                 6 2011-01-06                        0                    0 14980
              http://skardhamar.github.com/rga/
                                                                                                                     6/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     6/19
 
 9/3/13                                                                                                Count outcomes
              Plot data
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                                                                                                                     7/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     7/19
 
 9/3/13                                                                                                Count outcomes
              Linear regression
                                                                                      N Hi = b 0 + b 1 J D i + e i
              NH i      - number of hits to the website
              JD i     - day of the year (Julian day)
              b0    - number of hits on Julian day 0 (1970-01-01)
              b1    - increase in number of hits per unit day
              ei   - variation due to everything we didn't measure
                                                                                                                     8/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     8/19
 
 9/3/13                                                                                                Count outcomes
              Linear regression line
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                 lm1 <- lm(gaData$visits ~ gaData$julian)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                     9/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     9/19
 
 9/3/13                                                                                                Count outcomes
              Linear vs. Poisson regression
              Linear
                                                                                      N Hi = b 0 + b 1 J D i + e i
              or
                                                                              E[N H i |J Di , b 0 , b 1 ] = b 0 + b 1 J Di
              Poisson/log-linear
                                                                          log (E[NH i |JD i , b 0 , b 1 ]) = b 0 + b 1 J Di
              or
                                                                         E[N H i |J D i , b 0 , b 1 ] = exp ( b 0 + b 1 J Di )
                                                                                                                               10/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                10/19
 
 9/3/13                                                                                                 Count outcomes
              Multiplicative differences
                                                                         E[N H i |J D i , b 0 , b 1 ] = exp ( b 0 + b 1 J Di )
                                                                      E[N H i |J Di , b 0 , b 1 ] = exp ( b 0 ) exp ( b 1 J Di )
              If JD is increased by one unit, E[N H |JD , b
                        i                                                        i      i     0,  b1 ] is multiplied by exp(b   1)
                                                                                                                                   11/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                    11/19
 
 9/3/13                                                                                                Count outcomes
              Poisson regression in R
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                 glm1 <- glm(gaData$visits ~ gaData$julian,family=""poisson"")
                 abline(lm1,col=""red"",lwd=3); lines(gaData$julian,glm1$fitted,col=""blue"",lwd=3)
                                                                                                                     12/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      12/19
 
 9/3/13                                                                                                Count outcomes
              Mean-variance relationship?
                 plot(glm1$fitted,glm1$residuals,pch=19,col=""grey"",ylab=""Residuals"",xlab=""Date"")
                                                                                                                     13/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      13/19
 
 9/3/13                                                                                                Count outcomes
              Model agnostic standard errors
                 library(sandwich)
                 confint.agnostic <- function (object, parm, level = 0.95, ...)
                 {
                          cf <- coef(object); pnames <- names(cf)
                          if (missing(parm))
                                 parm <- pnames
                          else if (is.numeric(parm))
                                 parm <- pnames[parm]
                          a <- (1 - level)/2; a <- c(a, 1 - a)
                          pct <- stats:::format.perc(a, 3)
                          fac <- qnorm(a)
                          ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                                                                                     pct))
                          ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
                          ci[] <- cf[parm] + ses %o% fac
                          ci
                 }
              http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval
                                                                                                                           14/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                            14/19
 
 9/3/13                                                                                                Count outcomes
              Estimating confidence intervals
                 confint(glm1)
                                                     2.5 %            97.5 %
                 (Intercept) -34.34658 -31.159716
                 gaData$julian 0.00219 0.002396
                 confint.agnostic(glm1)
                                                                                                                     15/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      15/19
 
 9/3/13                                                                                                Count outcomes
              Rates
                                                                   E[NH S S i |J Di , b 0 , b 1 ]/N H i = exp ( b 0 + b 1 J D i )
                                                             log (E[NH S S i |JD i , b 0 , b 1 ])       J    log(N Hi ) = b 0 + b 1 J Di
                                                             log (E[NH S S i |JD i , b 0 , b 1 ]) = log(N Hi ) + b 0 + b 1 J Di
                                                                                                                                         16/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                          16/19
 
 9/3/13                                                                                                Count outcomes
              Fitting rates in R
                 glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
                                        family=""poisson"",data=gaData)
                 plot(julian(gaData$date),glm2$fitted,col=""blue"",pch=19,xlab=""Date"",ylab=""Fitted Counts"")
                 points(julian(gaData$date),glm1$fitted,col=""red"",pch=19)
                                                                                                                     17/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      17/19
 
 9/3/13                                                                                                Count outcomes
              Fitting rates in R
                 glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
                                        family=""poisson"",data=gaData)
                 plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col=""grey"",xlab=""Date"",
                            ylab=""Fitted Rates"",pch=19)
                 lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col=""blue"",lwd=3)
                                                                                                                     18/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      18/19
 
 9/3/13                                                                                                Count outcomes
              More information
                 · Log-linear models and multiway tables
                 · Wikipedia on Poisson regression, Wikipedia on overdispersion
                 · Regression models for count data in R
                 · pscl package - the function zeroinfl fits zero inflated models.
                                                                                                                     19/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      19/19
"
"./07_RegressionModels/originalContent/002basicLeastSquares/Basic least squares.pdf","8/29/13                                                                                                  Basic least squares
                            Basic least squares
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                    1/19
 
 8/29/13                                                                                                  Basic least squares
              Goals of statistical modeling
                 · Describe the distribution of variables
                 · Describe the relationship between variables
                 · Make inferences about distributions or relationships
                                                                                                                             2/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         2/19
 
 8/29/13                                                                                                  Basic least squares
              Example: Average parent and child heights
              http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html
                                                                                                                             3/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         3/19
 
 8/29/13                                                                                                  Basic least squares
              Still relevant
              http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html
              Predicting height: the Victorian approach beats modern genomics
                                                                                                                             4/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         4/19
 
 8/29/13                                                                                                  Basic least squares
              Load Galton Data
              You may need to run install.packages(""UsingR"")if the UsingRlibrary is not installed
                 library(UsingR); data(galton)
                 par(mfrow=c(1,2))
                 hist(galton$child,col=""blue"",breaks=100)
                 hist(galton$parent,col=""blue"",breaks=100)
                                                                                                                             5/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         5/19
 
 8/29/13                                                                                                  Basic least squares
              The distribution of child heights
                 hist(galton$child,col=""blue"",breaks=100)
                                                                                                                             6/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         6/19
 
 8/29/13                                                                                                  Basic least squares
              Only know the child - average height
                 hist(galton$child,col=""blue"",breaks=100)
                 meanChild <- mean(galton$child)
                 lines(rep(meanChild,100),seq(0,150,length=100),col=""red"",lwd=5)
                                                                                                                             7/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         7/19
 
 8/29/13                                                                                                  Basic least squares
              Only know the child - why average?
              If C is the height of child i then the average is the value of + that minimizes:
                     i
                                                                                                             J+
                                                                                                 928
                                                                                                                      2
                                                                                                      (C i          )
                                                                                                ∑
                                                                                                 i=1
                                                                                                                             8/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         8/19
 
 8/29/13                                                                                                  Basic least squares
              What if we plot child versus average parent
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                                                                                                                             9/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                         9/19
 
 8/29/13                                                                                                  Basic least squares
              Jittered plot
                 set.seed(1234)
                 plot(jitter(galton$parent,factor=2),jitter(galton$child,factor=2),pch=19,col=""blue"")
                                                                                                                             10/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          10/19
 
 8/29/13                                                                                                  Basic least squares
              Average parent = 65 inches tall
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 near65 <- galton[abs(galton$parent - 65)<1, ]
                 points(near65$parent,near65$child,pch=19,col=""red"")
                 lines(seq(64,66,length=100),rep(mean(near65$child),100),col=""red"",lwd=4)
                                                                                                                             11/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          11/19
 
 8/29/13                                                                                                  Basic least squares
              Average parent = 71 inches tall
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 near71 <- galton[abs(galton$parent - 71)<1, ]
                 points(near71$parent,near71$child,pch=19,col=""red"")
                 lines(seq(70,72,length=100),rep(mean(near71$child),100),col=""red"",lwd=4)
                                                                                                                             12/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          12/19
 
 8/29/13                                                                                                  Basic least squares
              Fitting a line
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lm1 <- lm(galton$child ~ galton$parent)
                 lines(galton$parent,lm1$fitted,col=""red"",lwd=3)
                                                                                                                             13/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          13/19
 
 8/29/13                                                                                                  Basic least squares
              Why not this line?
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lines(galton$parent, 26 + 0.646*galton$parent)
                                                                                                                             14/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          14/19
 
 8/29/13                                                                                                  Basic least squares
              The equation for a line
              If C is the height of child i and P is the height of the average parent, then we can imagine writing the
                     i                                                     i
              equation for a line
                                                                                               C i = b 0 + b 1 Pi
                                                                                                                             15/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          15/19
 
 8/29/13                                                                                                  Basic least squares
              Not all points are on the line
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lines(galton$parent,lm1$fitted,col=""red"",lwd=3)
                                                                                                                             16/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          16/19
 
 8/29/13                                                                                                  Basic least squares
              Allowing for variation
              If C is the height of child i and P is the height of the average parent, then we can imagine writing the
                     i                                                     i
              equation for a line
                                                                                          C i = b 0 + b 1 Pi + e i
              ei    is everything we didn't measure (how much they eat, where they live, do they stretch in the
              morning...)
                                                                                                                             17/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          17/19
 
 8/29/13                                                                                                  Basic least squares
              How do we pick best?
              If C is the height of child i and P is the height of the average parent, pick the line that makes the child
                     i                                                    i
              values C and our guesses
                                i
                                                                                                   J
                                                                                       928
                                                                                                                             2
                                                                                            (C i       {b 0 + b 1 Pi })
                                                                                       ∑
                                                                                       i=1
                                                                                                                               18/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                            18/19
 
 8/29/13                                                                                                  Basic least squares
              Plot what is leftover
                 par(mfrow=c(1,2))
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lines(galton$parent,lm1$fitted,col=""red"",lwd=3)
                 plot(galton$parent,lm1$residuals,col=""blue"",pch=19)
                 abline(c(0,0),col=""red"",lwd=3)
                                                                                                                             19/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/002basicLeastSquares/index.html#1                          19/19
"
"./07_RegressionModels/originalContent/002binaryOutcomes/Binary outcomes.pdf","9/3/13                                                                                                 Binary outcomes
                            Binary outcomes
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                 1/22
 
 9/3/13                                                                                                 Binary outcomes
              Key ideas
                 · Frequently we care about outcomes that have two values
                          - Alive/dead
                          - Win/loss
                          - Success/Failure
                          - etc
                 · Called binary outcomes or 0/1 outcomes
                 · Linear regression (like we've seen) may not be the best
                                                                                                                       2/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                      2/22
 
 9/3/13                                                                                                 Binary outcomes
              Example: Baltimore Ravens
              http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens
                                                                                                                       3/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                      3/22
 
 9/3/13                                                                                                 Binary outcomes
              Ravens Data
                 download.file(""https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda"",
                                             destfile=""./data/ravensData.rda"",method=""curl"")
                 load(""./data/ravensData.rda"")
                 head(ravensData)
                    ravenWinNum ravenWin ravenScore opponentScore
                 1                      1                 W                 24                         9
                 2                      1                 W                 38                       35
                 3                      1                 W                 28                       13
                 4                      1                 W                 34                       31
                 5                      1                 W                 44                       13
                 6                      0                 L                 23                       24
                                                                                                                       4/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                      4/22
 
 9/3/13                                                                                                 Binary outcomes
              Linear regression
                                                                                      RW i = b 0 + b 1 RS i + e i
              RW i       - 1 if a Ravens win, 0 if not
              RS i     - Number of points Ravens scored
              b0    - probability of a Ravens win if they score 0 points
              b1    - increase in probability of a Ravens win for each additional point
              ei   - variation due to everything we didn't measure
                                                                                                                       5/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                      5/22
 
 9/3/13                                                                                                 Binary outcomes
              Linear regression in R
                 lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
                 summary(lmRavens)
                 Call:
                 lm(formula = ravensData$ravenWinNum ~ ravensData$ravenScore)
                 Residuals:
                       Min            1Q Median                   3Q        Max
                 -0.730 -0.508 0.182 0.322 0.572
                 Coefficients:
                                                            Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                                  0.28503            0.25664             1.11          0.281
                 ravensData$ravenScore 0.01590                                   0.00906             1.76          0.096 .
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                 Residual standard error: 0.446 on 18 degrees of freedom
                 Multiple R-squared: 0.146, Adjusted R-squared: 0.0987
                 F-statistic: 3.08 on 1 and 18 DF, p-value: 0.0963
                                                                                                                           6/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                          6/22
 
 9/3/13                                                                                                 Binary outcomes
              Linear regression
                 plot(ravensData$ravenScore,lmRavens$fitted,pch=19,col=""blue"",ylab=""Prob Win"",xlab=""Raven Score"")
                                                                                                                       7/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                      7/22
 
 9/3/13                                                                                                  Binary outcomes
              Odds
              Binary Outcome 0/1
                                                                                                       RW i
              Probability (0,1)
                                                                                         Pr(RWi |RSi , b 0 , b 1 )
              Odds (0,           V  )
                                                                                         Pr(RWi |RSi , b 0 , b 1 )
                                                                                      1  J   Pr(RWi |RSi , b 0 , b 1 )
              Log odds (            JV V    ,     )
                                                                                            Pr(RWi |RSi , b 0 , b 1 )
                                                                                log
                                                                                     (1     J   Pr(RWi |RSi , b 0 , b 1 ) )
                                                                                                                            8/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                           8/22
 
 9/3/13                                                                                                 Binary outcomes
              Linear vs. logistic regression
              Linear
                                                                                      RW i = b 0 + b 1 RS i + e i
              or
                                                                               E[R W i |RS i , b 0 , b 1 ] = b 0 + b 1 RS i
              Logistic
                                                                                                               exp(b 0 + b 1 RSi )
                                                                     Pr(RWi |RSi , b 0 , b 1 ) =
                                                                                                           1 + exp(b 0 + b 1 RSi )
              or
                                                                                Pr(RWi |RSi , b 0 , b 1 )
                                                                    log
                                                                         (1     J  Pr(RWi |RSi , b 0 , b 1 ) )
                                                                                                                       = b 0 + b 1 RS i
                                                                                                                                        9/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                                       9/22
 
 9/3/13                                                                                                 Binary outcomes
              Interpreting Logistic Regression
                                                                                Pr(RWi |RSi , b 0 , b 1 )
                                                                    log
                                                                         (1     J  Pr(RWi |RSi , b 0 , b 1 ) )
                                                                                                                       = b 0 + b 1 RS i
              b0    - Log odds of a Ravens win if they score zero points
              b1    - Log odds ratio of win probability for each point scored (compared to zero points)
              exp(b 1 )        - Odds ratio of win probability for each point scored (compared to zero points)
                                                                                                                                        10/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                                        10/22
 
 9/3/13                                                                                                 Binary outcomes
              Explaining Odds
              via Ken Rice
                                                                                                                       11/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       11/22
 
 9/3/13                                                                                                 Binary outcomes
              Probability of Death
              via Ken Rice
                                                                                                                       12/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       12/22
 
 9/3/13                                                                                                 Binary outcomes
              Odds of Death
              via Ken Rice
                                                                                                                       13/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       13/22
 
 9/3/13                                                                                                 Binary outcomes
              Odds Ratio = 1, Continuous Covariate
              via Ken Rice
                                                                                                                       14/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       14/22
 
 9/3/13                                                                                                 Binary outcomes
              Different odds ratios
              via Ken Rice
                                                                                                                       15/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       15/22
 
 9/3/13                                                                                                 Binary outcomes
              Ravens logistic regression
                 logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family=""binomial"")
                 summary(logRegRavens)
                 Call:
                 glm(formula = ravensData$ravenWinNum ~ ravensData$ravenScore,
                          family = ""binomial"")
                 Deviance Residuals:
                       Min              1Q Median                     3Q           Max
                 -1.758 -1.100 0.530                              0.806         1.495
                 Coefficients:
                                                            Estimate Std. Error z value Pr(>|z|)
                 (Intercept)                                  -1.6800              1.5541 -1.08                     0.28
                 ravensData$ravenScore 0.1066                                      0.0667            1.60           0.11
                 (Dispersion parameter for binomial family taken to be 1)
                          Null deviance: 24.435 on 19 degrees of freedom
                 Residual deviance: 20.895 on 18 degrees of freedom
                 AIC: 24.89
                                                                                                                         16/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                         16/22
 
 9/3/13                                                                                                 Binary outcomes
              Ravens fitted values
                 plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col=""blue"",xlab=""Score"",ylab=""Prob Ravens Win"")
                                                                                                                       17/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       17/22
 
 9/3/13                                                                                                 Binary outcomes
              Odds ratios and confidence intervals
                 exp(logRegRavens$coeff)
                                    (Intercept) ravensData$ravenScore
                                               0.1864                                 1.1125
                 exp(confint(logRegRavens))
                                                                  2.5 % 97.5 %
                 (Intercept)                                0.005675 3.106
                 ravensData$ravenScore 0.996230 1.303
                                                                                                                       18/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       18/22
 
 9/3/13                                                                                                 Binary outcomes
              ANOVA for logistic regression
                 anova(logRegRavens,test=""Chisq"")
                 Analysis of Deviance Table
                 Model: binomial, link: logit
                 Response: ravensData$ravenWinNum
                 Terms added sequentially (first to last)
                                                            Df Deviance Resid. Df Resid. Dev Pr(>Chi)
                 NULL                                                                         19             24.4
                 ravensData$ravenScore 1                                3.54                  18             20.9      0.06 .
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                                                                                                                              19/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                              19/22
 
 9/3/13                                                                                                 Binary outcomes
              Simpson's paradox
              http://en.wikipedia.org/wiki/Simpson's_paradox
                                                                                                                       20/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       20/22
 
 9/3/13                                                                                                 Binary outcomes
              Interpreting Odds Ratios
                 · Not probabilities
                 · Odds ratio of 1 = no difference in odds
                 · Log odds ratio of 0 = no difference in odds
                 · Odds ratio < 0.5 or > 2 commonly a ""moderate effect""
                                               Pr(R Wi |RS i =10)
                 · Relative risk               Pr(R Wi |RS i =0)
                                                                     often easier to interpret, harder to estimate
                 · For small probabilities RR                         ¡ OR but they are not the same!
              Wikipedia on Odds Ratio
                                                                                                                       21/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       21/22
 
 9/3/13                                                                                                 Binary outcomes
              Further resources
                 · Wikipedia on Logistic Regression
                 · Logistic regression and glms in R
                 · Brian Caffo's lecture notes on: Simpson's paradox, Case-control studies
                 · Open Intro Chapter on Logistic Regression
                                                                                                                       22/22
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/002binaryOutcomes/index.html#1                       22/22
"
"./07_RegressionModels/originalContent/002simulationForModelChecking/Simulation for model checking.pdf","9/3/13                                                                                           Simulation for model checking
                            Simulation for model checking
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1             1/15
 
 9/3/13                                                                                           Simulation for model checking
              Basic ideas
                 · Way back in the first week we talked about simulating data from distributions in R using the rfoo
                    functions.
                 · In general simulations are way more flexible/useful
                          - For bootstrapping as we saw in week 7
                          - For evaluating models
                          - For testing different hypotheses
                          - For sensitivity analysis
                 · At minimum it is useful to simulate
                          - A best case scenario
                          - A few examples where you know your approach won't work
                          - The importance of simulating the extremes
                                                                                                                               2/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  2/15
 
 9/3/13                                                                                           Simulation for model checking
              Simulating data from a model
              Suppose that you have a regression model
                                                                                          Yi = b 0 + b 1 Xi + e i
              Here is an example of generating data from this model where X and e are normal:                                  i i
                 set.seed(44333)
                 x <- rnorm(50)
                 e <- rnorm(50)
                 b0 <- 1; b1 <- 2
                 y <- b0 + b1*x + e
                                                                                                                                   3/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                      3/15
 
 9/3/13                                                                                           Simulation for model checking
              Violating assumptions
                 set.seed(44333)
                 x <- rnorm(50)
                 e <- rnorm(50); e2 <- rcauchy(50)
                 b0 <- 1; b1 <- 2
                 y <- b0 + b1*x + e; y2 <- b0 + b1*x + e2
                                                                                                                               4/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  4/15
 
 9/3/13                                                                                           Simulation for model checking
              Violating assumptions
                 par(mfrow=c(1,2))
                 plot(lm(y ~ x)$fitted,lm(y~x)$residuals,pch=19,xlab=""fitted"",ylab=""residuals"")
                 plot(lm(y2 ~ x)$fitted,lm(y2~x)$residuals,pch=19,xlab=""fitted"",ylab=""residuals"")
                                                                                                                               5/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  5/15
 
 9/3/13                                                                                           Simulation for model checking
              Repeated simulations
                 set.seed(44333)
                 betaNorm <- betaCauch <- rep(NA,1000)
                 for(i in 1:1000){
                    x <- rnorm(50); e <- rnorm(50); e2 <- rcauchy(50); b0 <- 1; b1 <- 2
                    y <- b0 + b1*x + e; y2 <- b0 + b1*x + e2
                    betaNorm[i] <- lm(y ~ x)$coeff[2]; betaCauch[i] <- lm(y2 ~ x)$coeff[2]
                 }
                 quantile(betaNorm)
                       0% 25% 50% 75% 100%
                 1.500 1.906 2.013 2.100 2.596
                 quantile(betaCauch)
                              0%             25%              50%             75%           100%
                 -278.352               1.130             1.965           2.804 272.391
                                                                                                                               6/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  6/15
 
 9/3/13                                                                                           Simulation for model checking
              Monte Carlo Error
                 boxplot(betaNorm,betaCauch,col=""blue"",ylim=c(-5,5))
                                                                                                                               7/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  7/15
 
 9/3/13                                                                                           Simulation for model checking
              Simulation based on a data set
                 library(UsingR); data(galton); nobs <- dim(galton)[1]
                 par(mfrow=c(1,2))
                 hist(galton$child,col=""blue"",breaks=100)
                 hist(galton$parent,col=""blue"",breaks=100)
                                                                                                                               8/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  8/15
 
 9/3/13                                                                                           Simulation for model checking
              Calculating means,variances
                 lm1 <- lm(galton$child ~ galton$parent)
                 parent0 <- rnorm(nobs,sd=sd(galton$parent),mean=mean(galton$parent))
                 child0 <- lm1$coeff[1] + lm1$coeff[2]*parent0 + rnorm(nobs,sd=summary(lm1)$sigma)
                 par(mfrow=c(1,2))
                 plot(galton$parent,galton$child,pch=19)
                 plot(parent0,child0,pch=19,col=""blue"")
                                                                                                                               9/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                  9/15
 
 9/3/13                                                                                           Simulation for model checking
              Simulating more complicated scenarios
                 library(bootstrap); data(stamp); nobs <- dim(stamp)[1]
                 hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
                 dens <- density(stamp$Thickness)
                 lines(dens,col=""blue"",lwd=3)
                                                                                                                               10/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   10/15
 
 9/3/13                                                                                           Simulation for model checking
              A simulation that is too simple
                 plot(density(stamp$Thickness),col=""black"",lwd=3)
                 for(i in 1:10){
                    newThick <- rnorm(nobs,mean=mean(stamp$Thickness),sd=sd(stamp$Thickness))
                    lines(density(newThick),col=""grey"",lwd=3)
                 }
                                                                                                                               11/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   11/15
 
 9/3/13                                                                                           Simulation for model checking
              How density estimation works
              http://en.wikipedia.org/wiki/File:Comparison_of_1D_histogram_and_KDE.png
                                                                                                                               12/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   12/15
 
 9/3/13                                                                                           Simulation for model checking
              Simulating from the density estimate
                 plot(density(stamp$Thickness),col=""black"",lwd=3)
                 for(i in 1:10){
                    newThick <- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw)
                    lines(density(newThick),col=""grey"",lwd=3)
                 }
                                                                                                                               13/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   13/15
 
 9/3/13                                                                                           Simulation for model checking
              Increasing variability
                 plot(density(stamp$Thickness),col=""black"",lwd=3)
                 for(i in 1:10){
                    newThick <- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
                    lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
                 }
                                                                                                                               14/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   14/15
 
 9/3/13                                                                                           Simulation for model checking
              Notes and further resources
              Notes
                 · Simulation can be applied to missing data problems - simulate what missing data might be
                 · Simulation values are often drawn from standard distributions, but this may not be appropriate
                 · Sensitivity analysis means trying different simulations with different assumptions and seeing how
                    estimates change
              Further resources
                 · Advanced Data Analysis From An Elementary Point of View
                 · The design of simulation studies in medical statistics
                 · Simulation studies in statistics
                                                                                                                               15/15
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week8/002simulationForModelChecking/index.html#1                   15/15
"
"./07_RegressionModels/originalContent/003countOutcomes/Count outcomes.pdf","9/3/13                                                                                                Count outcomes
                            Count outcomes
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                1/19
 
 9/3/13                                                                                                Count outcomes
              Key ideas
                 · Many data take the form of counts
                          - Calls to a call center
                          - Number of flu cases in an area
                          - Number of cars that cross a bridge
                 · Data may also be in the form of rates
                          - Percent of children passing a test
                          - Percent of hits to a website from a country
                 · Linear regression with transformation is an option
                                                                                                                     2/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     2/19
 
 9/3/13                                                                                                Count outcomes
              Poisson distribution
                 set.seed(3433); par(mfrow=c(1,2))
                 poisData2 <- rpois(100,lambda=100); poisData1 <- rpois(100,lambda=50)
                 hist(poisData1,col=""blue"",xlim=c(0,150)); hist(poisData2,col=""blue"",xlim=c(0,150))
                                                                                                                     3/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     3/19
 
 9/3/13                                                                                                Count outcomes
              Poisson distribution
                 c(mean(poisData1),var(poisData1))
                 [1] 49.85 49.38
                 c(mean(poisData2),var(poisData2))
                 [1] 100.12 95.26
                                                                                                                     4/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     4/19
 
 9/3/13                                                                                                Count outcomes
              Example: Leek Group Website Traffic
              http://biostat.jhsph.edu/~jleek/
                                                                                                                     5/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     5/19
 
 9/3/13                                                                                                Count outcomes
              Website data
                 download.file(""https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda"",destfile=""./data/gaData.rda""
                 load(""./data/gaData.rda"")
                 gaData$julian <- julian(gaData$date)
                 head(gaData)
                                 date visits simplystats julian
                 1 2011-01-01                        0                    0 14975
                 2 2011-01-02                        0                    0 14976
                 3 2011-01-03                        0                    0 14977
                 4 2011-01-04                        0                    0 14978
                 5 2011-01-05                        0                    0 14979
                 6 2011-01-06                        0                    0 14980
              http://skardhamar.github.com/rga/
                                                                                                                     6/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     6/19
 
 9/3/13                                                                                                Count outcomes
              Plot data
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                                                                                                                     7/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     7/19
 
 9/3/13                                                                                                Count outcomes
              Linear regression
                                                                                      N Hi = b 0 + b 1 J D i + e i
              NH i      - number of hits to the website
              JD i     - day of the year (Julian day)
              b0    - number of hits on Julian day 0 (1970-01-01)
              b1    - increase in number of hits per unit day
              ei   - variation due to everything we didn't measure
                                                                                                                     8/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     8/19
 
 9/3/13                                                                                                Count outcomes
              Linear regression line
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                 lm1 <- lm(gaData$visits ~ gaData$julian)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                     9/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                     9/19
 
 9/3/13                                                                                                Count outcomes
              Linear vs. Poisson regression
              Linear
                                                                                      N Hi = b 0 + b 1 J D i + e i
              or
                                                                              E[N H i |J Di , b 0 , b 1 ] = b 0 + b 1 J Di
              Poisson/log-linear
                                                                          log (E[NH i |JD i , b 0 , b 1 ]) = b 0 + b 1 J Di
              or
                                                                         E[N H i |J D i , b 0 , b 1 ] = exp ( b 0 + b 1 J Di )
                                                                                                                               10/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                10/19
 
 9/3/13                                                                                                 Count outcomes
              Multiplicative differences
                                                                         E[N H i |J D i , b 0 , b 1 ] = exp ( b 0 + b 1 J Di )
                                                                      E[N H i |J Di , b 0 , b 1 ] = exp ( b 0 ) exp ( b 1 J Di )
              If JD is increased by one unit, E[N H |JD , b
                        i                                                        i      i     0,  b1 ] is multiplied by exp(b   1)
                                                                                                                                   11/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                    11/19
 
 9/3/13                                                                                                Count outcomes
              Poisson regression in R
                 plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                 glm1 <- glm(gaData$visits ~ gaData$julian,family=""poisson"")
                 abline(lm1,col=""red"",lwd=3); lines(gaData$julian,glm1$fitted,col=""blue"",lwd=3)
                                                                                                                     12/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      12/19
 
 9/3/13                                                                                                Count outcomes
              Mean-variance relationship?
                 plot(glm1$fitted,glm1$residuals,pch=19,col=""grey"",ylab=""Residuals"",xlab=""Date"")
                                                                                                                     13/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      13/19
 
 9/3/13                                                                                                Count outcomes
              Model agnostic standard errors
                 library(sandwich)
                 confint.agnostic <- function (object, parm, level = 0.95, ...)
                 {
                          cf <- coef(object); pnames <- names(cf)
                          if (missing(parm))
                                 parm <- pnames
                          else if (is.numeric(parm))
                                 parm <- pnames[parm]
                          a <- (1 - level)/2; a <- c(a, 1 - a)
                          pct <- stats:::format.perc(a, 3)
                          fac <- qnorm(a)
                          ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                                                                                     pct))
                          ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
                          ci[] <- cf[parm] + ses %o% fac
                          ci
                 }
              http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval
                                                                                                                           14/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                            14/19
 
 9/3/13                                                                                                Count outcomes
              Estimating confidence intervals
                 confint(glm1)
                                                     2.5 %            97.5 %
                 (Intercept) -34.34658 -31.159716
                 gaData$julian 0.00219 0.002396
                 confint.agnostic(glm1)
                                                                                                                     15/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      15/19
 
 9/3/13                                                                                                Count outcomes
              Rates
                                                                   E[NH S S i |J Di , b 0 , b 1 ]/N H i = exp ( b 0 + b 1 J D i )
                                                             log (E[NH S S i |JD i , b 0 , b 1 ])       J    log(N Hi ) = b 0 + b 1 J Di
                                                             log (E[NH S S i |JD i , b 0 , b 1 ]) = log(N Hi ) + b 0 + b 1 J Di
                                                                                                                                         16/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                                          16/19
 
 9/3/13                                                                                                Count outcomes
              Fitting rates in R
                 glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
                                        family=""poisson"",data=gaData)
                 plot(julian(gaData$date),glm2$fitted,col=""blue"",pch=19,xlab=""Date"",ylab=""Fitted Counts"")
                 points(julian(gaData$date),glm1$fitted,col=""red"",pch=19)
                                                                                                                     17/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      17/19
 
 9/3/13                                                                                                Count outcomes
              Fitting rates in R
                 glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
                                        family=""poisson"",data=gaData)
                 plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col=""grey"",xlab=""Date"",
                            ylab=""Fitted Rates"",pch=19)
                 lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col=""blue"",lwd=3)
                                                                                                                     18/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      18/19
 
 9/3/13                                                                                                Count outcomes
              More information
                 · Log-linear models and multiway tables
                 · Wikipedia on Poisson regression, Wikipedia on overdispersion
                 · Regression models for count data in R
                 · pscl package - the function zeroinfl fits zero inflated models.
                                                                                                                     19/19
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/003countOutcomes/index.html#1                      19/19
"
"./07_RegressionModels/originalContent/003inferenceBasics/Inference basics.pdf","8/29/13                                                                                                 Inference basics
                            Inference basics
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                  1/21
 
 8/29/13                                                                                                 Inference basics
              Fit a line to the Galton Data
                 library(UsingR); data(galton);
                 plot(galton$parent,galton$child,pch=19,col=""blue"")
                 lm1 <- lm(galton$child ~ galton$parent)
                 lines(galton$parent,lm1$fitted,col=""red"",lwd=3)
                                                                                                                         2/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       2/21
 
 8/29/13                                                                                                 Inference basics
              Fit a line to the Galton Data
                 lm1
                 Call:
                 lm(formula = galton$child ~ galton$parent)
                 Coefficients:
                    (Intercept) galton$parent
                               23.942                         0.646
                                                                                                                         3/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       3/21
 
 8/29/13                                                                                                 Inference basics
              Create a ""population"" of 1 million families
                 newGalton <- data.frame(parent=rep(NA,1e6),child=rep(NA,1e6))
                 newGalton$parent <- rnorm(1e6,mean=mean(galton$parent),sd=sd(galton$parent))
                 newGalton$child <- lm1$coeff[1] + lm1$coeff[2]*newGalton$parent + rnorm(1e6,sd=sd(lm1$residuals))
                 smoothScatter(newGalton$parent,newGalton$child)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                         4/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       4/21
 
 8/29/13                                                                                                 Inference basics
              Let's take a sample
                 set.seed(134325); sampleGalton1 <- newGalton[sample(1:1e6,size=50,replace=F),]
                 sampleLm1 <- lm(sampleGalton1$child ~ sampleGalton1$parent)
                 plot(sampleGalton1$parent,sampleGalton1$child,pch=19,col=""blue"")
                 lines(sampleGalton1$parent,sampleLm1$fitted,lwd=3,lty=2)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                         5/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       5/21
 
 8/29/13                                                                                                 Inference basics
              Let's take another sample
                 sampleGalton2 <- newGalton[sample(1:1e6,size=50,replace=F),]
                 sampleLm2 <- lm(sampleGalton2$child ~ sampleGalton2$parent)
                 plot(sampleGalton2$parent,sampleGalton2$child,pch=19,col=""blue"")
                 lines(sampleGalton2$parent,sampleLm2$fitted,lwd=3,lty=2)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                         6/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       6/21
 
 8/29/13                                                                                                 Inference basics
              Let's take another sample
                 sampleGalton3 <- newGalton[sample(1:1e6,size=50,replace=F),]
                 sampleLm3 <- lm(sampleGalton3$child ~ sampleGalton3$parent)
                 plot(sampleGalton3$parent,sampleGalton3$child,pch=19,col=""blue"")
                 lines(sampleGalton3$parent,sampleLm3$fitted,lwd=3,lty=2)
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                         7/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       7/21
 
 8/29/13                                                                                                 Inference basics
              Many samples
                 sampleLm <- vector(100,mode=""list"")
                 for(i in 1:100){
                    sampleGalton <- newGalton[sample(1:1e6,size=50,replace=F),]
                    sampleLm[[i]] <- lm(sampleGalton$child ~ sampleGalton$parent)
                 }
                                                                                                                         8/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       8/21
 
 8/29/13                                                                                                 Inference basics
              Many samples
                 smoothScatter(newGalton$parent,newGalton$child)
                 for(i in 1:100){abline(sampleLm[[i]],lwd=3,lty=2)}
                 abline(lm1,col=""red"",lwd=3)
                                                                                                                         9/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                       9/21
 
 8/29/13                                                                                                 Inference basics
              Histogram of estimates
                 par(mfrow=c(1,2))
                 hist(sapply(sampleLm,function(x){coef(x)[1]}),col=""blue"",xlab=""Intercept"",main="""")
                 hist(sapply(sampleLm,function(x){coef(x)[2]}),col=""blue"",xlab=""Slope"",main="""")
                                                                                                                         10/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        10/21
 
 8/29/13                                                                                                 Inference basics
              Distribution of coefficients
              From the central limit theorem it turns out that in many cases:
                                                                                         ė
                                                                                         b0    t                     ė
                                                                                                    N( b 0 , V ar( b 0 ))
                                                                                         ė
                                                                                         b1    t                     ė
                                                                                                    N( b 0 , V ar( b 1 ))
              which we can estimate with:
                                                                                         ė
                                                                                         b0    ¡               ė ė
                                                                                                    N( b 0 , V ar ( b 0 ))
                                                                                         ė
                                                                                         b1    ¡               ė ė
                                                                                                    N( b 0 , V ar ( b 1 ))
                   _ _ė____   ė __
              √
                   V ar b   (   0)  is the ""standard error"" of the estimate bė and is abbreviated S. E. (bė
                                                                                                          0                0)
                                                                                                                              11/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                             11/21
 
 8/29/13                                                                                                 Inference basics
              Estimating the values in R
                 sampleGalton4 <- newGalton[sample(1:1e6,size=50,replace=F),]
                 sampleLm4 <- lm(sampleGalton4$child ~ sampleGalton4$parent)
                 summary(sampleLm4)
                 Call:
                 lm(formula = sampleGalton4$child ~ sampleGalton4$parent)
                 Residuals:
                       Min            1Q Median                   3Q        Max
                 -4.360 -1.610 -0.289 2.020 4.387
                 Coefficients:
                                                          Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                                  15.863              11.773             1.35          0.18
                 sampleGalton4$parent                           0.770              0.174             4.43 5.4e-05 ***
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                 Residual standard error: 2.29 on 48 degrees of freedom
                 Multiple R-squared: 0.291, Adjusted R-squared: 0.276
                 F-statistic: 19.7 on 1 and 48 DF, p-value: 5.36e-05
                                                                                                                         12/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        12/21
 
 8/29/13                                                                                                 Inference basics
              Estimating the values in R
                 hist(sapply(sampleLm,function(x){coef(x)[2]}),col=""blue"",xlab=""Slope"",main="""",freq=F)
                 lines(seq(0,5,length=100),dnorm(seq(0,5,length=100),mean=coef(sampleLm4)[2],
                              sd=summary(sampleLm4)$coeff[2,2]),lwd=3,col=""red"")
                                                                                                                         13/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        13/21
 
 8/29/13                                                                                                   Inference basics
              Why do we standardize?
                                                                                            K
                                                                                               P  = C
                                                                                                        P  + 273.15
                                                                                                        P
                                                                                           K
                                                                                              P  =
                                                                                                      F    + 459.67
                                                                                                              1.8
              http://en.wikipedia.org/wiki/Kelvin
                                                                                                                           14/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                          14/21
 
 8/29/13                                                                                                 Inference basics
              Why do we standardize?
                 par(mfrow=c(1,2))
                 hist(sapply(sampleLm,function(x){coef(x)[1]}),col=""blue"",xlab=""Intercept"",main="""")
                 hist(sapply(sampleLm,function(x){coef(x)[2]}),col=""blue"",xlab=""Slope"",main="""")
                                                                                                                         15/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        15/21
 
 8/29/13                                                                                                   Inference basics
              Standardized coefficients
                                                                                       ė ¡
                                                                                      b0                          ė      ė
                                                                                                   N ( b 0 , V ar ( b 0 ))
                                                                                       ė ¡
                                                                                      b1                          ė      ė
                                                                                                   N ( b 0 , V ar ( b 1 ))
              and
                                                                                              ė
                                                                                             b0    J    b0
                                                                                                                t        J
                                                                                            S. E. (b
                                                                                                        ė 0)
                                                                                                                     tn    2
                                                                                              ė
                                                                                             b1    J    b1
                                                                                                                t        J
                                                                                            S. E. (b 1 )
                                                                                                        ė            tn    2
              Degrees of Freedom                       ¡ number of samples - number of things you estimated.
                                                                                                                             16/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                            16/21
 
 8/29/13                                                                                                 Inference basics
              tn     J      2     versus N (0, 1)
                 x <- seq(-5,5,length=100)
                 plot(x,dnorm(x),type=""l"",lwd=3)
                 lines(x,dt(x,df=3),lwd=3,col=""red"")
                 lines(x,dt(x,df=10),lwd=3,col=""blue"")
                                                                                                                         17/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        17/21
 
 8/29/13                                                                                                 Inference basics
              Confidence intervals
              We have an estimate bė 1 and we want to know something about how good our estimate is.
              One way is to create a ""level                              confidence interval"".
              A confidence interval will include the real parameter                                             percent of the time in repeated studies.
                                                                                                                                                         18/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                                                        18/21
 
 8/29/13                                                                                                 Inference basics
              Confidence intervals
                                                                      ė
                                                                    (b 1 + T       /2
                                                                                                     ė
                                                                                      × S. E. (b 1 ), b 1
                                                                                                           ė   J   T    /2
                                                                                                                                    ė
                                                                                                                           × S. E. (b 1 ))
                 summary(sampleLm4)$coeff
                                                          Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                                15.8632             11.7726 1.347 1.842e-01
                 sampleGalton4$parent 0.7698                                      0.1736 4.434 5.364e-05
                 confint(sampleLm4,level=0.95)
                                                              2.5 % 97.5 %
                 (Intercept)                              -7.8072 39.534
                 sampleGalton4$parent 0.4208 1.119
                                                                                                                                           19/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                                          19/21
 
 8/29/13                                                                                                 Inference basics
              Confidence intervals
                 par(mar=c(4,4,0,2));plot(1:10,type=""n"",xlim=c(0,1.5),ylim=c(0,100),
                                                                  xlab=""Coefficient Values"",ylab=""Replication"")
                 for(i in 1:100){
                          ci <- confint(sampleLm[[i]]); color=""red"";
                          if((ci[2,1] < lm1$coeff[2]) & (lm1$coeff[2] < ci[2,2])){color = ""grey""}
                          segments(ci[2,1],i,ci[2,2],i,col=color,lwd=3)
                 }
                 lines(rep(lm1$coeff[2],100),seq(0,100,length=100),lwd=3)
                                                                                                                         20/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        20/21
 
 8/29/13                                                                                                 Inference basics
              How you report the inference
                 sampleLm4$coeff
                                   (Intercept) sampleGalton4$parent
                                           15.8632                                 0.7698
                 confint(sampleLm4,level=0.95)
                                                              2.5 % 97.5 %
                 (Intercept)                              -7.8072 39.534
                 sampleGalton4$parent 0.4208 1.119
              A one inch increase in parental height is associated with a 0.77 inch increase in child's height (95%
              CI: 0.42-1.12 inches).
                                                                                                                         21/21
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/003inferenceBasics/index.html#1                        21/21
"
"./07_RegressionModels/originalContent/004modelChecking/Model checking and model selection.pdf","9/3/13                                                                                         Model checking and model selection
                            Model checking and model selection
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                             1/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking and model selection
                 · Sometimes model checking/selection not allowed
                 · Often it can lead to problems
                          - Overfitting
                          - Overtesting
                          - Biased inference
                 · But you don't want to miss something obvious
                                                                                                                                  2/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  2/25
 
 9/3/13                                                                                         Model checking and model selection
              Linear regression - basic assumptions
                 · Variance is constant
                 · You are summarizing a linear trend
                 · You have all the right terms in the model
                 · There are no big outliers
                                                                                                                                  3/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  3/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - constant variance
                 set.seed(3433); par(mfrow=c(1,2))
                 data <- rnorm(100,mean=seq(0,3,length=100),sd=seq(0.1,3,length=100))
                 lm1 <- lm(data ~ seq(0,3,length=100))
                 plot(seq(0,3,length=100),data,pch=19,col=""grey""); abline(lm1,col=""red"",lwd=3)
                 plot(seq(0,3,length=100),lm1$residuals,,pch=19,col=""grey""); abline(c(0,0),col=""red"",lwd=3)
                                                                                                                                  4/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  4/25
 
 9/3/13                                                                                         Model checking and model selection
              What to do
                 · See if another variable explains the increased variance
                 · Use the vcovHC {sandwich} variance estimators (if n is big)
                                                                                                                                  5/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  5/25
 
 9/3/13                                                                                         Model checking and model selection
              Using the sandwich estimate
                 set.seed(3433); par(mfrow=c(1,2)); data <- rnorm(100,mean=seq(0,3,length=100),sd=seq(0.1,3,length=100))
                 lm1 <- lm(data ~ seq(0,3,length=100))
                 vcovHC(lm1)
                 summary(lm1)$cov.unscaled
                                                                (Intercept) seq(0, 3, length = 100)
                 (Intercept)                                          0.03941                                   -0.01960
                 seq(0, 3, length = 100)                             -0.01960                                     0.01307
                                                                                                                                  6/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  6/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - linear trend
                 set.seed(3433); par(mfrow=c(1,2))
                 data <- rnorm(100,mean=seq(0,3,length=100)^3,sd=2)
                 lm1 <- lm(data ~ seq(0,3,length=100))
                 plot(seq(0,3,length=100),data,pch=19,col=""grey""); abline(lm1,col=""red"",lwd=3)
                 plot(seq(0,3,length=100),lm1$residuals,,pch=19,col=""grey""); abline(c(0,0),col=""red"",lwd=3)
                                                                                                                                  7/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  7/25
 
 9/3/13                                                                                         Model checking and model selection
              What to do
                 · Use Poisson regression (if it looks exponential/multiplicative)
                 · Use a data transformation (e.g. take the log)
                 · Smooth the data/fit a nonlinear trend (next week's lectures)
                 · Use linear regression anyway
                          - Interpret as the linear trend between the variables
                          - Use the vcovHC {sandwich} variance estimators (if n is big)
                                                                                                                                  8/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  8/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - missing covariate
                 set.seed(3433); par(mfrow=c(1,3)); z <- rep(c(-0.5,0.5),50)
                 data <- rnorm(100,mean=(seq(0,3,length=100) + z),sd=seq(0.1,3,length=100))
                 lm1 <- lm(data ~ seq(0,3,length=100))
                 plot(seq(0,3,length=100),data,pch=19,col=((z>0)+3)); abline(lm1,col=""red"",lwd=3)
                 plot(seq(0,3,length=100),lm1$residuals,pch=19,col=((z>0)+3)); abline(c(0,0),col=""red"",lwd=3)
                 boxplot(lm1$residuals ~ z,col = ((z>0)+3) )
                                                                                                                                  9/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                  9/25
 
 9/3/13                                                                                         Model checking and model selection
              What to do
                 · Use exploratory analysis to identify other variables to include
                 · Use the vcovHC {sandwich} variance estimators (if n is big)
                 · Report unexplained patterns in the data
                                                                                                                                  10/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   10/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - outliers
                 set.seed(343); par(mfrow=c(1,2)); betahat <- rep(NA,100)
                 x <- seq(0,3,length=100); y <- rcauchy(100); lm1 <- lm(y ~ x)
                 plot(x,y,pch=19,col=""blue""); abline(lm1,col=""red"",lwd=3)
                 for(i in 1:length(data)){betahat[i] <- lm(y[-i] ~ x[-i])$coeff[2]}
                 plot(betahat - lm1$coeff[2],col=""blue"",pch=19); abline(c(0,0),col=""red"",lwd=3)
                                                                                                                                  11/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   11/25
 
 9/3/13                                                                                         Model checking and model selection
              What to do
                 · If outliers are experimental mistakes -remove and document them
                 · If they are real - consider reporting how sensitive your estimate is to the outliers
                 · Consider using a robust linear model fit like rlm {MASS}
                                                                                                                                  12/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   12/25
 
 9/3/13                                                                                         Model checking and model selection
              Robust linear modeling
                 set.seed(343); x <- seq(0,3,length=100); y <- rcauchy(100);
                 lm1 <- lm(y ~ x); rlm1 <- rlm(y ~ x)
                 lm1$coeff
                 (Intercept)                                x
                            0.3523               -0.4011
                 rlm1$coeff
                 (Intercept)                                x
                       0.008527              -0.017892
                                                                                                                                  13/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   13/25
 
 9/3/13                                                                                         Model checking and model selection
              Robust linear modeling
                 par(mfrow=c(1,2))
                 plot(x,y,pch=19,col=""grey"")
                 lines(x,lm1$fitted,col=""blue"",lwd=3); lines(x,rlm1$fitted,col=""green"",lwd=3)
                 plot(x,y,pch=19,col=""grey"",ylim=c(-5,5),main=""Zoomed In"")
                 lines(x,lm1$fitted,col=""blue"",lwd=3); lines(x,rlm1$fitted,col=""green"",lwd=3)
                                                                                                                                  14/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   14/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - default plots
                 set.seed(343); par(mfrow=c(1,2))
                 x <- seq(0,3,length=100); y <- rnorm(100); lm1 <- lm(y ~ x)
                 plot(lm1)
                                                                                                                                  15/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   15/25
 
 9/3/13                                                                                         Model checking and model selection
              Model checking - deviance
                 · Commonly reported for GLM's
                 · Usually compares the model where every point gets its own parameter to the model you are using
                 · On it's own it doesn't tell you what is wrong
                 · In large samples the deviance may be big even for ""conservative"" models
                 · You can not compare deviances for models with different sample sizes
                                                                                                                                  16/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   16/25
 
 9/3/13                                                                                         Model checking and model selection
              R
                     2
                             may be a bad summary
                                                                                                                                  17/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   17/25
 
 9/3/13                                                                                         Model checking and model selection
              Model selection
                 · Many times you have multiple variables to evaluate
                 · Options for choosing variables
                          - Domain-specific knowledge
                          - Exploratory analysis
                          - Statistical selection
                 · There are many statistical selection options
                          - Step-wise
                          - AIC
                          - BIC
                          - Modern approaches: Lasso, Ridge-Regression, etc.
                 · Statistical selection may bias your inference
                          - If possible, do selection on a held out sample
                                                                                                                                  18/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   18/25
 
 9/3/13                                                                                         Model checking and model selection
              Error measures
                        2                                                                                   2
                 ·  R       alone isn't enough - more variables = bigger R
                                         2          2
                 · Adjusted R is R taking into account the number of estimated parameters
                 · AIC also penalizes models with more parameters
                 · BIC does the same, but with a bigger penalty
                                                                                                                                  19/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   19/25
 
 9/3/13                                                                                         Model checking and model selection
              Movie Data
                 download.file(""http://www.rossmanchance.com/iscam2/data/movies03RT.txt"",destfile=""./data/movies.txt"")
                 movies <- read.table(""./data/movies.txt"",sep=""\t"",header=T,quote="""")
                 head(movies)
                                                  X score rating                                  genre box.office running.time
                 1 2 Fast 2 Furious 48.9 PG-13 action/adventure                                                    127.15         107
                 2          28 Days Later 78.2                            R                     horror               45.06        113
                 3             A Guy Thing 39.5 PG-13                                   rom comedy                   15.54        101
                 4             A Man Apart 42.9                           R action/adventure                         26.25        110
                 5          A Mighty Wind 79.9 PG-13                                            comedy               17.78         91
                 6 Agent Cody Banks 57.9                                PG action/adventure                          47.81        102
              http://www.rossmanchance.com/
                                                                                                                                      20/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                       20/25
 
 9/3/13                                                                                         Model checking and model selection
              Model selection - step
                 movies <- movies[,-1]
                 lm1 <- lm(score ~ .,data=movies)
                 aicFormula <- step(lm1)
                 Start: AIC=727.5
                 score ~ rating + genre + box.office + running.time
                                               Df Sum of Sq RSS AIC
                 - genre                       12             2575 22132 721
                 - rating                        3                40 19596 722
                 - running.time 1                               237 19793 727
                 <none>                                               19556 728
                 - box.office                    1            3007 22563 746
                 Step: AIC=720.8
                 score ~ rating + box.office + running.time
                                               Df Sum of Sq RSS AIC
                 - rating                        3              491 22623 718
                 <none>                                               22132 721
                 - running.time 1                             1192 23324 726
                 - box.office                    1            2456 24588 734
                                                                                                                                  21/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   21/25
 
 9/3/13                                                                                         Model checking and model selection
              Model selection - step
                 aicFormula
                 Call:
                 lm(formula = score ~ box.office + running.time, data = movies)
                 Coefficients:
                  (Intercept)                    box.office running.time
                            37.2364                     0.0824                   0.1275
                                                                                                                                  22/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   22/25
 
 9/3/13                                                                                         Model checking and model selection
              Model selection - regsubsets
                 library(leaps);
                 regSub <- regsubsets(score ~ .,data=movies)
                 plot(regSub)
              http://cran.r-project.org/web/packages/leaps/leaps.pdf
                                                                                                                                  23/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   23/25
 
 9/3/13                                                                                         Model checking and model selection
              Model selection - bic.glm
                 library(BMA)
                 bicglm1 <- bic.glm(score ~.,data=movies,glm.family=""gaussian"")
                 print(bicglm1)
                 Call:
                 bic.glm.formula(f = score ~ ., data = movies, glm.family = ""gaussian"")
                  Posterior probabilities(%):
                  <NA> <NA> <NA> <NA>
                    0.0 100.0 100.0 18.2
                  Coefficient posterior expected values:
                                    (Intercept)                                     ratingPG                             ratingPG-13              ratingR
                                               45.263                                     0.000                                    0.000            0.000
                 genreaction/adventure                                      genreanimated                                genrecomedy     genredocumentary
                                               -0.120                                     7.628                                    2.077            8.642
                                      genredrama                             genrefantasy                                genrehorror         genremusical
                                               13.041                                     1.504                                   -3.458          -12.255
                              genrerom comedy                                  genresci-fi                           genresuspense           genrewestern
                                                 1.244                                  -3.324                                     3.815           17.563
                                      box.office                             running.time                                                                 24/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                                           24/25
 
 9/3/13                                                                                         Model checking and model selection
              Notes and further resources
                 · Exploratory/visual analysis is key
                 · Automatic selection produces an answer - but may bias inference
                 · You may think about separating the sample into two groups
                 · The goal is not to get the ""causal"" model
                 · Lars package
                 · Elements of machine learning
                                                                                                                                  25/25
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week5/004modelChecking/index.html#1                                   25/25
"
"./07_RegressionModels/originalContent/006multipleVariables/Multiple regression.pdf","8/30/13                                                                                                   Multiple regression
                            Multiple regression
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                     1/16
 
 8/30/13                                                                                                   Multiple regression
              Key ideas
                 · Regression with multiple covariates
                 · Still using least squares/central limit theorem
                 · Interpretation depends on all variables
                                                                                                                              2/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          2/16
 
 8/30/13                                                                                                   Multiple regression
              Example - Millenium Development Goal 1
              http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf
              http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:;SEX:
                                                                                                                              3/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          3/16
 
 8/30/13                                                                                                   Multiple regression
              WHO childhood hunger data
                 download.file(""http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*
                 hunger <- read.csv(""./data/hunger.csv"")
                 hunger <- hunger[hunger$Sex!=""Both sexes"",]
                 head(hunger)
                                                                             Indicator Data.Source PUBLISH.STATES Year                                  WHO.region
                 2 Children aged <5 years underweight (%) NLIS_312819                                                         Published 2004 Eastern Mediterranean
                 3 Children aged <5 years underweight (%) NLIS_312819                                                         Published 2004 Eastern Mediterranean
                 6 Children aged <5 years underweight (%) NLIS_312361                                                         Published 2000                Europe
                 7 Children aged <5 years underweight (%) NLIS_312361                                                         Published 2000                Europe
                 9 Children aged <5 years underweight (%) NLIS_312879                                                         Published 2005                Europe
                 10 Children aged <5 years underweight (%) NLIS_312879                                                        Published 2005                Europe
                               Country               Sex Display.Value Numeric Low High Comments
                 2 Afghanistan Female                                       33.0           33.0 NA NA                         NA
                 3 Afghanistan Male                                         32.7           32.7 NA NA                         NA
                 6             Albania Male                                 19.6           19.6 NA NA                         NA
                 7             Albania Female                               14.2           14.2 NA NA                         NA
                 9             Albania Male                                  7.3              7.3 NA NA                       NA
                 10            Albania Female                                5.8              5.8 NA NA                       NA
                                                                                                                                                                 4/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                                                             4/16
 
 8/30/13                                                                                                   Multiple regression
              Plot percent hungry versus time
                 lm1 <- lm(hunger$Numeric ~ hunger$Year)
                 plot(hunger$Year,hunger$Numeric,pch=19,col=""blue"")
                                                                                                                              5/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          5/16
 
 8/30/13                                                                                                   Multiple regression
              Remember the linear model
                                                                                         H u i = b 0 + b 1 Yi + e i
              b0    = percent hungry at Year 0
              b1    = decrease in percent hungry per year
              ei   = everything we didn't measure
                                                                                                                              6/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          6/16
 
 8/30/13                                                                                                   Multiple regression
              Add the linear model
                 lm1 <- lm(hunger$Numeric ~ hunger$Year)
                 plot(hunger$Year,hunger$Numeric,pch=19,col=""blue"")
                 lines(hunger$Year,lm1$fitted,lwd=3,col=""darkgrey"")
                                                                                                                              7/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          7/16
 
 8/30/13                                                                                                   Multiple regression
              Color by male/female
                 plot(hunger$Year,hunger$Numeric,pch=19)
                 points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
                                                                                                                              8/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          8/16
 
 8/30/13                                                                                                   Multiple regression
              Now two lines
                                                                                 HuFi = bf 0 + bf 1 Y Fi + ef i
              bf 0     = percent of girls hungry at Year 0
              bf 1     = decrease in percent of girls hungry per year
              ef i   = everything we didn't measure
                                                                            HuM i = bm0 + bm1 Y M i + emi
              bm0         = percent of boys hungry at Year 0
              bm1         = decrease in percent of boys hungry per year
              emi       = everything we didn't measure
                                                                                                                              9/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                          9/16
 
 8/30/13                                                                                                   Multiple regression
              Color by male/female
                 lmM <- lm(hunger$Numeric[hunger$Sex==""Male""] ~ hunger$Year[hunger$Sex==""Male""])
                 lmF <- lm(hunger$Numeric[hunger$Sex==""Female""] ~ hunger$Year[hunger$Sex==""Female""])
                 plot(hunger$Year,hunger$Numeric,pch=19)
                 points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
                 lines(hunger$Year[hunger$Sex==""Male""],lmM$fitted,col=""black"",lwd=3)
                 lines(hunger$Year[hunger$Sex==""Female""],lmF$fitted,col=""red"",lwd=3)
                                                                                                                              10/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                           10/16
 
 8/30/13                                                                                                   Multiple regression
              Two lines, same slope
                                                               Hui = b0 + b1                Ċ(Sex i        ="" Male "") + b 2 Y i + e
                                                                                                                                    O
                                                                                                                                    i
              b0     - percent hungry at year zero for females
              b0 + b1           - percent hungry at year zero for males
              b2     - change in percent hungry (for either males or females) in one year
              e
                 O
                 i
                    - everything we didn't measure
                                                                                                                                      11/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                                   11/16
 
 8/30/13                                                                                                   Multiple regression
              Two lines, same slope in R
                 lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
                 plot(hunger$Year,hunger$Numeric,pch=19)
                 points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
                 abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col=""red"",lwd=3)
                 abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] ),col=""black"",lwd=3)
                                                                                                                              12/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                           12/16
 
 8/30/13                                                                                                   Multiple regression
              Two lines, different slopes (interactions)
                                         H u i = b0 + b1           Ċ(Sex    i   ="" Male "") + b 2 Yi + b 3                  Ċ(Sex i ="" Male "") × Y i + e
                                                                                                                                                        +
                                                                                                                                                        i
              b0    - percent hungry at year zero for females
              b0 + b 1         - percent hungry at year zero for males
              b2    - change in percent hungry (females) in one year
              b2 + b 3         - change in percent hungry (males) in one year
                 +
              e
                 i
                    - everything we didn't measure
                                                                                                                                                          13/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                                                       13/16
 
 8/30/13                                                                                                   Multiple regression
              Two lines, different slopes in R
                 lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex*hunger$Year)
                 plot(hunger$Year,hunger$Numeric,pch=19)
                 points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
                 abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col=""red"",lwd=3)
                 abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] +lmBoth$coeff[4]),col=""black"",lwd=3)
                                                                                                                              14/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                           14/16
 
 8/30/13                                                                                                   Multiple regression
              Two lines, different slopes in R
                 summary(lmBoth)
                 Call:
                 lm(formula = hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex *
                          hunger$Year)
                 Residuals:
                       Min            1Q Median                   3Q        Max
                 -25.09 -11.61 -2.15                          7.29 46.15
                 Coefficients:
                                                                     Estimate Std. Error t value Pr(>|t|)
                 (Intercept)                                         522.4721 191.8987                           2.72 0.0066 **
                 hunger$Year                                          -0.2527                 0.0959 -2.63 0.0086 **
                 hunger$SexMale                                       58.3730 271.3858                           0.22 0.8297
                 hunger$Year:hunger$SexMale -0.0282                                           0.1357 -0.21 0.8353
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                 Residual standard error: 13.5 on 854 degrees of freedom
                 Multiple R-squared: 0.0228,                                 Adjusted R-squared: 0.0193
                 F-statistic: 6.64 on 3 and 854 DF, p-value: 0.000197                                                           15/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                             15/16
 
 8/30/13                                                                                                   Multiple regression
              Interactions for continuous variables
                                                                                                                              +
                                                                     H u i = b 0 + b 1 I n i + b 2 Yi + b 3 I n i × Yi + e
                                                                                                                              i
              b0    - percent hungry at year zero for children with whose parents have no income
              b1    - change in percent hungry for each dollar of income in year zero
              b2    - change in percent hungry in one year for children whose parents have no income
              b3    - increased change in percent hungry by year for each dollar of income - e.g. if income is $10,000,
              then change in percent hungry in one year will be
                                                                                                b 2 + 1e4 × b 3
                 +
              e
                 i
                    - everything we didn't measure
              Lot's of care/caution needed!
                                                                                                                                16/16
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/006multipleVariables/index.html#1                             16/16
"
"./07_RegressionModels/originalContent/007realData/Regression in the real world.pdf","8/30/13                                                                                          Regression in the real world
                            Regression in the real world
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                              1/30
 
 8/30/13                                                                                          Regression in the real world
              Things to pay attention to
                 · Confounders
                 · Complicated interactions
                 · Skewness
                 · Outliers
                 · Non-linear patterns
                 · Variance changes
                 · Units/scale issues
                 · Overloading regression
                 · Correlation and causation
                                                                                                                              2/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   2/30
 
 8/30/13                                                                                          Regression in the real world
              The ideal data for regression
                 library(UsingR); data(galton)
                 plot(galton$parent,galton$child,col=""blue"",pch=19)
                                                                                                                              3/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   3/30
 
 8/30/13                                                                                          Regression in the real world
              Confounders
              Confounder: A variable that is correlated with both the outcome and the covariates
                 · Confounders can change the regression line
                 · They can even change the sign of the line
                 · They can sometimes be detected by careful exploration
                                                                                                                              4/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   4/30
 
 8/30/13                                                                                          Regression in the real world
              Example - Millenium Development Goal 1
              http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf
                                                                                                                              5/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   5/30
 
 8/30/13                                                                                          Regression in the real world
              WHO childhood hunger data
                 download.file(""http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filt
                                             er=COUNTRY:*;SEX:*"",""./data/hunger.csv"",method=""curl"")
                 hunger <- read.csv(""./data/hunger.csv"")
                 hunger <- hunger[hunger$Sex!=""Both sexes"",]
                 head(hunger)
                                                                              Indicator Data.Source PUBLISH.STATES Year                                 WHO.region
                 2 Children aged <5 years underweight (%) NLIS_312819                                                         Published 2004 Eastern Mediterranean
                 3 Children aged <5 years underweight (%) NLIS_312819                                                         Published 2004 Eastern Mediterranean
                 6 Children aged <5 years underweight (%) NLIS_312361                                                         Published 2000                Europe
                 7 Children aged <5 years underweight (%) NLIS_312361                                                         Published 2000                Europe
                 9 Children aged <5 years underweight (%) NLIS_312879                                                         Published 2005                Europe
                 10 Children aged <5 years underweight (%) NLIS_312879                                                        Published 2005                Europe
                               Country               Sex Display.Value Numeric Low High Comments
                 2 Afghanistan Female                                       33.0           33.0 NA NA                         NA
                 3 Afghanistan Male                                         32.7           32.7 NA NA                         NA
                 6             Albania Male                                 19.6           19.6 NA NA                         NA
                 7             Albania Female                               14.2           14.2 NA NA                         NA
                 9             Albania Male                                   7.3            7.3 NA NA                        NA
                 10            Albania Female                                 5.8            5.8 NA NA                        NA
                                                                                                                                                                 6/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                                                      6/30
 
 8/30/13                                                                                          Regression in the real world
              Hunger over time by region
                 par(mfrow=c(1,2))
                 plot(hunger$Year,hunger$Numeric,col=as.numeric(hunger$WHO.region),pch=19)
                 plot(1:10,type=""n"",xaxt=""n"",yaxt=""n"",xlab="""",ylab="""")
                 legend(1,10,col=unique(as.numeric(hunger$WHO.region)),legend=unique(hunger$WHO.region),pch=19)
                                                                                                                              7/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   7/30
 
 8/30/13                                                                                          Regression in the real world
              Region correlated with year
                 anova(lm(hunger$Year ~ hunger$WHO.region))
                 Analysis of Variance Table
                 Response: hunger$Year
                                                      Df Sum Sq Mean Sq F value Pr(>F)
                 hunger$WHO.region 5                              538 107.5                  2.33 0.041 *
                 Residuals                           852 39316                46.1
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                                                                                                                              8/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                   8/30
 
 8/30/13                                                                                           Regression in the real world
              Region correlated with hunger
                 anova(lm(hunger$Numeric ~ hunger$WHO.region))
                 Analysis of Variance Table
                 Response: hunger$Numeric
                                                      Df Sum Sq Mean Sq F value Pr(>F)
                 hunger$WHO.region 5 76032 15206                                               154 <2e-16 ***
                 Residuals                           852 84211                   99
                 ---
                 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                                                                                                                               9/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    9/30
 
 8/30/13                                                                                          Regression in the real world
              Including region - a complicated interaction
                 plot(hunger$Year,hunger$Numeric,pch=19,col=as.numeric(hunger$WHO.region))
                 lmRegion <- lm(hunger$Numeric ~ hunger$Year + hunger$WHO.region + hunger$Year*hunger$WHO.region )
                 abline(c(lmRegion$coeff[1] + lmRegion$coeff[6],lmRegion$coeff[2]+ lmRegion$coeff[12]),col=5,lwd=3)
                                                                                                                              10/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    10/30
 
 8/30/13                                                                                          Regression in the real world
              Income data
                 download.file(""http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"",""./data/income.csv
                 incomeData <- read.csv(""./data/income.csv"",header=FALSE)
                 income <- incomeData[,3]
                 age <- incomeData[,1]
              http://archive.ics.uci.edu/ml/datasets/Census+Income
                                                                                                                              11/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    11/30
 
 8/30/13                                                                                          Regression in the real world
              Logs to address right-skew
                 par(mfrow=c(1,4))
                 smoothScatter(age,income)
                 hist(income,col=""blue"",breaks=100)
                 hist(log(income+1),col=""blue"",breaks=100)
                 smoothScatter(age,log(income+1))
              (Data transforms)
                                                                                                                              12/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    12/30
 
 8/30/13                                                                                          Regression in the real world
              Outliers
               ""outliers"" ... are data points that do not appear to follow the pattern of the other
                                                                                               data points.
              A dataset that is 44% outliers
                                                                                                                              13/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    13/30
 
 8/30/13                                                                                          Regression in the real world
              Example - extreme points
                 set.seed(1235)
                 xVals <- rcauchy(50)
                 hist(xVals,col=""blue"")
                                                                                                                              14/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    14/30
 
 8/30/13                                                                                          Regression in the real world
              Example - Outliers may be real
                 # Add Tim Cook, CEO of Apple 2011 income
                 age <- c(age,52)
                 income <- c(income,378e6)
                 smoothScatter(age,income)
              http://www.macworld.com/article/2023491/apple-gives-tim-cook-51-percent-salary-increase.html
                                                                                                                              15/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    15/30
 
 8/30/13                                                                                          Regression in the real world
              Example - Does not fit the trend
              A dataset that is 44% outliers
                                                                                                                              16/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    16/30
 
 8/30/13                                                                                          Regression in the real world
              Outliers - what you can do
                 · If you know they aren't real/of interest, remove them (but changes question!)
                 · Alternatively
                          - Sensitivity analysis - is it a big difference if you leave it in/take it out?
                          - Logs - if the data are right skewed (lots of outliers)
                          - Robust methods - we've been doing averages, but there are more robust approaches
                              (Robust,rlm)
                                                                                                                              17/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    17/30
 
 8/30/13                                                                                          Regression in the real world
              A line isn't always the best summary
              http://en.wikipedia.org/wiki/Linear_regression
                                                                                                                              18/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    18/30
 
 8/30/13                                                                                          Regression in the real world
              You can end up saying some pretty silly stuff
              http://www.nature.com/nature/journal/v431/n7008/fig_tab/431525a_F1.html
              ""We are students aged 16–18 in a Texas high school. Our biology teacher Vidya Rajan asked us to
              comment on the paper by A. J. Tatem and colleagues (Nature 431, 525; 2004); we believe the
              projection                     on             which                it           is        based                 is riddled with flaws...""
              http://www.nature.com/nature/journal/v432/n7014/full/432147c.html
              ""They omit to mention, however, that (according to their analysis) a far more interesting race should
              occur in about 2636, when times of less than zero seconds will be recorded""
              http://www.nature.com/nature/journal/v432/n7014/full/432147b.html
                                                                                                                                                  19/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                                        19/30
 
 8/30/13                                                                                          Regression in the real world
              Variance changes
                 bupaData <- read.csv(""ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liver-disorders/bupa.data"",header
                 ggt <- bupaData[,5]; aat <- bupaData[,3]
                 plot(log(ggt),aat,col=""blue"",pch=19)
                                                                                                                              20/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    20/30
 
 8/30/13                                                                                          Regression in the real world
              Plot the residuals
                 lm1 <- lm(aat ~ log(ggt))
                 plot(log(ggt),lm1$residuals,col=""blue"",pch=19)
              Power (a.k.a. Box-Cox) transform
                                                                                                                              21/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    21/30
 
 8/30/13                                                                                          Regression in the real world
              Changing variance - what you can do
                 · There is a long literature on this problem (heteroskedasticity)
                 · A few examples
                          - Box-Cox Transform
                          - Variance stabilizing transform
                          - Weighted least squares
                          - Huber-white standard errors
                                                                                                                              22/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    22/30
 
 8/30/13                                                                                          Regression in the real world
              Variation in units
              All Deaths
                                                                                                                              23/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    23/30
 
 8/30/13                                                                                          Regression in the real world
              Relative units
              Per 1000 Deaths
                                                                                                                              24/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    24/30
 
 8/30/13                                                                                          Regression in the real world
              When there is variation in units
                 · Standardize, but keep track
                          - Affects model fits
                          - Affects interpretation
                          - Affects inference
                                                                                                                              25/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    25/30
 
 8/30/13                                                                                          Regression in the real world
              Overloading regression
              http://bit.ly/YiB5Um http://wmbriggs.com/blog/?p=7026
                                                                                                                              26/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    26/30
 
 8/30/13                                                                                          Regression in the real world
              Correlation and Causation
              http://xkcd.com/552/
                                                                                                                              27/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    27/30
 
 8/30/13                                                                                          Regression in the real world
              Even when looking for associations
              http://www.nejm.org/doi/full/10.1056/NEJMon1211064
                                                                                                                              28/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    28/30
 
 8/30/13                                                                                          Regression in the real world
              Again, it can get silly
              http://www.statschat.org.nz/2012/10/12/even-better-than-chocolate/
                                                                                                                              29/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    29/30
 
 8/30/13                                                                                          Regression in the real world
              Correlation vs. Causation
                 · Use caution when interpreting regression results
                 · Be critical of surprising associations
                 · Consider alternative explanations
                                                                                                                              30/30
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week4/007realData/index.html#1                                    30/30
"
"./07_RegressionModels/pdfs/01_01.pdf","Introduction to regression
Regression
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 A famous motivating example
(Perhaps surprisingly, this example is still relevant)
http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html
Predicting height: the Victorian approach beats modern genomics
                                                                2/15
 
 Questions for this class
∙ Consider trying to answer the following kinds of questions:
   ­ To use the parents' heights to predict childrens' heights.
   ­ To try to find a parsimonious, easily described mean relationship between parent and children's
     heights.
   ­ To investigate the variation in childrens' heights that appears unrelated to parents' heights
     (residual variation).
   ­ To quantify what impact genotype information has beyond parental height in explaining child
     height.
   ­ To figure out how/whether and what assumptions are needed to generalize findings beyond the
     data in question.
   ­ Why do children of very tall parents tend to be tall, but a little shorter than their parents and why
     children of very short parents tend to be short, but a little taller than their parents? (This is a
     famous question called 'Regression to the mean'.)
                                                                                                       3/15
 
 Galton's Data
∙ Let's look at the data first, used by Francis Galton in 1885.
∙ Galton was a statistician who invented the term and concepts of regression and correlation,
  founded the journal Biometrika, and was the cousin of Charles Darwin.
∙ You may need to run install.packages(""UsingR"")if the UsingRlibrary is not installed.
∙ Let's look at the marginal (parents disregarding children and children disregarding parents)
  distributions first.
    ­ Parent distribution is all heterosexual couples.
    ­ Correction for gender via multiplying female heights by 1.08.
    ­ Overplotting is an issue from discretization.
                                                                                          4/15
 
 Code
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col=""blue"",breaks=100)
hist(galton$parent,col=""blue"",breaks=100)
                                          5/15
 
 Finding the middle via least squares
∙ Consider only the children's heights.
    ­ How could one describe the ""middle""?
    ­ One definition, let Yi be the height of child i for i = 1, … , n = 928 , then define the middle as the
       value of μ that minimizes
                                                    n
                                                       (Yi − μ)2
                                                   ∑
                                                   i=1
∙ This is physical center of mass of the histrogram.
∙ You might have guessed that the answer μ = Xˉ .
                                                                                                         6/15
 
 Experiment
Use R studio's manipulate to see what value of μ minimizes the sum of the
squared deviations.
 library(manipulate)
 myHist <- function(mu){
   hist(galton$child,col=""blue"",breaks=100)
   lines(c(mu, mu), c(0, 150),col=""red"",lwd=5)
   mse <- mean((galton$child - mu)^2)
   text(63, 150, paste(""mu = "", mu))
   text(63, 140, paste(""MSE = "", round(mse, 2)))
 }
 manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
                                                                        7/15
 
 The least squares estimate is the empirical mean
 hist(galton$child,col=""blue"",breaks=100)
 meanChild <- mean(galton$child)
 lines(rep(meanChild,100),seq(0,150,length=100),col=""red"",lwd=5)
                                                                 8/15
 
 The math follows as:
             n                  n
            ∑
                (Yi − μ) 2 =        ( Yi − Yˉ + Yˉ − μ) 2
                              ∑
            i=1                i=1
                              n                        n                          n
                           =      ( Yi − Yˉ ) + 2
                                             2
                                                           ( Yi − Yˉ)( Yˉ − μ) +     ( Yˉ − μ)2
                             ∑                       ∑                           ∑
                             i=1                      i=1                        i=1
                              n                                    n              n
                           = ∑( Yi − Yˉ )2 + 2( Yˉ − μ) ∑( Yi − Yˉ) +            ∑
                                                                                     ( Yˉ − μ)2
                             i=1                                 i=1             i=1
                              n                                    n                n
                           = ∑( Yi − Yˉ )2 + 2( Yˉ − μ)( ∑ Yi − n Yˉ) +            ∑
                                                                                        ( Yˉ − μ)2
                             i=1                                  i=1              i=1
                              n                    n
                           =      ( Yi − Yˉ )2 +       ( Yˉ − μ)2
                             ∑                   ∑
                             i=1                  i=1
                              n
                           ≥      ( Yi − Yˉ )2
                             ∑
                             i=1
                                                                                                   9/15
 
 Comparing childrens' heights and their parents'
heights
plot(galton$parent,galton$child,pch=19,col=""blue"")
                                                   10/15
 
 Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).
                                                                                                   11/15
 
 Regression through the origin
∙ Suppose that Xi are the parents' heights.
∙ Consider picking the slope β that minimizes
                                                n
                                                   (Yi − Xi β)2
                                              ∑
                                               i=1
∙ This is exactly using the origin as a pivot point picking the line that minimizes the sum of the
  squared vertical distances of the points to the line
∙ Use R studio's manipulate function to experiment
∙ Subtract the means so that the origin is the mean of the parent and children's heights
                                                                                              12/15
 
 myPlot <- function(beta){
  y <- galton$child - mean(galton$child)
  x <- galton$parent - mean(galton$parent)
  freqData <- as.data.frame(table(x, y))
  names(freqData) <- c(""child"", ""parent"", ""freq"")
  plot(
    as.numeric(as.vector(freqData$parent)),
    as.numeric(as.vector(freqData$child)),
    pch = 21, col = ""black"", bg = ""lightblue"",
    cex = .15 * freqData$freq,
    xlab = ""parent"",
    ylab = ""child""
    )
  abline(0, beta, lwd = 3)
  points(0, 0, cex = 2, pch = 19)
  mse <- mean( (y - beta * x)^2 )
  title(paste(""beta = "", beta, ""mse = "", round(mse, 3)))
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
                                                               13/15
 
 The solution
In the next few lectures we'll talk about why this is the solution
 lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
 Call:
 lm(formula = I(child - mean(child)) ~ I(parent - mean(parent)) -
     1, data = galton)
 Coefficients:
 I(parent - mean(parent))
                    0.646
                                                                         14/15
 
 Visualizing the best fit line
Size of points are frequencies at that X, Y combination
                                                        15/15
"
"./07_RegressionModels/pdfs/01_02.pdf","Some basic notation and background
Regression
Brian Caffo, PhD
Johns Hopkins Bloomberg School of Public Health
 
 Some basic definitions
∙ In this module, we'll cover some basic definitions and notation used throughout the class.
∙ We will try to minimize the amount of mathematics required for this class.
∙ No caclculus is required.
                                                                                             2/8
 
 Notation for data
∙ We write X1 , X2 , … , Xn to describe n data points.
∙ As an example, consider the data set {1, 2, 5} then
    ­ X1 = 1 , X2 = 2 , X3 = 5 and n = 3.
∙ We often use a different letter than X , such as Y1 , … , Yn .
∙ We will typically use Greek letters for things we don't know. Such as, μ is a mean that we'd like to
  estimate.
∙ We will use capital letters for conceptual values of the variables and lowercase letters for realized
  values.
    ­ So this way we can write P(Xi > x) .
    ­ Xi is a conceptual random variable.
    ­ x is a number that we plug into.
                                                                                                     3/8
 
 The empirical mean
∙ Define the empirical mean as
                                                         n
                                                     1
                                              Xˉ =          Xi .
                                                     n ∑
                                                        i=1
∙ Notice if we subtract the mean from data points, we get data that has mean 0. That is, if we define
                                               X̃ i = Xi − Xˉ .
  The the mean of the X̃ i is 0.
∙ This process is called ""centering"" the random variables.
∙ The mean is a measure of central tendancy of the data.
∙ Recall from the previous lecture that the mean is the least squares solution for minimizing
                                                 n
                                                    (Xi − μ)2
                                               ∑
                                                i=1
                                                                                                    4/8
 
 The emprical standard deviation and variance
∙ Define the empirical variance as
                                       n                         n
                                  1                       1                    2
                            2
                           S =            ( Xi − Xˉ )2 =            Xi2 − n Xˉ
                                n−1   ∑
                                      i=1
                                                         n−1  (
                                                                ∑
                                                                i=1              )
∙ The empirical standard deviation is defined as S = √‾S‾‾2 . Notice that the standard deviation has the
  same units as the data.
∙ The data defined by Xi /s have empirical standard deviation 1. This is called ""scaling"" the data.
∙ The empirical standard deviation is a measure of spread.
∙ Sometimes people divide by n rather than n − 1 (the latter produces an unbiased estimate.)
                                                                                                      5/8
 
 Normalization
∙ The the data defined by
                                                   Xi − Xˉ
                                             Zi =
                                                      s
  have empirical mean zero and empirical standard deviation 1.
∙ The process of centering then scaling the data is called ""normalizing"" the data.
∙ Normalized data are centered at 0 and have units equal to standard deviations of the original data.
∙ Example, a value of 2 form normalized data means that data point was two standard deviations
  larger than the mean.
                                                                                                    6/8
 
 The empirical covariance
∙ Consider now when we have pairs of data, (Xi , Yi ) .
∙ Their empirical covariance is
                                      n                                n
                                 1                               1
                  Cov(X, Y) =            (Xi − Xˉ )(Yi − Yˉ ) =           Xi Yi − nXˉ Yˉ
                                n−1 ∑
                                     i=1
                                                                n−1 (
                                                                      ∑
                                                                      i=1                )
∙ Some people prefer to divide by n rather than n − 1 (the latter produces an unbiased estimate.)
∙ The correlation is defined is
                                                        Cov(X, Y)
                                         Cor(X, Y) =
                                                           Sx Sy
  where S x and Sy are the estimates of standard deviations for the X observations and Y
  observations, respectively.
                                                                                                  7/8
 
 Some facts about correlation
∙ Cor(X, Y) = Cor(Y, X)
∙ −1 ≤ Cor(X, Y) ≤ 1
∙ Cor(X, Y) = 1 and Cor(X, Y) = −1 only when the X or Y observations fall perfectly on a positive
  or negative sloped line, respectively.
∙ Cor(X, Y) measures the strength of the linear relationship between the X and Y data, with stronger
  relationships as Cor(X, Y) heads towards ­1 or 1.
∙ Cor(X, Y) = 0 implies no linear relationship.
                                                                                                  8/8
"
"./07_RegressionModels/pdfs/01_03.pdf","Least squares estimation of regression lines
Regression via least squares
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 General least squares for linear equations
Consider again the parent and child height data from Galton
                                                            2/19
 
 Fitting the best line
∙ Let Yi be the ith child's height and Xi be the ith (average over the pair of) parents' heights.
∙ Consider finding the best line
    ­ Child's Height = β 0 + Parent's Height β 1
∙ Use least squares
                                            n
                                               {Yi − (β 0 + β 1 Xi )}2
                                           ∑
                                           i=1
∙ How do we do it?
                                                                                                  3/19
 
 Let's solve this problem generally
∙ Let μi = β 0 + β 1 Xi and our estimates be μ̂i = β̂ 0 + β̂ 1 Xi .
∙ We want to minimize
                       n                  n                      n                              n
                                    2                   2
                   †      (Yi − μi ) =       (Yi − μ̂i ) + 2         (Yi − μ̂i )(μ̂i − μi ) +      (μ̂i − μi )2
                     ∑                  ∑                      ∑                              ∑
                      i=1                i=1                    i=1                            i=1
∙ Suppose that
                                                 n
                                                    ( Yi − μ̂i )( μ̂i − μi ) = 0
                                               ∑
                                                i=1
  then
                                       n                   n                     n
                                                     2                    2
                                 †=       (Yi − μ̂i) +        (μ̂i − μi) ≥           (Yi − μ̂i )2
                                      ∑                   ∑                     ∑
                                      i=1                 i=1                   i=1
                                                                                                                4/19
 
 Mean only regression
∙ So we know that if:
                                                n
                                                   ( Yi − μ̂i )( μ̂i − μi ) = 0
                                              ∑
                                               i=1
  where μi = β 0 + β 1 Xi and μ̂i = β̂ 0 + β̂ 1 Xi then the line
                                                      Y = β̂ 0 + β̂ 1 X
  is the least squares line.
∙ Consider forcing β 1 = 0 and thus β̂ 1 = 0 ; that is, only considering horizontal lines
∙ The solution works out to be
                                                         β̂ 0 = Yˉ .
                                                                                          5/19
 
 Let's show it
                                   n                             n
                                      (Yi − μ̂i )(μ̂i − μi ) =      (Yi − β̂ 0 )(β̂ 0 − β 0 )
                                 ∑                             ∑
                                  i=1                           i=1
                                                                              n
                                                             = (β̂ 0 − β 0 )     (Yi − β̂ 0 )
                                                                             ∑
                                                                             i=1
Thus, this will equal 0 if ∑i=1 (Yi − β̂ 0 ) = nYˉ − nβ̂ 0 = 0
                            n
Thus β̂ 0 = Yˉ .
                                                                                              6/19
 
 Regression through the origin
∙ Recall that if:
                                                n
                                                   ( Yi − μ̂i )( μ̂i − μi ) = 0
                                              ∑
                                               i=1
  where μi = β 0 + β 1 Xi and μ̂i = β̂ 0 + β̂ 1 Xi then the line
                                                      Y = β̂ 0 + β̂ 1 X
  is the least squares line.
∙ Consider forcing β 0 = 0 and thus β̂ 0 = 0 ; that is, only considering lines through the origin
∙ The solution works out to be
                                                          ∑i=1n Yi Xi
                                                   β̂ 1 =       n           .
                                                            ∑i=1 Xi2
                                                                                                  7/19
 
 Let's show it
                               n                              n
                                  (Yi − μ̂i )(μ̂i − μi ) =        (Yi − β̂ 1 Xi )(β̂ 1 Xi − β 1 Xi )
                             ∑                              ∑
                              i=1                            i=1
                                                                            n
                                                          = (β̂ 1 − β 1 )     ( Yi Xi − β̂ 1 Xi2 )
                                                                          ∑
                                                                          i=1
                            n                             n                   n
Thus, this will equal 0 if ∑i=1 (Yi Xi − β̂ 1 Xi2 ) = ∑i=1 Yi Xi − β̂ 1 ∑i=1 Xi2 = 0
Thus
                                                            ∑i=1 n Yi Xi
                                                    β̂ 1 =       n         .
                                                              ∑i=1 Xi2
                                                                                                     8/19
 
 Recapping what we know
∙ If we define μi = β 0 then β̂ 0 = Yˉ .
     ­ If we only look at horizontal lines, the least squares estimate of the intercept of that line is the
       average of the outcomes.
                                         ∑ i=1n Yi X i
∙ If we define μi = Xi β 1 then β̂ 1 =
                                          ∑ni=1 X i2
     ­ If we only look at lines through the origin, we get the estimated slope is the cross product of the
       X and Ys divided by the cross product of the Xs with themselves.
∙ What about when μi = β 0 + β 1 Xi ? That is, we don't want to restrict ourselves to horizontal lines or
  lines through the origin.
                                                                                                        9/19
 
 Let's figure it out
          n                              n
            ( Yi − μ̂i )( μ̂i − μi ) =       ( Yi − β̂ 0 − β̂ 1 Xi )( β̂ 0 + β̂ 1 Xi − β 0 − β 1 Xi )
        ∑                               ∑
        i=1                             i=1
                                                        n                                          n
                                     = (β̂ 0 − β 0 )      ( Yi − β̂ 0 − β̂ 1 Xi ) + (β 1 − β 1 )      (Yi − β̂ 0 − β̂ 1 Xi) Xi
                                                     ∑                                            ∑
                                                      i=1                                         i=1
Note that
                         n
                 0=          (Yi − β̂ 0 − β̂ 1 Xi ) = nYˉ − nβ̂ 0 − nβ̂ 1 Xˉ implies that β̂ 0 = Yˉ − β̂ 1 Xˉ
                       ∑
                        i=1
Then
                                 n                                  n
                                    (Yi − β̂ 0 − β̂ 1 Xi )Xi =          (Yi − Yˉ + β̂ 1 Xˉ − β̂ 1 Xi )Xi
                               ∑                                 ∑
                                i=1                                i=1
                                                                                                                               10/19
 
 Continued
                                          n
                                      =      {(Yi − Yˉ ) − β̂ 1 (Xi − Xˉ )}Xi
                                         ∑
                                         i=1
And thus
                                   n                       n
                                      ( Yi − Yˉ)Xi − β̂ 1      (Xi − Xˉ) Xi = 0.
                                 ∑                        ∑
                                  i=1                     i=1
So we arrive at
                      ∑ni=1 {(Yi − Yˉ )Xi      ∑ni=1 (Yi − Yˉ )(Xi − Xˉ )              Sd(Y)
                β̂ 1 = n                    = n                            = Cor(Y, X)       .
                       ∑i=1 (Xi − Xˉ )Xi       ∑i=1 (Xi − Xˉ )(Xi − Xˉ )               Sd(X)
And recall
                                                β̂ 0 = Yˉ − β̂ 1 Xˉ.
                                                                                               11/19
 
 Consequences
∙ The least squares model fit to the line Y = β 0 + β 1 X through the data pairs (Xi , Yi ) with Yi as the
  outcome obtains the line Y = β̂ 0 + β̂ 1 X where
                                                         Sd(Y)
                                     β̂ 1 = Cor(Y, X)             β̂ 0 = Yˉ − β̂ 1 Xˉ
                                                         Sd(X)
∙ β̂ 1 has the units of Y/X , β̂ 0 has the units of Y .
∙ The line passes through the point (Xˉ , Yˉ )
∙ The slope of the regression line with X as the outcome and Y as the predictor is
  Cor(Y, X)Sd(X)/Sd(Y).
∙ The slope is the same one you would get if you centered the data, (Xi − Xˉ , Yi − Yˉ) , and did
  regression through the origin.
                                         ˉ  Yi −Yˉ
∙ If you normalized the data, { X i −X ,           } the slope is Cor(Y, X) .
                                    Sd(X)   Sd(Y) ,
                                                                                                      12/19
 
 Revisiting Galton's data
Double check our calculations using R
 y <- galton$child
 x <- galton$parent
 beta1 <- cor(y, x) * sd(y) / sd(x)
 beta0 <- mean(y) - beta1 * mean(x)
 rbind(c(beta0, beta1), coef(lm(y ~ x)))
      (Intercept)      x
 [1,]       23.94 0.6463
 [2,]       23.94 0.6463
                                         13/19
 
 Revisiting Galton's data
Reversing the outcome/predictor relationship
 beta1 <- cor(y, x) * sd(x) / sd(y)
 beta0 <- mean(x) - beta1 * mean(y)
 rbind(c(beta0, beta1), coef(lm(x ~ y)))
      (Intercept)      y
 [1,]       46.14 0.3256
 [2,]       46.14 0.3256
                                             14/19
 
 Revisiting Galton's data
Regression through the origin yields an equivalent slope if you center the
data first
 yc <- y - mean(y)
 xc <- x - mean(x)
 beta1 <- sum(yc * xc) / sum(xc ^ 2)
 c(beta1, coef(lm(y ~ x))[2])
             x
 0.6463 0.6463
                                                                          15/19
 
 Revisiting Galton's data
Normalizing variables results in the slope being the correlation
 yn <- (y - mean(y))/sd(y)
 xn <- (x - mean(x))/sd(x)
 c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
                   xn
 0.4588 0.4588 0.4588
                                                                 16/19
 
 Plotting the fit
∙ Size of points are frequencies at that X, Y combination.
∙ For the red lie the child is outcome.
∙ For the blue, the parent is the outcome (accounting for the fact that the response is plotted on the
  horizontal axis).
∙ Black line assumes Cor(Y, X) = 1 (slope is Sd(Y)/Sd(x)).
∙ Big black dot is (Xˉ , Yˉ ).
                                                                                                  17/19
 
 The code to add the lines
 abline(mean(y) - mean(x) * cor(y, x) * sd(y) / sd(x),
   sd(y) / sd(x) * cor(y, x),
   lwd = 3, col = ""red"")
 abline(mean(y) - mean(x) * sd(y) / sd(x) / cor(y, x),
   sd(y) cor(y, x) / sd(x),
   lwd = 3, col = ""blue"")
 abline(mean(y) - mean(x) * sd(y) / sd(x),
   sd(y) / sd(x),
   lwd = 2)
 points(mean(x), mean(y), cex = 2, pch = 19)
                                                       18/19
 
 19/19"
"./07_RegressionModels/pdfs/01_04.pdf","Historical side note, Regression to
Mediocrity
Regression to the mean
Brian Caffo, Jeff Leek, Roger Peng PhD
Johns Hopkins Bloomberg School of Public Health
 
 A historically famous idea, Regression to the
Mean
∙ Why is it that the children of tall parents tend to be tall, but not as tall as their parents?
∙ Why do children of short parents tend to be short, but not as short as their parents?
∙ Why do parents of very short children, tend to be short, but not a short as their child? And the same
  with parents of very tall children?
∙ Why do the best performing athletes this year tend to do a little worse the following?
                                                                                                     2/8
 
 Regression to the mean
∙ These phenomena are all examples of so­called regression to the mean
∙ Invented by Francis Galton in the paper ""Regression towvards mediocrity in hereditary stature"" The
  Journal of the Anthropological Institute of Great Britain and Ireland , Vol. 15, (1886).
∙ Think of it this way, imagine if you simulated pairs of random normals
    ­ The largest first ones would be the largest by chance, and the probability that there are smaller
      for the second simulation is high.
    ­ In other words P(Y < x|X = x) gets bigger as x heads into the very large values.
    ­ Similarly P(Y > x|X = x) gets bigger as x heads to very small values.
∙ Think of the regression line as the intrisic part.
    ­ Unless Cor(Y, X) = 1 the intrinsic part isn't perfect
                                                                                                    3/8
 
 Regression to the mean
∙ Suppose that we normalize X (child's height) and Y (parent's height) so that they both have mean 0
  and variance 1.
∙ Then, recall, our regression line passes through (0, 0) (the mean of the X and Y).
∙ If the slope of the regression line is Cor(Y, X) , regardless of which variable is the outcome (recall,
  both standard deviations are 1).
∙ Notice if X is the outcome and you create a plot where X is the horizontal axis, the slope of the
  least squares line that you plot is 1/Cor(Y, X) .
                                                                                                     4/8
 
 Normalizing the data and setting plotting
parameters
library(UsingR)
data(father.son)
y <- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight)
x <- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight)
rho <- cor(x, y)
myPlot <- function(x, y) {
  plot(x, y,
       xlab = ""Father's height, normalized"",
       ylab = ""Son's height, normalized"",
       xlim = c(-3, 3), ylim = c(-3, 3),
       bg = ""lightblue"", col = ""black"", cex = 1.1, pch = 21,
       frame = FALSE)
}
                                                                              5/8
 
 Plot the data, code
myPlot(x, y)
abline(0, 1) # if there were perfect correlation
abline(0, rho, lwd = 2) # father predicts son
abline(0, 1 / rho, lwd = 2) # son predicts father, son on vertical axis
abline(h = 0); abline(v = 0) # reference lines for no relathionship
                                                                        6/8
 
 Plot the data, results
                       7/8
 
 Discussion
∙ If you had to predict a son's normalized height, it would be Cor(Y, X) ∗ Xi
∙ If you had to predict a father's normalized height, it would be Cor(Y, X) ∗ Yi
∙ Multiplication by this correlation shrinks toward 0 (regression toward the mean)
∙ If the correlation is 1 there is no regression to the mean (if father's height perfectly determine's
  child's height and vice versa)
∙ Note, regression to the mean has been thought about quite a bit and generalized
                                                                                                   8/8
"
"./07_RegressionModels/pdfs/01_05.pdf","Statistical linear regression models
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Basic regression model with additive Gaussian
errors.
∙ Least squares is an estimation tool, how do we do inference?
∙ Consider developing a probabilistic model for linear regression
                                               Yi = β 0 + β 1 Xi + ϵi
∙ Here the ϵi are assumed iid N(0, σ 2 ) .
∙ Note, E[ Yi | Xi = x i ] = μi = β 0 + β 1 xi
∙ Note, Var( Yi | Xi = xi ) = σ 2 .
∙ Likelihood equivalent model specification is that the Yi are independent N( μi , σ 2 ) .
                                                                                           2/14
 
 Likelihood
                                      n
                                                                    1
                          (β, σ) =       (2π σ 2 )−1/2 exp −         2
                                                                        (yi − μi )2
                                     ∏{
                                     i=1
                                                             (    2σ                )}
so that the twice the negative log (base e) likelihood is
                                                        n
                                                   1
                             −2 log{(β, σ)} = 2           (yi − μi )2 + n log(σ 2 )
                                                  σ   ∑
                                                       i=1
Discussion
 ∙ Maximizing the likelihood is the same as minimizing ­2 log likelihood
 ∙ The least squares estimate for μi = β 0 + β 1 xi is exactly the maximimum likelihood estimate
   (regardless of σ )
                                                                                             3/14
 
 Recap
∙ Model Yi = μi + ϵi = β 0 + β 1 Xi + ϵi where ϵi are iid N(0, σ 2 )
∙ ML estimates of β 0 and β 1 are the least squares estimates
                                                   Sd(Y)
                                  β̂ 1 = Cor(Y, X)         β̂ 0 = Yˉ − β̂ 1 Xˉ
                                                   Sd(X)
∙ E[Y | X = x] = β 0 + β 1 x
∙ Var(Y | X = x) = σ 2
                                                                               4/14
 
 Interpretting regression coefficients, the itc
 ∙ β 0 is the expected value of the response when the predictor is 0
                                         E[Y|X = 0] = β 0 + β 1 × 0 = β 0
 ∙ Note, this isn't always of interest, for example when X = 0 is impossible or far outside of the range
   of data. (X is blood pressure, or height etc.)
 ∙ Consider that
                  Yi = β 0 + β 1 Xi + ϵi = β 0 + aβ 1 + β 1 (Xi − a) + ϵi = β̃ 0 + β 1 (Xi − a) + ϵi
   So, shifting you X values by value a changes the intercept, but not the slope.
 ∙ Often a is set to Xˉ so that the intercept is interpretted as the expected response at the average X
   value.
                                                                                                     5/14
 
 Interpretting regression coefficients, the slope
 ∙ β 1 is the expected change in response for a 1 unit change in the predictor
                     E[Y | X = x + 1] − E[Y | X = x] = β 0 + β 1 (x + 1) − (β 0 + β 1 x) = β 1
 ∙ Consider the impact of changing the units of X .
                                                      β1
                       Yi = β 0 + β 1 Xi + ϵi = β 0 +    (Xi a) + ϵi = β 0 + β̃ 1 (Xi a) + ϵi
                                                       a
 ∙ Therefore, multiplication of X by a factor a results in dividing the coefficient by a factor of a.
 ∙ Example: X is height in m and Y is weight in kg. Then β 1 is kg/m. Converting X to cm implies
   multiplying X by 100cm/m . To get β 1 in the right units, we have to divide by 100cm/m to get it to
   have the right units.
                            100cm                            kg      1m             β1      kg
                     Xm ×             = (100X)cm and β 1         ×          =
                               m                             m 100cm            ( 100 ) cm
                                                                                                      6/14
 
 Using regression coeficients for prediction
∙ If we would like to guess the outcome at a particular value of the predictor, say X , the regression
  model guesses
                                                 β̂ 0 + β̂ 1 X
∙ Note that at the observed value of X s, we obtain the predictions
                                          μ̂i = Ŷ i = β̂ 0 + β̂ 1 Xi
∙ Remember that least squares minimizes
                                                 n
                                                     (Yi − μi )
                                               ∑
                                                i=1
  for μi expressed as points on a line
                                                                                                   7/14
 
 Example
diamonddata set from UsingR
Data is diamond prices (Signapore dollars) and diamond weight in carats (standard measure of
diamond mass, 0.2 g). To get the data use library(UsingR); data(diamond)
Plotting the fitted regression line and data
 data(diamond)
 plot(diamond$carat, diamond$price,
       xlab = ""Mass (carats)"",
       ylab = ""Price (SIN $)"",
       bg = ""lightblue"",
       col = ""black"", cex = 1.1, pch = 21,frame = FALSE)
 abline(lm(price ~ carat, data = diamond), lwd = 2)
                                                                                         8/14
 
 The plot
         9/14
 
 Fitting the linear regression model
fit <- lm(price ~ carat, data = diamond)
coef(fit)
(Intercept)        carat
     -259.6       3721.0
∙ We estimate an expected 3721.02 (SIN) dollar increase in price for every carat increase in mass of
  diamond.
∙ The intercept ­259.63 is the expected price of a 0 carat diamond.
                                                                                                10/14
 
 Getting a more interpretable intercept
 fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
 coef(fit2)
            (Intercept) I(carat - mean(carat))
                   500.1                  3721.0
Thus $500.1 is the expected price for the average sized diamond of the data (0.2042 carats).
                                                                                             11/14
 
 Changing scale
∙ A one carat increase in a diamond is pretty big, what about changing units to 1/10th of a carat?
∙ We can just do this by just dividing the coeficient by 10.
    ­ We expect a 372.102 (SIN) dollar change in price for every 1/10th of a carat increase in mass
      of diamond.
∙ Showing that it's the same if we rescale the Xs and refit
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
  (Intercept) I(carat * 10)
       -259.6           372.1
                                                                                                   12/14
 
 Predicting the price of a diamond
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
[1] 335.7 745.1 1005.5
predict(fit, newdata = data.frame(carat = newx))
     1      2      3
 335.7 745.1 1005.5
                                                 13/14
 
 Predicted values at the observed Xs (red) and at the new Xs (lines)
                                                                    14/14
"
"./07_RegressionModels/pdfs/01_06.pdf","Residuals and residual variation
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Residuals
∙ Model Yi = β 0 + β 1 Xi + ϵi where ϵi ∼ N(0, σ 2 ) .
∙ Observed outcome i is Yi at predictor value Xi
∙ Predicted outcome i is Ŷ i at predictor valuve Xi is
                                               Ŷ i = β̂ 0 + β̂ 1 Xi
∙ Residual, the between the observed and predicted outcome
                                                 ei = Yi − Ŷ i
    ­ The vertical distance between the observed data point and the regression line
                               n
∙ Least squares minimizes ∑i=1 e2i
∙ The ei can be thought of as estimates of the ϵi .
                                                                                    2/17
 
 Properties of the residuals
∙ E[ ei ] = 0.
                                 n
∙ If an intercept is included, ∑i=1 ei = 0
                                                          n
∙ If a regressor variable, Xi , is included in the model ∑i=1 ei Xi = 0 .
∙ Residuals are useful for investigating poor model fit.
∙ Positive residuals are above the line, negative residuals are below.
∙ Residuals can be thought of as the outcome (Y ) with the linear association of the predictor (X )
  removed.
∙ One differentiates residual variation (variation after removing the predictor) from systematic
  variation (variation explained by the regression model).
∙ Residual plots highlight poor model fit.
                                                                                               3/17
 
 Code
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e -(y - yhat)))
[1] 9.486e-13
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
[1] 9.486e-13
                                                       4/17
 
 Residuals are the signed length of the red lines
                                                 5/17
 
 Residuals versus X
                   6/17
 
 Non-linear data
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y); abline(lm(y ~ x))
                                                               7/17
 
 plot(x, resid(lm(y ~ x)));
abline(h = 0)
                           8/17
 
 Heteroskedasticity
x <- runif(100, 0, 6); y <- x + rnorm(100, mean = 0, sd = .001 * x);
plot(x, y); abline(lm(y ~ x))
                                                                     9/17
 
 Getting rid of the blank space can be helpful
plot(x, resid(lm(y ~ x)));
abline(h = 0)
                                              10/17
 
 Estimating residual variation
∙ Model Yi = β 0 + β 1 Xi + ϵi where ϵi ∼ N(0, σ 2 ) .
∙ The ML estimate of σ 2 is   1
                              n ∑ni=1 e2i , the average squared residual.
∙ Most people use
                                                            n
                                                   2   1
                                                 σ̂ =          e2i .
                                                      n−2  ∑
                                                           i=1
                                            2
∙ The n − 2 instead of n is so that E[ σ̂ ] = σ 2
                                                                          11/17
 
 Diamond example
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
[1] 31.84
sqrt(sum(resid(fit)^2) / (n - 2))
[1] 31.84
                                                       12/17
 
 Summarizing variation
                         n                  n
                            (Yi − Yˉ ) =
                                      2
                                               (Yi − Ŷ i + Ŷ i − Yˉ )2
                       ∑                   ∑
                        i=1                i=1
                                            n                      n                               n
                                        =                  2
                                               (Yi − Ŷ i ) + 2       ( Yi − Ŷ i )(Ŷ i − Yˉ ) +     (Ŷ i − Yˉ )2
                                           ∑                     ∑                                ∑
                                           i=1                    i=1                             i=1
Scratch work
(Yi − Ŷ i ) = {Yi − (Yˉ − β̂ 1 Xˉ ) − β̂ 1 Xi } = ( Yi − Yˉ ) − β̂ 1 (Xi − Xˉ )
(Ŷ i − Yˉ ) = (Yˉ − β̂ 1 Xˉ − β̂ 1 Xi − Yˉ) = β̂ 1 (Xi − Xˉ )
∑ni=1 (Yi − Ŷ i )(Ŷ i − Yˉ) = ∑ni=1 {(Yi − Yˉ ) − β̂ 1 (Xi − Xˉ ))}{β̂ 1 (Xi − Xˉ )}
                                        2 n
= β̂ 1 ∑i=1 (Yi − Yˉ )(Xi − Xˉ) − β̂ 1 ∑i=1 (Xi − Xˉ )2
         n
      2 n                      2 n
= β̂ 1 ∑i=1 (Xi − Xˉ )2 − β̂ 1 ∑i=1 (Xi − Xˉ) 2 = 0
                                                                                                                    13/17
 
 Summarizing variation
                                n                    n                    n
                                   (Yi − Yˉ ) =
                                             2                      2
                                                        (Yi − Ŷ i ) +       (Ŷ i − Yˉ )2
                              ∑                    ∑                     ∑
                               i=1                  i=1                  i=1
Or
Total Variation = Residual Variation + Regression Variation
Define the percent of total varation described by the model as
                                  2
                                       ∑ni=1 (Ŷ i − Yˉ )2           ∑ni=1 (Yi − Ŷ i )2
                                R = n                       = 1− n
                                       ∑i=1 ( Yi − Yˉ) 2              ∑i=1 (Yi − Yˉ )2
                                                                                           14/17
 
                                                       2
Relation between R and r (the corrrelation)
Recall that (Ŷ i − Yˉ ) = β̂ 1 (Xi − Xˉ ) so that
                                 2
                                     ∑ni=1 (Ŷ i − Yˉ )2           n          ˉ
                                                               2 ∑i=1 (Xi − X )
                              R = n                       = β̂ 1 n                 = Cor(Y, X)2
                                     ∑i=1 (Yi − Yˉ )   2
                                                                 ∑i=1 (Yi − Yˉ ) 2
Since, recall,
                                                                        Sd(Y)
                                                   β̂ 1 = Cor(Y, X)
                                                                       Sd(X)
So, R2 is literally r squared.
                                                                                                15/17
 
                                          2
Some facts about R
∙ R2 is the percentage of variation explained by the regression model.
∙ 0 ≤ R2 ≤ 1
∙ R2 is the sample correlation squared.
∙ R2 can be a misleading summary of model fit.
    ­ Deleting data can inflate R2 .
    ­ (For later.) Adding terms to a regression model always increases R2 .
∙ Do example(anscombe)to see the following data.
    ­ Basically same mean and variance of X and Y.
    ­ Identical correlations (hence same R2 ).
    ­ Same linear regression relationship.
                                                                            16/17
 
 data(anscombe);example(anscombe)
                                 17/17
"
"./07_RegressionModels/pdfs/01_07.pdf","Inference in regression
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Recall our model and fitted values
∙ Consider the model
                                        Yi = β 0 + β 1 Xi + ϵi
∙ ϵ ∼ N(0, σ 2 ) .
∙ We assume that the true model is known.
∙ We assume that you've seen confidence intervals and hypothesis tests before.
∙ β̂ 0 = Yˉ − β̂ 1 Xˉ
                      Sd(Y)
∙ β̂ 1 = Cor(Y, X)    Sd(X) .
                                                                               2/14
 
 Review
∙ Statistics like θ̂ −θ
                   σ̂ θ̂
                         often have the following properties.
    1. Is normally distributed and has a finite sample Student's T distribution if the estimated variance
       is replaced with a sample estimate (under normality assumptions).
    2. Can be used to test H0 : θ = θ 0 versus Ha : θ >, <, ≠ θ 0 .
    3. Can be used to create a confidence interval for θ via θ̂ ± Q1−α/2 σ̂θ̂ where Q1−α/2 is the relevant
       quantile from either a normal or T distribution.
∙ In the case of regression with iid sampling assumptions and normal errors, our inferences will follow
  very similarily to what you saw in your inference class.
∙ We won't cover asymptotics for regression analysis, but suffice it to say that under assumptions on
  the ways in which the X values are collected, the iid sampling model, and mean model, the normal
  results hold to create intervals and confidence intervals
                                                                                                       3/14
 
 Standard errors (conditioned on X)
                                   ∑ni=1 (Yi − Yˉ )(Xi − Xˉ )
              Var(β̂ 1 ) = Var
                                (       ∑n (Xi − Xˉ )2
                                          i=1                 )
                           Var (∑i=1 Yi (Xi − Xˉ ))
                                    n
                         =                         2
                                ∑i=1 (Xi − Xˉ )2 )
                                  n
                              (
                           ∑n σ 2 (Xi − Xˉ )2
                              i=1
                         =                       2
                            ∑i=1 (Xi − Xˉ) 2 )
                                n
                           (
                                   σ2
                         =
                           ∑ni=1 (Xi − Xˉ) 2
                                                                4/14
 
 Results
∙ σ 2 = Var( β̂ 1 ) = σ 2 / ∑i=1 ( Xi − Xˉ ) 2
                              n
    β̂ 1
                                         2
∙ σ 2 = Var( β̂ ) =      1            Xˉ
                0        n  +                     σ2
    β̂ 0              (        ∑ni=1 (X i−Xˉ ) 2 )
∙ In practice, σ is replaced by its estimate.
∙ It's probably not surprising that under iid Gaussian errors
                                                     β̂ j − β j
                                                         σ̂ β̂
                                                               j
  follows a t distribution with n − 2 degrees of freedom and a normal distribution for large n.
∙ This can be used to create confidence intervals and perform hypothesis tests.
                                                                                                5/14
 
 Example diamond data set
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c(""Estimate"", ""Std. Error"", ""t value"", ""P(>|t|)"")
rownames(coefTable) <- c(""(Intercept)"", ""x"")
                                                                                         6/14
 
 Example continued
coefTable
            Estimate Std. Error t value P(>|t|)
(Intercept) -259.6        17.32 -14.99 2.523e-19
x             3721.0      81.79 45.50 6.751e-40
fit <- lm(y ~ x);
summary(fit)$coefficients
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -259.6        17.32 -14.99 2.523e-19
x             3721.0      81.79 45.50 6.751e-40
                                                 7/14
 
 Getting a confidence interval
  sumCoef <- summary(fit)$coefficients
  sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
  [1] -294.5 -224.8
  sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]
  [1] 3556 3886
With 95% confidence, we estimate that a 0.1 carat increase in diamond size results in a 355.6 to 388.6
increase in price in (Singapore) dollars.
                                                                                                   8/14
 
 Prediction of outcomes
∙ Consider predicting Y at a value of X
    ­ Predicting the price of a diamond given the carat
    ­ Predicting the height of a child given the height of the parents
∙ The obvious estimate for prediction at point x 0 is
                                                              β̂ 0 + β̂ 1 x 0
∙ A standard error is needed to create a prediction interval.
∙ There's a distinction between intervals for the regression line at point x 0 and the prediction of what
  a y would be at point x 0 .
∙ Line at x se, σ̂   ‾1‾‾‾‾‾‾‾‾‾‾‾‾
                              (x0 −Xˉ ) 2 ‾
           0          n +   n           ˉ 2
                   √      ∑i=1 (X i−X )
∙ Prediction interval se at x , σ̂          ‾ ‾‾ ‾‾‾ ‾‾ ‾‾‾  ‾‾‾
                                                         (x0 −Xˉ ) 2‾‾ ‾
                                0           1 + 1n + n             ˉ 2
                                       √               ∑i=1 (X i−X )
                                                                                                      9/14
 
 Plotting the prediction intervals
plot(x, y, frame=FALSE,xlab=""Carat"",ylab=""Dollars"",pch=21,col=""black"", bg=""lightblue"", cex=2)
abline(fit, lwd = 2)
xVals <- seq(min(x), max(x), by = .01)
yVals <- beta0 + beta1 * xVals
se1 <- sigma * sqrt(1 / n + (xVals - mean(x))^2/ssx)
se2 <- sigma * sqrt(1 + 1 / n + (xVals - mean(x))^2/ssx)
lines(xVals, yVals + 2 * se1)
lines(xVals, yVals - 2 * se1)
lines(xVals, yVals + 2 * se2)
lines(xVals, yVals - 2 * se2)
                                                                                            10/14
 
 Plotting the prediction intervals
                                  11/14
 
 Discussion
∙ Both intervals have varying widths.
    ­ Least width at the mean of the Xs.
∙ We are quite confident in the regression line, so that interval is very narrow.
    ­ If we knew β 0 and β 1 this interval would have zero width.
∙ The prediction interval must incorporate the variabilibity in the data around the line.
    ­ Even if we knew β 0 and β 1 this interval would still have width.
                                                                                          12/14
 
 In R
 newdata <- data.frame(x = xVals)
 p1 <- predict(fit, newdata, interval = (""confidence""))
 p2 <- predict(fit, newdata, interval = (""prediction""))
 plot(x, y, frame=FALSE,xlab=""Carat"",ylab=""Dollars"",pch=21,col=""black"", bg=""lightblue"", cex=2)
 abline(fit, lwd = 2)
 lines(xVals, p1[,2]); lines(xVals, p1[,3])
 lines(xVals, p2[,2]); lines(xVals, p2[,3])
                                                                                             13/14
 
 In R
     14/14
"
"./07_RegressionModels/pdfs/02_01.pdf","Multivariable regression
Brian Caffo, Roger Peng and Jeff Leek
Johns Hopkins Bloomberg School of Public Health
 
 Multivariable regression analyses
∙ If I were to present evidence of a relationship between breath mint useage (mints per day, X) and
  pulmonary function (measured in FEV), you would be skeptical.
     ­ Likely, you would say, 'smokers tend to use more breath mints than non smokers, smoking is
        related to a loss in pulmonary function. That's probably the culprit.'
     ­ If asked what would convince you, you would likely say, 'If non­smoking breath mint users had
        lower lung function than non­smoking non­breath mint users and, similarly, if smoking breath
        mint users had lower lung function than smoking non­breath mint users, I'd be more inclined to
        believe you'.
∙ In other words, to even consider my results, I would have to demonstrate that they hold while
  holding smoking status fixed.
                                                                                                   2/20
 
 Multivariable regression analyses
∙ An insurance company is interested in how last year's claims can predict a person's time in the
  hospital this year.
    ­ They want to use an enormous amount of data contained in claims to predict a single number.
      Simple linear regression is not equipped to handle more than one predictor.
∙ How can one generalize SLR to incoporate lots of regressors for the purpose of prediction?
∙ What are the consequences of adding lots of regressors?
    ­ Surely there must be consequences to throwing variables in that aren't related to Y?
    ­ Surely there must be consequences to omitting variables that are?
                                                                                              3/20
 
 The linear model
∙ The general linear model extends simple linear regression (SLR) by adding terms linearly into the
  model.
                                                                             p
                            Yi = β 1 X1i + β 2 X2i + … + β p Xpi + ϵi =         Xik β j + ϵi
                                                                            ∑
                                                                            k=1
∙ Here X1i = 1 typically, so that an intercept is included.
∙ Least squares (and hence ML estimates under iid Gaussianity of the errors) minimizes
                                                n            p            2
                                                     Yi −       Xki β j
                                               ∑(         ∑
                                                                        )
                                               i=1        k=1
∙ Note, the important linearity is linearity in the coefficients. Thus
                                     Yi = β 1 X1i2 + β 2 X2i2 + … + β p Xpi2 + ϵi
  is still a linear model. (We've just squared the elements of the predictor variables.)
                                                                                                4/20
 
 How to get estimates
∙ The real way requires linear algebra. We'll go over an intuitive development instead.
∙ Recall that the LS estimate for regression through the origin, E[ Yi ] = X1i β 1 , was ∑ Xi Yi / ∑ Xi2 .
∙ Let's consider two regressors, E[ Yi ] = X1i β 1 + X2i β 2 = μi .
∙ Also, recall, that if μ̂i satisfies
                                                ( Yi − μ̂i )( μ̂i − μi ) = 0
                                           ∑
                                            i=1
  for all possible values of μi , then we've found the LS estimates.
                                                                                                           5/20
 
                n                               n
                  ( Yi − μ̂i )( μ̂i − μi ) =      ( Yi − β̂ 1 X1i − β̂ 2 X2i ){ X1i ( β̂ 1 − β 1 ) + X2i (β̂ 2 − β 2 )}
             ∑                               ∑
              i=1                             i=1
∙ Thus we need
         n
   1. ∑i=1 (Yi − β̂ 1 X1i − β̂ 2 X2i )X1i = 0
         n
   2. ∑i=1 (Yi − β̂ 1 X1i − β̂ 2 X2i )X2i = 0
∙ Hold β̂ 1 fixed in 2. and solve and we get that
                                                             ∑i=1 (Yi − X1i β̂ 1 )X2i
                                                     β̂ 2 =            n
                                                                    ∑i=1 X2i2
∙ Plugging this into 1. we get that
                                    n           ∑j X2j Yj                         ∑j X2j X1j
                          0=              Yi −               X2i + β 1 X1i −                      X2i       X1i
                                 ∑
                                  i=1 {           ∑j X2j2                (           ∑j X2j2          )}
                                                                                                                        6/20
 
 Continued
∙ Re writing this we get
                                                   n
                                             0=         ei,Y|X 2 − β̂ 1 ei,X 1 |X 2 }X1i
                                                 ∑{
                                                  i=1
                      ∑nj=1 aj b j
  where ei,a|b = ai −              bi is the residual when regressing b from a without an intercept.
                       ∑ni=1 b2j
∙ We get the solution
                                                            n
                                                         ∑i=1 ei,Y|X 2 ei,X 1 |X 2
                                                 β̂ 1 =
                                                            ∑ni=1 ei,X 1 |X 2 X1
                                                                                                     7/20
 
 ∙ But note that
                                    n                  n                          ∑j X2j X1j
                                       e2X X     =       ei,X 1 |X 2 X1i −                        X2i
                                  ∑ i, 1 | 2
                                   i=1
                                                     ∑
                                                     i=1            (               ∑j X2j2           )
                                         n                       ∑j X2j X1j       n
                                      =     ei,X 1 |X 2 X1i −                         ei,X 1 |X 2 X2i
                                        ∑
                                        i=1                        ∑j X2j2      ∑
                                                                                 i=1
       n
  But ∑i=1 ei,X 1 |X 2 X2i = 0 . So we get that
                                                 n                   n
                                                      e2X X      =       ei,X 1 |X 2 X1i
                                               ∑ i, 1 | 2           ∑
                                                i=1                 i=1
  Thus we get that
                                                             n
                                                           ∑i=1 ei,Y|X 2 ei,X 1 |X 2
                                                  β̂ 1 =
                                                               ∑ni=1 e2i,X 1 |X 2
                                                                                                        8/20
 
 Summing up fitting with two regressors
                                                 n
                                                ∑i=1 ei,Y|X 2 ei,X 1 |X 2
                                         β̂ 1 =
                                                   ∑ni=1 e2i,X 1 |X 2
∙ That is, the regression estimate for β 1 is the regression through the origin estimate having
  regressed X2 out of both the response and the predictor.
∙ (Similarly, the regression estimate for β 2 is the regression through the origin estimate having
  regressed X1 out of both the response and the predictor.)
∙ More generally, multivariate regression estimates are exactly those having removed the linear
  relationship of the other variables from both the regressor and response.
                                                                                               9/20
 
 Example with two variables, simple linear
regression
 ∙ Yi = β 1 X1i + β 2 X2i where X2i = 1 is an intercept term.
              ∑j X 2j X 1j         ∑ j X 1j
 ∙ Then
               ∑j X 2j2
                           X2i =      n     = Xˉ 1 .
 ∙ ei,X 1 |X 2 = X1i − Xˉ 1 .
 ∙ Simiarly ei,Y|X 2 = Yi − Yˉ .
 ∙ Thus
                                      n
                                   ∑i=1 ei,Y|X 2 ei,X 1 |X 2   ∑ni=1 (Xi − Xˉ )(Yi − Yˉ )             Sd(Y)
                            β̂ 1 =                           =                            = Cor(X, Y)
                                        ∑ni=1 e2i,X 1 |X 2          ∑i=1 (Xi − Xˉ )
                                                                      n             2                 Sd(X)
                                                                                                            10/20
 
 The general case
∙ The equations
                                     n
                                        (Yi − X1i β̂ 1 − … − Xip β̂ p )Xk = 0
                                   ∑
                                    i=1
  for k = 1, … , p yields p equations with p unknowns.
∙ Solving them yields the least squares estimates. (With obtaining a good, fast, general solution
  requiring some knowledge of linear algebra.)
∙ The least squares estimate for the coefficient of a multivariate regression model is exactly
  regression through the origin with the linear relationships with the other regressors removed from
  both the regressor and outcome by taking residuals.
∙ In this sense, multivariate regression ""adjusts"" a coefficient for the linear impact of the other
  variables.
                                                                                                 11/20
 
 Fitting LS equations
Just so I don't leave you hanging, let's show a way to get estimates. Recall the equations:
                                              n
                                                  (Yi − X1i β̂ 1 − … − Xip β̂ p )Xk = 0
                                             ∑
                                             i=1
If I hold β̂ 1 , … , β̂ p−1 fixed then we get that
                                               ∑ni=1 (Yi − X1i β̂ 1 − … − Xi,p−1 β̂ p−1 )Xip
                                       β̂ p =                           n
                                                                      ∑i=1 Xip2
Plugging this back into the equations, we wind up with
                                    n
                                        (ei,Y|X p − ei,X 1 |X p β̂ 1 − … − ei,X p −1|X p β̂ p−1 )Xk = 0
                                   ∑
                                   i=1
                                                                                                        12/20
 
 We can tidy it up a bit more, though
Note that
                                                                       n
                                                                     ∑i=1 Xik Xip
                                               Xk = ei,X k |X p +                        Xp
                                                                      ∑ni=1 Xip 2
      n
and ∑i=1 ei,X j |X p Xip = 0 . Thus
                                   n
                                      (ei,Y|X p − ei,X 1 |X p β̂ 1 − … − ei,X p −1|X p β̂ p−1 )Xk = 0
                                 ∑
                                  i=1
is equal to
                                n
                                    (ei,Y|X p − ei,X 1 |X p β̂ 1 − … − ei,X p −1|X p β̂ p−1 )ei,X k |X p = 0
                               ∑
                               i=1
                                                                                                             13/20
 
 To sum up
∙ We've reduced p LS equations and p unknowns to p − 1 LS equations and p − 1 unknowns.
    ­ Every variable has been replaced by its residual with Xp .
    ­ This process can then be iterated until only Y and one variable remains.
∙ Think of it as follows. If we want an adjusted relationship between y and x, keep taking residuals
  over confounders and do regression through the origin.
    ­ The order that you do the confounders doesn't matter.
    ­ (It can't because our choice of doing p first was arbitrary.)
∙ This isn't a terribly efficient way to get estimates. But, it's nice conceputally, as it shows how
  regression estimates are adjusted for the linear relationship with other variables.
                                                                                                 14/20
 
 Demonstration that it works using an example
Linear model with two variables and an intercept
 n <- 100; x <- rnorm(n); x2 <- rnorm(n); x3 <- rnorm(n)
 y <- x + x2 + x3 + rnorm(n, sd = .1)
 e <- function(a, b) a - sum( a * b ) / sum( b ^ 2) * b
 ey <- e(e(y, x2), e(x3, x2))
 ex <- e(e(x, x2), e(x3, x2))
 sum(ey * ex) / sum(ex ^ 2)
 [1] 1.004
 coef(lm(y ~ x + x2 + x3 - 1)) #the -1 removes the intercept term
      x     x2     x3
 1.0040 0.9899 1.0078
                                                                  15/20
 
 Showing that order doesn't matter
ey <- e(e(y, x3), e(x2, x3))
ex <- e(e(x, x3), e(x2, x3))
sum(ey * ex) / sum(ex ^ 2)
[1] 1.004
coef(lm(y ~ x + x2 + x3 - 1)) #the -1 removes the intercept term
     x     x2     x3
1.0040 0.9899 1.0078
                                                                 16/20
 
 Residuals again
ey <- resid(lm(y ~ x2 + x3 - 1))
ex <- resid(lm(x ~ x2 + x3 - 1))
sum(ey * ex) / sum(ex ^ 2)
[1] 1.004
coef(lm(y ~ x + x2 + x3 - 1)) #the -1 removes the intercept term
     x     x2     x3
1.0040 0.9899 1.0078
                                                                 17/20
 
 Interpretation of the coeficient
                                                                     p
                                  E[Y|X1 = x1 , … , Xp = xp ] =        xk β k
                                                                   ∑
                                                                   k=1
So that
                       E[Y|X1 = x1 + 1, … , Xp = x p ] − E[Y|X1 = x1 , … , Xp = xp ]
                                                    p        p
                                  = (x 1 + 1)β 1 +     xk +     xk β k = β 1
                                                   ∑        ∑
                                                   k=2      k=1
So that the interpretation of a multivariate regression coefficient is the expected change in the
response per unit change in the regressor, holding all of the other regressors fixed.
In the next lecture, we'll do examples and go over context­specific interpretations.
                                                                                             18/20
 
 Fitted values, residuals and residual variation
All of our SLR quantities can be extended to linear models
                  p
 ∙ Model Yi = ∑k=1 Xik β k + ϵi where ϵi ∼ N(0, σ 2 )
                            p
 ∙ Fitted responses Ŷ i = ∑k=1 Xik β̂ k
 ∙ Residuals ei = Yi − Ŷ i
                         2
 ∙ Variance estimate σ̂ =
                              1
                            n−p ∑ni=1 e2i
 ∙ To get predicted responses at new values, x 1 , … , x p , simply plug them into the linear model
    ∑pk=1 xk β̂ k
 ∙ Coefficients have standard errors, σ̂ , and    β̂ k −β k
                                          β̂         σ̂β̂
                                                            follows a T distribution with n − p degrees of
                                             k
                                                          k
    freedom.
 ∙ Predicted responses have standard errors and we can calculate predicted and expected response
    intervals.
                                                                                                      19/20
 
 Linear models
∙ Linear models are the single most important applied statistical and machine learning techniqe, by
  far.
∙ Some amazing things that you can accomplish with linear models
     ­ Decompose a signal into its harmonics.
     ­ Flexibly fit complicated functions.
     ­ Fit factor variables as predictors.
     ­ Uncover complex multivariate relationships with the response.
     ­ Build accurate prediction models.
                                                                                               20/20
"
"./07_RegressionModels/pdfs/02_02.pdf","Multivariable regression examples
Regression Models
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Swiss fertility data
library(datasets); data(swiss); require(stats); require(graphics)
pairs(swiss, panel = panel.smooth, main = ""Swiss data"", col = 3 + (swiss$Catholic > 50))
                                                                                         2/35
 
 ?swiss
Description
Standardized fertility measure and socio­economic indicators for each of 47 French­speaking
provinces of Switzerland at about 1888.
A data frame with 47 observations on 6 variables, each of which is in percent, i.e., in [0, 100].
 ∙ [,1] Fertility Ig, ‘ common standardized fertility measure’
 ∙ [,2] Agriculture % of males involved in agriculture as occupation
 ∙ [,3] Examination % draftees receiving highest mark on army examination
 ∙ [,4] Education % education beyond primary school for draftees.
 ∙ [,5] Catholic % ‘catholic’ (as opposed to ‘protestant’).
 ∙ [,6] Infant.Mortality live births who live less than 1 year.
All variables but ‘Fertility’ give proportions of the population.
                                                                                                  3/35
 
 Calling lm
summary(lm(Fertility ~ . , data = swiss))
                 Estimate Std. Error t value Pr(>|t|)
 (Intercept)      66.9152 10.70604 6.250 1.906e-07
 Agriculture      -0.1721    0.07030 -2.448 1.873e-02
 Examination      -0.2580    0.25388 -1.016 3.155e-01
 Education        -0.8709    0.18303 -4.758 2.431e-05
 Catholic          0.1041    0.03526 2.953 5.190e-03
 Infant.Mortality 1.0770     0.38172 2.822 7.336e-03
                                                      4/35
 
 Example interpretation
∙ Agriculture is expressed in percentages (0 ­ 100)
∙ Estimate is ­0.1721.
∙ We estimate an expected 0.17 decrease in standardized fertility for every 1\% increase in
  percentage of males involved in agriculture in holding the remaining variables constant.
∙ The t­test for H0 : β Agri = 0 versus Ha : β Agri ≠ 0 is significant.
∙ Interestingly, the unadjusted estimate is
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 60.3044         4.25126 14.185 3.216e-18
Agriculture 0.1942          0.07671 2.532 1.492e-02
                                                                                           5/35
 
 How can adjustment reverse the sign of an effect? Let's try a simulation.
 n <- 100; x2 <- 1 : n; x1 <- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
 summary(lm(y ~ x1))$coef
             Estimate Std. Error t value Pr(>|t|)
 (Intercept)    1.618      1.200 1.349 1.806e-01
 x1            95.854      2.058 46.579 1.153e-68
 summary(lm(y ~ x1 + x2))$coef
               Estimate Std. Error t value Pr(>|t|)
 (Intercept) 0.0003683 0.0020141       0.1829 8.553e-01
 x1          -1.0215256 0.0166372 -61.4001 1.922e-79
 x2           1.0001909 0.0001681 5950.1818 1.369e-271
                                                                                              6/35
 
 7/35 
 Back to this data set
∙ The sign reverses itself with the inclusion of Examination and Education, but of which are negatively
  correlated with Agriculture.
∙ The percent of males in the province working in agriculture is negatively related to educational
  attainment (correlation of ­0.6395) and Education and Examination (correlation of 0.6984) are
  obviously measuring similar things.
     ­ Is the positive marginal an artifact for not having accounted for, say, Education level?
       (Education does have a stronger effect, by the way.)
∙ At the minimum, anyone claiming that provinces that are more agricultural have higher fertility rates
  would immediately be open to criticism.
                                                                                                    8/35
 
 What if we include an unnecessary variable?
z adds no new linear information, since it's a linear combination of variables already included. R just
drops terms that are linear combinations of other terms.
 z <- swiss$Agriculture + swiss$Education
 lm(Fertility ~ . + z, data = swiss)
 Call:
 lm(formula = Fertility ~ . + z, data = swiss)
 Coefficients:
      (Intercept)         Agriculture       Examination          Education           Catholic
           66.915              -0.172             -0.258            -0.871              0.104
 Infant.Mortality                   z
            1.077                  NA
                                                                                                   9/35
 
 Dummy variables are smart
∙ Consider the linear model
                                              Yi = β 0 + Xi1 β 1 + ϵi
  where each Xi1 is binary so that it is a 1 if measurement i is in a group and 0 otherwise. (Treated
  versus not in a clinical trial, for example.)
∙ Then for people in the group E[ Yi ] = β 0 + β 1
∙ And for people not in the group E[ Yi ] = β 0
∙ The LS fits work out to be β̂ 0 + β̂ 1 is the mean for those in the group and β̂ 0 is the mean for those
  not in the group.
∙ β 1 is interpretted as the increase or decrease in the mean comparing those in the group to those
  not.
∙ Note including a binary variable that is 1 for those not in the group would be redundant. It would
  create three parameters to describe two means.
                                                                                                       10/35
 
 More than 2 levels
∙ Consider a multilevel factor level. For didactic reasons, let's say a three level factor (example, US
  political party affiliation: Republican, Democrat, Independent)
∙ Yi = β 0 + Xi1 β 1 + Xi2 β 2 + ϵi .
∙ Xi1 is 1 for Republicans and 0 otherwise.
∙ Xi2 is 1 for Democrats and 0 otherwise.
∙ If i is Republican E[ Yi ] = β 0 + β 1
∙ If i is Democrat E[ Yi ] = β 0 + β 2 .
∙ If i is Independent E[ Yi ] = β 0 .
∙ β 1 compares Republicans to Independents.
∙ β 2 compares Democrats to Independents.
∙ β 1 − β 2 compares Republicans to Democrats.
∙ (Choice of reference category changes the interpretation.)
                                                                                                    11/35
 
 Insect Sprays
              12/35
 
 Linear model fit, group A is the reference
summary(lm(count ~ spray, data = InsectSprays))$coef
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 14.5000       1.132 12.8074 1.471e-19
sprayB        0.8333      1.601 0.5205 6.045e-01
sprayC      -12.4167      1.601 -7.7550 7.267e-11
sprayD       -9.5833      1.601 -5.9854 9.817e-08
sprayE      -11.0000      1.601 -6.8702 2.754e-09
sprayF        2.1667      1.601 1.3532 1.806e-01
                                                     13/35
 
 Hard coding the dummy variables
summary(lm(count ~
             I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
             I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
             I(1 * (spray == 'F'))
           , data = InsectSprays))$coef
                      Estimate Std. Error t value Pr(>|t|)
(Intercept)            14.5000      1.132 12.8074 1.471e-19
I(1 * (spray == ""B"")) 0.8333        1.601 0.5205 6.045e-01
I(1 * (spray == ""C"")) -12.4167      1.601 -7.7550 7.267e-11
I(1 * (spray == ""D"")) -9.5833       1.601 -5.9854 9.817e-08
I(1 * (spray == ""E"")) -11.0000      1.601 -6.8702 2.754e-09
I(1 * (spray == ""F"")) 2.1667        1.601 1.3532 1.806e-01
                                                             14/35
 
 What if we include all 6?
lm(count ~
   I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
   I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
   I(1 * (spray == 'F')) + I(1 * (spray == 'A')), data = InsectSprays)
Call:
lm(formula = count ~ I(1 * (spray == ""B"")) + I(1 * (spray ==
    ""C"")) + I(1 * (spray == ""D"")) + I(1 * (spray == ""E"")) + I(1 *
    (spray == ""F"")) + I(1 * (spray == ""A"")), data = InsectSprays)
Coefficients:
          (Intercept) I(1 * (spray == ""B"")) I(1 * (spray == ""C"")) I(1 * (spray == ""D""))
               14.500                  0.833                -12.417              -9.583
I(1 * (spray == ""E"")) I(1 * (spray == ""F"")) I(1 * (spray == ""A""))
              -11.000                  2.167                     NA
                                                                                        15/35
 
 What if we omit the intercept?
summary(lm(count ~ spray - 1, data = InsectSprays))$coef
       Estimate Std. Error t value Pr(>|t|)
sprayA 14.500        1.132 12.807 1.471e-19
sprayB 15.333        1.132 13.543 1.002e-20
sprayC    2.083      1.132 1.840 7.024e-02
sprayD    4.917      1.132 4.343 4.953e-05
sprayE    3.500      1.132 3.091 2.917e-03
sprayF 16.667        1.132 14.721 1.573e-22
unique(ave(InsectSprays$count, InsectSprays$spray))
[1] 14.500 15.333 2.083 4.917 3.500 16.667
                                                         16/35
 
 Summary
∙ If we treat Spray as a factor, R includes an intercept and omits the alphabetically first level of the
  factor.
     ­ All t­tests are for comparisons of Sprays versus Spray A.
     ­ Emprirical mean for A is the intercept.
     ­ Other group means are the itc plus their coefficient.
∙ If we omit an intercept, then it includes terms for all levels of the factor.
     ­ Group means are the coefficients.
     ­ Tests are tests of whether the groups are different than zero. (Are the expected counts zero
       for that spray.)
∙ If we want comparisons between, Spray B and C, say we could refit the model with C (or B) as the
  reference level.
                                                                                                   17/35
 
 Reordering the levels
spray2 <- relevel(InsectSprays$spray, ""C"")
summary(lm(count ~ spray2, data = InsectSprays))$coef
            Estimate Std. Error t value Pr(>|t|)
(Intercept)    2.083      1.132 1.8401 7.024e-02
spray2A       12.417      1.601 7.7550 7.267e-11
spray2B       13.250      1.601 8.2755 8.510e-12
spray2D        2.833      1.601 1.7696 8.141e-02
spray2E        1.417      1.601 0.8848 3.795e-01
spray2F       14.583      1.601 9.1083 2.794e-13
                                                      18/35
 
 Doing it manually
Equivalently
                        Var(β̂ B − β̂ C ) = Var(β̂ B ) + Var(β̂ C ) − 2Cov(β̂ B , β̂ C )
 fit <- lm(count ~ spray, data = InsectSprays) #A is ref
 bbmbc <- coef(fit)[2] - coef(fit)[3] #B - C
 temp <- summary(fit)
 se <- temp$sigma * sqrt(temp$cov.unscaled[2, 2] + temp$cov.unscaled[3,3] - 2 *temp$cov.unscaled[2,3])
 t <- (bbmbc) / se
 p <- pt(-abs(t), df = fit$df)
 out <- c(bbmbc, se, t, p)
 names(out) <- c(""B - C"", ""SE"", ""T"", ""P"")
 round(out, 3)
  B-C        SE     T      P
 13.250 1.601 8.276 0.000
                                                                                              19/35
 
 Other thoughts on this data
∙ Counts are bounded from below by 0, violates the assumption of normality of the errors.
    ­ Also there are counts near zero, so both the actual assumption and the intent of the
      assumption are violated.
∙ Variance does not appear to be constant.
∙ Perhaps taking logs of the counts would help.
    ­ There are 0 counts, so maybe log(Count + 1)
∙ Also, we'll cover Poisson GLMs for fitting count data.
                                                                                          20/35
 
 Example - Millenium Development Goal 1
http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf
http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:;SEX:
                                                                                        21/35
 
 WHO childhood hunger data
#download.file(""http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX
hunger <- read.csv(""hunger.csv"")
hunger <- hunger[hunger$Sex!=""Both sexes"",]
head(hunger)
                               Indicator Data.Source PUBLISH.STATES Year            WHO.region
1 Children aged <5 years underweight (%) NLIS_310044      Published 1986                Africa
2 Children aged <5 years underweight (%) NLIS_310233      Published 1990              Americas
3 Children aged <5 years underweight (%) NLIS_312902      Published 2005              Americas
5 Children aged <5 years underweight (%) NLIS_312522      Published 2002 Eastern Mediterranean
6 Children aged <5 years underweight (%) NLIS_312955      Published 2008                Africa
8 Children aged <5 years underweight (%) NLIS_312963      Published 2008                Africa
        Country    Sex Display.Value Numeric Low High Comments
1       Senegal Male            19.3    19.3 NA NA          NA
2      Paraguay Male             2.2     2.2 NA NA          NA
3     Nicaragua Male             5.3     5.3 NA NA          NA
5        Jordan Female           3.2     3.2 NA NA          NA
6 Guinea-Bissau Female          17.0    17.0 NA NA          NA
8         Ghana Male            15.7    15.7 NA NA          NA
                                                                                             22/35
 
 Plot percent hungry versus time
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19,col=""blue"")
                                                   23/35
 
 Remember the linear model
                                         Hui = b0 + b1 Yi + ei
b0 = percent hungry at Year 0
b1 = decrease in percent hungry per year
ei = everything we didn't measure
                                                               24/35
 
 Add the linear model
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19,col=""blue"")
lines(hunger$Year,lm1$fitted,lwd=3,col=""darkgrey"")
                                                   25/35
 
 Color by male/female
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
                                                                         26/35
 
 Now two lines
                                         HuFi = bf 0 + bf 1 Y Fi + ef i
bf 0 = percent of girls hungry at Year 0
bf 1 = decrease in percent of girls hungry per year
ef i = everything we didn't measure
                                       HuMi = bm0 + bm1 YM i + emi
bm0 = percent of boys hungry at Year 0
bm1 = decrease in percent of boys hungry per year
emi = everything we didn't measure
                                                                        27/35
 
 Color by male/female
lmM <- lm(hunger$Numeric[hunger$Sex==""Male""] ~ hunger$Year[hunger$Sex==""Male""])
lmF <- lm(hunger$Numeric[hunger$Sex==""Female""] ~ hunger$Year[hunger$Sex==""Female""])
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
lines(hunger$Year[hunger$Sex==""Male""],lmM$fitted,col=""black"",lwd=3)
lines(hunger$Year[hunger$Sex==""Female""],lmF$fitted,col=""red"",lwd=3)
                                                                                    28/35
 
 Two lines, same slope
                              Hui = b0 + b1 ᵽ(Sex i ="" Male "") + b2 Yi + e∗
                                                                          i
b0 ­ percent hungry at year zero for females
b0 + b1 ­ percent hungry at year zero for males
b2 ­ change in percent hungry (for either males or females) in one year
e∗
 i ­ everything we didn't measure
                                                                            29/35
 
 Two lines, same slope in R
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col=""red"",lwd=3)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] ),col=""black"",lwd=3)
                                                                                30/35
 
 Two lines, different slopes (interactions)
                 Hui = b0 + b1 ᵽ(Sexi ="" Male "") + b2 Yi + b3 ᵽ(Sex i ="" Male "") × Yi + e+i
b0 ­ percent hungry at year zero for females
b0 + b1 ­ percent hungry at year zero for males
b2 ­ change in percent hungry (females) in one year
b2 + b3 ­ change in percent hungry (males) in one year
e+i ­ everything we didn't measure
                                                                                            31/35
 
 Two lines, different slopes in R
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex*hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==""Male"")*1+1))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col=""red"",lwd=3)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] +lmBoth$coeff[4]),col=""black"",lwd=3)
                                                                                             32/35
 
 Two lines, different slopes in R
summary(lmBoth)
Call:
lm(formula = hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex *
    hunger$Year)
Residuals:
   Min     1Q Median     3Q    Max
-25.91 -11.25 -1.85    7.09 46.15
Coefficients:
                           Estimate Std. Error t value Pr(>|t|)
(Intercept)                603.5058 171.0552      3.53 0.00044 ***
hunger$Year                 -0.2934     0.0855 -3.43 0.00062 ***
hunger$SexMale              61.9477 241.9086      0.26 0.79795
hunger$Year:hunger$SexMale -0.0300      0.1209 -0.25 0.80402
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 13.2 on 944 degrees of freedom
Multiple R-squared: 0.0318,     Adjusted R-squared: 0.0287            33/35
 
 Interpretting a continuous interaction
                            E[ Yi | X1i = x1 , X2i = x2 ] = β 0 + β 1 x 1 + β 2 x2 + β 3 x1 x 2
Holding X2 constant we have
                   E[ Yi | X1i = x1 + 1, X2i = x2 ] − E[ Yi | X1i = x 1 , X2i = x2 ] = β 1 + β 3 x2
And thus the expected change in Y per unit change in X1 holding all else constant is not constant. β 1 is
the slope when x2 = 0 . Note further that:
                      E[ Yi | X1i = x1 + 1, X2i = x2 + 1] − E[ Yi | X1i = x1 , X2i = x2 + 1]
                           −E[Yi |X1i = x1 + 1, X2i = x 2 ] − E[Yi |X1i = x1 , X2i = x 2 ]
                                                          = β3
Thus, β 3 is the change in the expected change in Y per unit change in X1 , per unit change in X2 .
Or, the change in the slope relating X1 and Y per unit change in X2 .
                                                                                                    34/35
 
 Example
                                Hui = b0 + b1 I ni + b2 Yi + b3 I ni × Yi + e+i
b0 ­ percent hungry at year zero for children with whose parents have no income
b1 ­ change in percent hungry for each dollar of income in year zero
b2 ­ change in percent hungry in one year for children whose parents have no income
b3 ­ increased change in percent hungry by year for each dollar of income ­ e.g. if income is $10,000,
then change in percent hungry in one year will be
                                               b2 + 1e4 × b3
e+i ­ everything we didn't measure
Lot's of care/caution needed!
                                                                                                  35/35
"
"./07_RegressionModels/pdfs/02_03.pdf","Multivariable regression
Regression
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Consider the following simulated data
Code for the first plot, rest omitted (See the git repo for the rest of the code.)
 n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), runif(n/2));
 beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
 y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
 plot(x, y, type = ""n"", frame = FALSE)
 abline(lm(y ~ x), lwd = 2)
 abline(h = mean(y[1 : (n/2)]), lwd = 3)
 abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
 fit <- lm(y ~ x + t)
 abline(coef(fit)[1], coef(fit)[2], lwd = 3)
 abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
 points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = ""black"", bg = ""lightblue"", cex = 2)
 points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = ""black"", bg = ""salmon"", cex = 2)
                                                                                             2/17
 
 Simulation 1
             3/17
 
 Discussion
Some things to note in this simulation
 ∙ The X variable is unrelated to group status
 ∙ The X variable is related to Y, but the intercept depends on group status.
 ∙ The group variable is related to Y.
     ­ The relationship between group status and Y is constant depending on X.
     ­ The relationship between group and Y disregarding X is about the same as holding X constant
                                                                                                4/17
 
 Simulation 2
             5/17
 
 Discussion
Some things to note in this simulation
 ∙ The X variable is highly related to group status
 ∙ The X variable is related to Y, the intercept doesn't depend on the group variable.
     ­ The X variable remains related to Y holding group status constant
 ∙ The group variable is marginally related to Y disregarding X.
 ∙ The model would estimate no adjusted effect due to group.
     ­ There isn't any data to inform the relationship between group and Y.
     ­ This conclusion is entirely based on the model.
                                                                                       6/17
 
 Simulation 3
             7/17
 
 Discussion
Some things to note in this simulation
 ∙ Marginal association has red group higher than blue.
 ∙ Adjusted relationship has blue group higher than red.
 ∙ Group status related to X.
 ∙ There is some direct evidence for comparing red and blue holding X fixed.
                                                                             8/17
 
 Simulation 4
             9/17
 
 Discussion
Some things to note in this simulation
 ∙ No marginal association between group status and Y.
 ∙ Strong adjusted relationship.
 ∙ Group status not related to X.
 ∙ There is lots of direct evidence for comparing red and blue holding X fixed.
                                                                                10/17
 
 Simulation 5
             11/17
 
 Discussion
Some things to note from this simulation
 ∙ There is no such thing as a group effect here.
     ­ The impact of group reverses itself depending on X.
     ­ Both intercept and slope depends on group.
 ∙ Group status and X unrelated.
     ­ There's lots of information about group effects holding X fixed.
                                                                        12/17
 
 Simulation 6
             13/17
 
 Do this to investigate the bivariate relationship
 library(rgl)
 plot3d(x1, x2, y)
                                                  14/17
 
 Residual relationship
                      15/17
 
 Discussion
Some things to note from this simulation
 ∙ X1 unrelated to X2
 ∙ X2 strongly related to Y
 ∙ Adjusted relationship between X1 and Y largely unchanged by considering X2.
     ­ Almost no residual variability after accounting for X2.
                                                                               16/17
 
 Some final thoughts
∙ Modeling multivariate relationships is difficult.
∙ Play around with simulations to see how the inclusion or exclustion of another variable can change
  analyses.
∙ The results of these analyses deal with the impact of variables on associations.
    ­ Ascertaining mechanisms or cause are difficult subjects to be added on top of difficulty in
      understanding multivariate associations.
                                                                                                17/17
"
"./07_RegressionModels/pdfs/02_04.pdf","Residuals, diagnostics, variation
Regression
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 The linear model
                       p
∙ Specified as Yi = ∑k=1 Xik β j + ϵi
∙ We'll also assume here that ϵi iid
                                  ∼ N(0, σ 2 )
                                                    p
∙ Define the residuals as ei = Yi − Ŷ i = Yi − ∑k=1 Xik β̂ j
                                               ∑ ni=1 e2i
∙ Our estimate of residual variation is σ̂ 2 =  n−p
                                                                                   2
                                                          , the n − p so that E[ σ̂ ] = σ 2
                                                                                            2/16
 
 data(swiss); par(mfrow = c(2, 2))
fit <- lm(Fertility ~ . , data = swiss); plot(fit)
                                                   3/16
 
 Influential, high leverage and outlying points
                                               4/16
 
 Summary of the plot
Calling a point an outlier is vague.
 ∙ Outliers can be the result of spurious or real processes.
 ∙ Outliers can have varying degrees of influence.
 ∙ Outliers can conform to the regression relationship (i.e being marginally outlying in X or Y, but not
   outlying given the regression relationship).
      ­ Upper left hand point has low leverage, low influence, outlies in a way not conforming to the
        regression relationship.
      ­ Lower left hand point has low leverage, low influence and is not to be an outlier in any sense.
      ­ Upper right hand point has high leverage, but chooses not to extert it and thus would have low
        actual influence by conforming to the regresison relationship of the other points.
      ­ Lower right hand point has high leverage and would exert it if it were included in the fit.
                                                                                                      5/16
 
 Influence measures
 ∙ Do ?influence.measures to see the full suite of influence measures in stats. The measures
   include
      ­ rstandard­ standardized residuals, residuals divided by their standard deviations)
      ­ rstudent­ standardized residuals, residuals divided by their standard deviations, where the
        ith data point was deleted in the calculation of the standard deviation for the residual to follow a
        t distribution
      ­ hatvalues­ measures of leverage
      ­ dffits­ change in the predicted response when the ith point is deleted in fitting the model.
      ­ dfbetas­ change in individual coefficients when the ith point is deleted in fitting the model.
      ­ cooks.distance­ overall change in teh coefficients when the ith point is deleted.
      ­ resid­ returns the ordinary residuals
      ­ resid(fit) / (1 - hatvalues(fit)) where fit is the linear model fit returns the
        PRESS residuals, i.e. the leave one out cross validation residuals ­ the difference in the
        response and the predicted response at data point i, where it was not included in the model
        fitting.
                                                                                                         6/16
 
 How do I use all of these things?
∙ Be wary of simplistic rules for diagnostic plots and measures. The use of these tools is context
  specific. It's better to understand what they are trying to accomplish and use them judiciously.
∙ Not all of the measures have meaningful absolute scales. You can look at them relative to the
  values across the data.
∙ They probe your data in different ways to diagnose different problems.
∙ Patterns in your residual plots generally indicate some poor aspect of model fit. These can include:
     ­ Heteroskedasticity (non constant variance).
     ­ Missing model terms.
     ­ Temporal patterns (plot residuals versus collection order).
∙ Residual QQ plots investigate normality of the errors.
∙ Leverage measures (hat values) can be useful for diagnosing data entry errors.
∙ Influence measures get to the bottom line, 'how does deleting or including this point impact a
  particular aspect of the model'.
                                                                                                    7/16
 
 Case 1
       8/16
 
 The code
n <- 100; x <- c(10, rnorm(n)); y <- c(10, c(rnorm(n)))
plot(x, y, frame = FALSE, cex = 2, pch = 21, bg = ""lightblue"", col = ""black"")
abline(lm(y ~ x))
∙ The point c(10, 10)has created a strong regression relationship where there shouldn't be one.
                                                                                              9/16
 
 Showing a couple of the diagnostic values
fit <- lm(y ~ x)
round(dfbetas(fit)[1 : 10, 2], 3)
     1      2      3      4      5      6      7      8     9     10
 6.007 -0.019 -0.007 0.014 -0.002 -0.083 -0.034 -0.045 -0.112 -0.008
round(hatvalues(fit)[1 : 10], 3)
    1     2     3     4     5     6     7     8     9    10
0.445 0.010 0.011 0.011 0.030 0.017 0.012 0.033 0.021 0.010
                                                                     10/16
 
 Case 2
       11/16
 
 Looking at some of the diagnostics
round(dfbetas(fit2)[1 : 10, 2], 3)
     1      2      3      4      5      6      7      8     9 10
-0.072 -0.041 -0.007 0.012 0.008 -0.187 0.017 0.100 -0.059 0.035
round(hatvalues(fit2)[1 : 10], 3)
    1     2     3     4     5     6     7     8     9    10
0.164 0.011 0.014 0.012 0.010 0.030 0.017 0.017 0.013 0.021
                                                                 12/16
 
 Example described by Stefanski TAS 2007 Vol
61.
## Don't everyone hit this server at once. Read the paper first.
dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl
pairs(dat)
                                                                                             13/16
 
 Got our P-values, should we bother to do a
residual plot?
 summary(lm(V1 ~ . -1, data = dat))$coef
    Estimate Std. Error t value Pr(>|t|)
 V2 0.9856      0.12798 7.701 1.989e-14
 V3 0.9715      0.12664 7.671 2.500e-14
 V4 0.8606      0.11958 7.197 8.301e-13
 V5 0.9267      0.08328 11.127 4.778e-28
                                           14/16
 
 Residual plot
P-values significant, O RLY?
 fit <- lm(V1 ~ . - 1, data = dat); plot(predict(fit), resid(fit), pch = '.')
                                                                              15/16
 
 Back to the Swiss data
                       16/16
"
"./07_RegressionModels/pdfs/02_05.pdf","Multiple variables
Regression
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Multivariable regression
∙ We have an entire class on prediction and machine learning, so we'll focus on modeling.
     ­ Prediction has a different set of criteria, needs for interpretability and standards for
       generalizability.
     ­ In modeling, our interest lies in parsimonious, interpretable representations of the data that
       enhance our understanding of the phenomena under study.
     ­ A model is a lense through which to look at your data. (I attribute this quote to Scott Zeger)
     ­ Under this philosophy, what's the right model? Whatever model connects the data to a true,
       parsimonious statement about what you're studying.
∙ There are nearly uncontable ways that a model can be wrong, in this lecture, we'll focus on variable
  inclusion and exclusion.
∙ Like nearly all aspects of statistics, good modeling decisions are context dependent.
     ­ A good model for prediction versus one for studying mechanisms versus one for trying to
       establish causal effects may not be the same.
                                                                                                      2/14
 
 The Rumsfeldian triplet
There are known knowns. These are things we know that we know. There are known unknowns. That
is to say, there are things that we know we don't know. But there are also unknown unknowns. There
are things we don't know we don't know. Donald Rumsfeld
In our context
 ∙ (Known knowns) Regressors that we know we should check to include in the model and have.
 ∙ (Known Unknowns) Regressors that we would like to include in the model, but don't have.
 ∙ (Unknown Unknowns) Regressors that we don't even know about that we should have included in
    the model.
                                                                                               3/14
 
 General rules
∙ Omitting variables results in bias in the coeficients of interest ­ unless their regressors are
  uncorrelated with the omitted ones.
    ­ This is why we randomize treatments, it attempts to uncorrelate our treatment indicator with
       variables that we don't have to put in the model.
    ­ (If there's too many unobserved confounding variables, even randomization won't help you.)
∙ Including variables that we shouldn't have increases standard errors of the regression variables.
    ­ Actually, including any new variables increasese (actual, not estimated) standard errors of
       other regressors. So we don't want to idly throw variables into the model.
∙ The model must tend toward perfect fit as the number of non­redundant regressors approaches n.
∙ R2 increases monotonically as more regressors are included.
∙ The SSE decreases monotonically as more regressors are included.
                                                                                                    4/14
 
                     2
               R
Plot of versus n
For simulations as the number of variables included equals increases to n = 100 . No actual regression
relationship exist in any simulation
                                                                                                   5/14
 
 Variance inflation
n <- 100; nosim <- 1000
x1 <- rnorm(n); x2 <- rnorm(n); x3 <- rnorm(n);
betas <- sapply(1 : nosim, function(i){
  y <- x1 + rnorm(n, sd = .3)
  c(coef(lm(y ~ x1))[2],
    coef(lm(y ~ x1 + x2))[2],
    coef(lm(y ~ x1 + x2 + x3))[2])
})
round(apply(betas, 1, sd), 5)
     x1      x1      x1
0.02839 0.02872 0.02884
                                                6/14
 
 Variance inflation
n <- 100; nosim <- 1000
x1 <- rnorm(n); x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2);
betas <- sapply(1 : nosim, function(i){
  y <- x1 + rnorm(n, sd = .3)
  c(coef(lm(y ~ x1))[2],
    coef(lm(y ~ x1 + x2))[2],
    coef(lm(y ~ x1 + x2 + x3))[2])
})
round(apply(betas, 1, sd), 5)
     x1      x1      x1
0.03131 0.04270 0.09653
                                                     7/14
 
 Variance inflation factors
∙ Notice variance inflation was much worse when we included a variable that was highly related to x1.
∙ We don't know σ , so we can only estimate the increase in the actual standard error of the
  coefficients for including a regressor.
∙ However, σ drops out of the relative standard errors. If one sequentially adds variables, one can
  check the variance (or sd) inflation for including each one.
∙ When the other regressors are actually orthogonal to the regressor of interest, then there is no
  variance inflation.
∙ The variance inflation factor (VIF) is the increase in the variance for the ith regressor compared to
  the ideal setting where it is orthogonal to the other regressors.
      ­ (The square root of the VIF is the increase in the sd ...)
∙ Remember, variance inflation is only part of the picture. We want to include certain variables, even
  if they dramatically inflate our variance.
                                                                                                    8/14
 
 Revisting our previous simulation
##doesn't depend on which y you use,
y <- x1 + rnorm(n, sd = .3)
a <- summary(lm(y ~ x1))$cov.unscaled[2,2]
c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],
  summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a
[1] 1.895 9.948
temp <- apply(betas, 1, var); temp[2 : 3] / temp[1]
   x1    x1
1.860 9.506
                                                      9/14
 
 Swiss data
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
  c(summary(fit2)$cov.unscaled[2,2],
    summary(fit3)$cov.unscaled[2,2]) / a
[1] 1.892 2.089
                                                                       10/14
 
 Swiss data VIFs,
library(car)
fit <- lm(Fertility ~ . , data = swiss)
vif(fit)
     Agriculture      Examination       Education Catholic Infant.Mortality
           2.284            3.675           2.775    1.937            1.108
sqrt(vif(fit)) #I prefer sd
     Agriculture      Examination       Education Catholic Infant.Mortality
           1.511            1.917           1.666    1.392            1.052
                                                                            11/14
 
 What about residual variance estimation?
∙ Assuming that the model is linear with additive iid errors (with finite variance), we can
  mathematically describe the impact of omitting necessary variables or including unnecessary ones.
    ­ If we underfit the model, the variance estimate is biased.
    ­ If we correctly or overfit the model, including all necessary covariates and/or unnecessary
      covariates, the variance estimate is unbiased.
    ­ However, the variance of the variance is larger if we include unnecessary variables.
                                                                                                12/14
 
 Covariate model selection
∙ Automated covariate selection is a difficult topic. It depends heavily on how rich of a covariate
  space one wants to explore.
     ­ The space of models explodes quickly as you add interactions and polynomial terms.
∙ In the prediction class, we'll cover many modern methods for traversing large model spaces for the
  purposes of prediction.
∙ Principal components or factor analytic models on covariates are often useful for reducing complex
  covariate spaces.
∙ Good design can often eliminate the need for complex model searches at analyses; though often
  control over the design is limited.
∙ If the models of interest are nested and without lots of parameters differentiating them, it's fairly
  uncontroversial to use nested likelihood ratio tests. (Example to follow.)
∙ My favoriate approach is as follows. Given a coefficient that I'm interested in, I like to use covariate
  adjustment and multiple models to probe that effect to evaluate it for robustness and to see what
  other covariates knock it out. This isn't a terribly systematic approach, but it tends to teach you a lot
  about the the data as you get your hands dirty.
                                                                                                      13/14
 
 How to do nested model testing in R
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
fit5 <- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)
anova(fit1, fit3, fit5)
Analysis of Variance Table
Model 1: Fertility ~ Agriculture
Model 2: Fertility ~ Agriculture + Examination + Education
Model 3: Fertility ~ Agriculture + Examination + Education + Catholic +
    Infant.Mortality
  Res.Df RSS Df Sum of Sq     F Pr(>F)
1     45 6283
2     43 3181 2       3102 30.2 8.6e-09 ***
3     41 2105 2       1076 10.5 0.00021 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                                                                                             14/14
"
"./07_RegressionModels/pdfs/03_01.pdf","Generalized linear models
Regression Models
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Linear models
∙ Linear models are the most useful applied statistical technique. However, they are not without their
  limitations.
     ­ Additive response models don't make much sense if the response is discrete, or stricly
        positive.
     ­ Additive error models often don't make sense, for example if the outcome has to be positive.
     ­ Transformations are often hard to interpret.
     ­ There's value in modeling the data on the scale that it was collected.
     ­ Particularly interpetable transformations, natural logarithms in specific, aren't applicable for
        negative or zero values.
                                                                                                     2/9
 
 Generalized linear models
∙ Introduced in a 1972 RSSB paper by Nelder and Wedderburn.
∙ Involves three components
     ­ An exponential family model for the response.
     ­ A systematic component via a linear predictor.
     ­ A link function that connects the means of the response to the linear predictor.
                                                                                        3/9
 
 Example, linear models
∙ Assume that Yi ∼ N( μi , σ 2 ) (the Gaussian distribution is an exponential family distribution.)
                                            p
∙ Define the linear predictor to be η i = ∑k=1 Xik β k .
∙ The link function as g so that g(μ) = η .
    ­ For linear models g(μ) = μ so that μi = η i
∙ This yields the same likelihood model as our additive error Gaussian linear model
                                                    p
                                              Yi =      Xik β k + ϵi
                                                   ∑
                                                   k=1
            iid
  where ϵi ∼ N(0, σ 2 )
                                                                                                    4/9
 
 Example, logistic regression
∙ Assume that Yi ∼ Bernoulli( μi ) so that E[ Yi ] = μi where 0 ≤ μi ≤ 1 .
                          p
∙ Linear predictor η i = ∑k=1 Xik β k
                                  μ
∙ Link function g(μ) = η = log 1−μ       g is the (natural) log odds, referred to as the logit.
                               (      )
∙ Note then we can invert the logit function as
                                         exp(η i)                              1
                               μi =                     and 1 − μi =
                                       1 + exp(ηi )                       1 + exp(ηi )
  Thus the likelihood is
                              n                               n            n
                                   y
                                 μi i (1 − μi) 1−y i = exp       yi η i       (1 + ηi )−1
                             ∏                              ∑             ∏
                                                           (            )
                             i=1                             i=1          i=1
                                                                                                5/9
 
 Example, Poisson regression
∙ Assume that Yi ∼ Poisson(μi) so that E[Yi ] = μi where 0 ≤ μi
                           p
∙ Linear predictor η i = ∑k=1 Xik β k
∙ Link function g(μ) = η = log(μ)
∙ Recall that ex is the inverse of log(x) so that
                                                        μi = eηi
  Thus, the likelihood is
                                  n                               n           n
                                               y
                                     (yi !)−1 μi i e−μi ∝ exp        yi ηi −     μi
                                 ∏                               ∑           ∑
                                                              (                     )
                                 i=1                             i=1         i=1
                                                                                      6/9
 
 Some things to note
∙ In each case, the only way in which the likelihood depends on the data is through
                                  n           n       p             p      n
                                     yi ηi =     yi     Xik β k =      βk     Xik yi
                                ∑            ∑      ∑              ∑      ∑
                                 i=1         i=1    k=1            k=1    i=1
                                                  n
  Thus if we don't need the full data, only ∑i=1 Xik yi . This simplification is a consequence of chosing
  so­called 'canonical' link functions.
∙ (This has to be derived). All models acheive their maximum at the root of the so called normal
  equations
                                                     n
                                                        (Yi − μi )
                                               0=                   Wi
                                                    ∑
                                                    i=1
                                                         Var( Yi)
  where W i are the derivative of the inverse of the link function.
                                                                                                       7/9
 
 About variances
                                                  n
                                                      (Yi − μi )
                                            0=                   Wi
                                                ∑
                                                 i=1
                                                       Var(Yi )
∙ For the linear model Var( Yi ) = σ 2 is constant.
∙ For Bernoulli case Var( Yi ) = μi (1 − μi )
∙ For the Poisson case Var( Yi ) = μi .
∙ In the latter cases, it is often relevant to have a more flexible variance model, even if it doesn't
  correspond to an actual likelihood
                                  n                                  n
                                      (Yi − μi )                        (Yi − μi )
                            0=                       W i and 0 =                   Wi
                                ∑
                                 i=1
                                     ϕμi(1 − μi )                   ∑
                                                                    i=1
                                                                          ϕμ i
∙ These are called 'quasi­likelihood' normal equations
                                                                                                   8/9
 
 Odds and ends
∙ The normal equations have to be solved iteratively. Resulting in β̂ k and, if included, ϕ̂ .
                                                                 p
∙ Predicted linear predictor responses can be obtained as η̂ = ∑k=1 Xk β̂ k
∙ Predicted mean responses as μ̂ = g−1 (η̂)
∙ Coefficients are interpretted as
                 g(E[Y| Xk = xk + 1, X∼k = x ∼k ]) − g(E[Y|Xk = x k , X∼k = x∼k ]) = β k
  or the change in the link function of the expected response per unit change in Xk holding other
  regressors constant.
∙ Variations on Newon/Raphson's algorithm are used to do it.
∙ Asymptotics are used for inference usually.
∙ Many of the ideas from linear models can be brought over to GLMs.
                                                                                               9/9
"
"./07_RegressionModels/pdfs/03_02.pdf","Generalized linear models, binary data
Regression models
Brian Caffo, Jeff Leek and Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Key ideas
∙ Frequently we care about outcomes that have two values
    ­ Alive/dead
    ­ Win/loss
    ­ Success/Failure
    ­ etc
∙ Called binary, Bernoulli or 0/1 outcomes
∙ Collection of exchangeable binary outcomes for the same covariate data are called binomial
  outcomes.
                                                                                        2/16
 
 Example Baltimore Ravens win/loss
Ravens Data
 download.file(""https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda""
               , destfile=""./data/ravensData.rda"",method=""curl"")
 load(""./data/ravensData.rda"")
 head(ravensData)
   ravenWinNum ravenWin ravenScore opponentScore
 1           1        W         24             9
 2           1        W         38            35
 3           1        W         28            13
 4           1        W         34            31
 5           1        W         44            13
 6           0        L         23            24
                                                                                 3/16
 
 Linear regression
                                          RW i = b0 + b1 RSi + ei
RW i ­ 1 if a Ravens win, 0 if not
RS i ­ Number of points Ravens scored
b0 ­ probability of a Ravens win if they score 0 points
b1 ­ increase in probability of a Ravens win for each additional point
ei ­ residual variation due
                                                                       4/16
 
 Linear regression in R
lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef
                      Estimate Std. Error t value Pr(>|t|)
(Intercept)             0.2850 0.256643 1.111 0.28135
ravensData$ravenScore 0.0159 0.009059 1.755 0.09625
                                                               5/16
 
 Odds
Binary Outcome 0/1
                                RW i
Probability (0,1)
                         Pr(RWi |RSi, b0 , b1 )
Odds (0, ∞)
                         Pr(RWi |RSi, b0 , b1 )
                       1 − Pr(RWi |RSi , b0 , b1 )
Log odds (−∞, ∞)
                          Pr(RWi |RSi , b0 , b1 )
                   log
                      ( 1 − Pr(RWi |RSi , b0 , b1 ) )
                                                      6/16
 
 Linear vs. logistic regression
Linear
                          RW i = b0 + b1 RSi + ei
or
                     E[RW i |RSi , b0 , b1 ] = b0 + b1 RS i
Logistic
                                             exp(b0 + b1 RSi )
                Pr(RWi |RSi , b0 , b1 ) =
                                          1 + exp(b0 + b1 RSi)
or
                      Pr(RWi |RSi , b0 , b1 )
               log                                 = b0 + b1 RS i
                  ( 1 − Pr(RWi |RSi , b0 , b1 ) )
                                                                  7/16
 
 Interpreting Logistic Regression
                                       Pr(RWi |RSi , b0 , b1 )
                               log                                = b0 + b1 RS i
                                  ( 1 − Pr(RWi |RSi , b0 , b1 ) )
b0 ­ Log odds of a Ravens win if they score zero points
b1 ­ Log odds ratio of win probability for each point scored (compared to zero points)
exp(b1 ) ­ Odds ratio of win probability for each point scored (compared to zero points)
                                                                                         8/16
 
 Odds
∙ Imagine that you are playing a game where you flip a coin with success probability p.
∙ If it comes up heads, you win X . If it comes up tails, you lose Y .
∙ What should we set X and Y for the game to be fair?
                                      E[earnings] = Xp − Y(1 − p) = 0
∙ Implies
                                                  Y      p
                                                     =
                                                  X    1−p
∙ The odds can be said as ""How much should you be willing to pay for a p probability of winning a
  dollar?""
      ­ (If p > 0.5 you have to pay more if you lose than you get if you win.)
      ­ (If p < 0.5 you have to pay less if you lose than you get if you win.)
                                                                                              9/16
 
 Visualizing fitting logistic regression curves
x <- seq(-10, 10, length = 1000)
manipulate(
    plot(x, exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x)),
         type = ""l"", lwd = 3, frame = FALSE),
    beta1 = slider(-2, 2, step = .1, initial = 2),
    beta0 = slider(-2, 2, step = .1, initial = 0)
    )
                                                                   10/16
 
 Ravens logistic regression
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family=""binomial"")
summary(logRegRavens)
Call:
glm(formula = ravensData$ravenWinNum ~ ravensData$ravenScore,
    family = ""binomial"")
Deviance Residuals:
   Min      1Q Median       3Q     Max
-1.758 -1.100 0.530      0.806   1.495
Coefficients:
                      Estimate Std. Error z value Pr(>|z|)
(Intercept)            -1.6800     1.5541 -1.08       0.28
ravensData$ravenScore 0.1066       0.0667    1.60     0.11
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 24.435 on 19 degrees of freedom
Residual deviance: 20.895 on 18 degrees of freedom
AIC: 24.89                                                                            11/16
 
 Ravens fitted values
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col=""blue"",xlab=""Score"",ylab=""Prob Ravens Win"")
                                                                                             12/16
 
 Odds ratios and confidence intervals
exp(logRegRavens$coeff)
          (Intercept) ravensData$ravenScore
               0.1864                1.1125
exp(confint(logRegRavens))
                         2.5 % 97.5 %
(Intercept)           0.005675 3.106
ravensData$ravenScore 0.996230 1.303
                                            13/16
 
 ANOVA for logistic regression
anova(logRegRavens,test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: ravensData$ravenWinNum
Terms added sequentially (first to last)
                      Df Deviance Resid. Df Resid. Dev Pr(>Chi)
NULL                                     19       24.4
ravensData$ravenScore 1      3.54        18       20.9     0.06 .
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
                                                                  14/16
 
 Interpreting Odds Ratios
 ∙ Not probabilities
 ∙ Odds ratio of 1 = no difference in odds
 ∙ Log odds ratio of 0 = no difference in odds
 ∙ Odds ratio < 0.5 or > 2 commonly a ""moderate effect""
                 Pr(RWi |RSi =10)
 ∙ Relative risk Pr(RWi |RSi =0)  often easier to interpret, harder to estimate
 ∙ For small probabilities RR ≈ OR but they are not the same!
Wikipedia on Odds Ratio
                                                                                15/16
 
 Further resources
∙ Wikipedia on Logistic Regression
∙ Logistic regression and glms in R
∙ Brian Caffo's lecture notes on: Simpson's paradox, Case­control studies
∙ Open Intro Chapter on Logistic Regression
                                                                          16/16
"
"./07_RegressionModels/pdfs/03_03.pdf","Count outcomes, Poisson GLMs
Regression Models
Brian Caffo, Jeffrey Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 Key ideas
∙ Many data take the form of counts
    ­ Calls to a call center
    ­ Number of flu cases in an area
    ­ Number of cars that cross a bridge
∙ Data may also be in the form of rates
    ­ Percent of children passing a test
    ­ Percent of hits to a website from a country
∙ Linear regression with transformation is an option
                                                     2/23
 
 Poisson distribution
∙ The Poisson distribution is a useful model for counts and rates
∙ Here a rate is count per some monitoring time
∙ Some examples uses of the Poisson distribution
    ­ Modeling web traffic hits
    ­ Incidence rates
    ­ Approximating binomial probabilities with small p and large n
    ­ Analyzing contigency table data
                                                                    3/23
 
 The Poisson mass function
∙ X ∼ Poisson(tλ) if
                                                     (tλ)x e−tλ
                                         P(X = x) =
                                                         x!
  For x = 0, 1, … .
∙ The mean of the Poisson is E[X] = tλ , thus E[X/t] = λ
∙ The variance of the Poisson is Var(X) = tλ .
∙ The Poisson tends to a normal as tλ gets large.
                                                                4/23
 
 par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = ""h"", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = ""h"", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = ""h"", frame = FALSE)
                                                                       5/23
 
 Poisson distribution
Sort of, showing that the mean and variance are equal
 x <- 0 : 10000; lambda = 3
 mu <- sum(x * dpois(x, lambda = lambda))
 sigmasq <- sum((x - mu)^2 * dpois(x, lambda = lambda))
 c(mu, sigmasq)
 [1] 3 3
                                                        6/23
 
 Example: Leek Group Website Traffic
 ∙ Consider the daily counts to Jeff Leek's web site
http://biostat.jhsph.edu/~jleek/
 ∙ Since the unit of time is always one day, set t = 1 and then the Poisson mean is interpretted as web
    hits per day. (If we set t = 24 , it would be web hits per hour).
                                                                                                    7/23
 
 Website data
 download.file(""https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda"",destfile=""./data/gaData.rda""
 load(""./data/gaData.rda"")
 gaData$julian <- julian(gaData$date)
 head(gaData)
           date visits simplystats julian
 1 2011-01-01        0           0 14975
 2 2011-01-02        0           0 14976
 3 2011-01-03        0           0 14977
 4 2011-01-04        0           0 14978
 5 2011-01-05        0           0 14979
 6 2011-01-06        0           0 14980
http://skardhamar.github.com/rga/
                                                                                               8/23
 
 Plot data
plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
                                                                                    9/23
 
 Linear regression
                                           NHi = b0 + b1 J Di + ei
NHi ­ number of hits to the website
J Di ­ day of the year (Julian day)
b0 ­ number of hits on Julian day 0 (1970­01­01)
b1 ­ increase in number of hits per unit day
ei ­ variation due to everything we didn't measure
                                                                   10/23
 
 Linear regression line
plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col=""red"",lwd=3)
                                                                                    11/23
 
 Aside, taking the log of the outcome
 ∙ Taking the natural log of the outcome has a specific interpretation.
 ∙ Consider the model
                                        log(N Hi) = b0 + b1 J Di + ei
NHi ­ number of hits to the website
J Di ­ day of the year (Julian day)
b0 ­ log number of hits on Julian day 0 (1970­01­01)
b1 ­ increase in log number of hits per unit day
ei ­ variation due to everything we didn't measure
                                                                        12/23
 
 Exponentiating coefficients
∙ eE[log(Y)] geometric mean of Y .
                                                  1   n
                                                                       n
     ­ With no covariates, this is estimated by e n ∑ i=1 log(y i) = (∏i=1 yi )1/n
∙ When you take the natural log of outcomes and fit a regression model, your exponentiated
  coefficients estimate things about geometric means.
∙ eβ 0 estimated geometric mean hits on day 0
∙ eβ 1 estimated relative increase or decrease in geometric mean hits per day
∙ There's a problem with logs with you have zero counts, adding a constant works
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
  (Intercept) gaData$julian
          0.000          1.002
                                                                                      13/23
 
 Linear vs. Poisson regression
Linear
                          NHi = b0 + b1 J Di + ei
or
                      E[NHi |J Di , b0 , b1 ] = b0 + b1 J Di
Poisson/log­linear
                   log(E[NHi |JDi , b0 , b1 ]) = b0 + b1 J Di
or
                   E[NHi |J Di, b0 , b1 ] = exp(b0 + b1 J Di )
                                                               14/23
 
 Multiplicative differences
                                    E[NHi |J Di, b0 , b1 ] = exp(b0 + b1 J Di )
                                  E[NHi |J Di , b0 , b1 ] = exp(b0 ) exp(b1 J Di )
If J Di is increased by one unit, E[NHi |J Di, b0 , b1 ] is multiplied by exp(b1 )
                                                                                   15/23
 
 Poisson regression in R
plot(gaData$julian,gaData$visits,pch=19,col=""darkgrey"",xlab=""Julian"",ylab=""Visits"")
glm1 <- glm(gaData$visits ~ gaData$julian,family=""poisson"")
abline(lm1,col=""red"",lwd=3); lines(gaData$julian,glm1$fitted,col=""blue"",lwd=3)
                                                                                    16/23
 
 Mean-variance relationship?
plot(glm1$fitted,glm1$residuals,pch=19,col=""grey"",ylab=""Residuals"",xlab=""Fitted"")
                                                                                  17/23
 
 Model agnostic standard errors
 library(sandwich)
 confint.agnostic <- function (object, parm, level = 0.95, ...)
 {
       cf <- coef(object); pnames <- names(cf)
       if (missing(parm))
           parm <- pnames
       else if (is.numeric(parm))
           parm <- pnames[parm]
       a <- (1 - level)/2; a <- c(a, 1 - a)
       pct <- stats:::format.perc(a, 3)
       fac <- qnorm(a)
       ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                                  pct))
       ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
       ci[] <- cf[parm] + ses %o% fac
       ci
 }
http://stackoverflow.com/questions/3817182/vcovhc­and­confidence­interval
                                                                          18/23
 
 Estimating confidence intervals
confint(glm1)
                  2.5 %   97.5 %
(Intercept) -34.34658 -31.159716
gaData$julian 0.00219 0.002396
confint.agnostic(glm1)
                   2.5 %   97.5 %
(Intercept) -36.362675 -29.136997
gaData$julian 0.002058 0.002528
                                  19/23
 
 Rates
         E[NHSSi |JDi , b0 , b1 ]/NHi = exp(b0 + b1 J Di )
      log(E[NHSSi |JDi , b0 , b1 ]) − log(NHi ) = b0 + b1 J Di
      log(E[NHSSi |JDi , b0 , b1 ]) = log(NHi ) + b0 + b1 J Di
                                                               20/23
 
 Fitting rates in R
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family=""poisson"",data=gaData)
plot(julian(gaData$date),glm2$fitted,col=""blue"",pch=19,xlab=""Date"",ylab=""Fitted Counts"")
points(julian(gaData$date),glm1$fitted,col=""red"",pch=19)
                                                                                         21/23
 
 Fitting rates in R
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family=""poisson"",data=gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col=""grey"",xlab=""Date"",
     ylab=""Fitted Rates"",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col=""blue"",lwd=3)
                                                                                      22/23
 
 More information
∙ Log­linear models and multiway tables
∙ Wikipedia on Poisson regression, Wikipedia on overdispersion
∙ Regression models for count data in R
∙ pscl package ­ the function zeroinfl fits zero inflated models.
                                                                  23/23
"
"./07_RegressionModels/pdfs/03_04.pdf","Hodgepodge
Regression models
Brian Caffo, Jeff Leek, Roger Peng
Johns Hopkins Bloomberg School of Public Health
 
 How to fit functions using linear models
∙ Consider a model Yi = f ( Xi ) + ϵ .
∙ How can we fit such a model using linear models (called scatterplot smoothing)
∙ Consider the model
                                                       d
                                  Yi = β 0 + β 1 Xi +     (xi − ξk )+ γ k + ϵi
                                                      ∑
                                                      k=1
  where (a)+ = a if a > 0 and 0 otherwise and ξ1 ≤. . . ≤ ξd are known knot points.
∙ Prove to yourelf that the mean function
                                                       d
                                       β 0 + β 1 Xi +     (x i − ξ k ) + γ k
                                                      ∑
                                                      k=1
  is continuous at the knot points.
                                                                                    2/9
 
 Simulated example
n <- 500; x <- seq(0, 4 * pi, length = n); y <- sin(x) + rnorm(n, sd = .3)
knots <- seq(0, 8 * pi, length = 20);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = ""lightblue"", cex = 2)
lines(x, yhat, col = ""red"", lwd = 2)
                                                                           3/9
 
 Adding squared terms
∙ Adding squared terms makes it continuously differentiable at the knot points.
∙ Adding cubic terms makes it twice continuously differentiable at the knot points; etcetera.
                                                           d
                           Yi =  β 0 + β 1 Xi + β 2 Xi2 +     (x i − ξk )2+ γk + ϵi
                                                          ∑
                                                          k=1
                                                                                              4/9
 
 splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot)^2)
xMat <- cbind(1, x, x^2, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = ""lightblue"", cex = 2)
lines(x, yhat, col = ""red"", lwd = 2)
                                                                       5/9
 
 Notes
∙ The collection of regressors is called a basis.
    ­ People have spent a lot of time thinking about bases for this kind of problem. So, consider this
       as just a teaser.
∙ Single knot point terms can fit hockey stick like processes.
∙ These bases can be used in GLMs as well.
∙ An issue with these approaches is the large number of parameters introduced.
    ­ Requires some method of so called regularization.
                                                                                                   6/9
 
 Harmonics using linear models
##Chord finder, playing the white keys on a piano from octave c4 - c5
notes4 <- c(261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25)
t <- seq(0, 2, by = .001); n <- length(t)
c4 <- sin(2 * pi * notes4[1] * t); e4 <- sin(2 * pi * notes4[3] * t);
g4 <- sin(2 * pi * notes4[5] * t)
chord <- c4 + e4 + g4 + rnorm(n, 0, 0.3)
x <- sapply(notes4, function(freq) sin(2 * pi * freq * t))
fit <- lm(chord ~ x - 1)
                                                                            7/9
 
 8/9 
 ##(How you would really do it)
a <- fft(chord); plot(Re(a)^2, type = ""l"")
                                           9/9
"
"./08_PracticalMachineLearning/025combiningPredictors/Combining predictors.pdf","9/3/13                                                                                                  Combining predictors
                            Combining predictors
                            Jeffrey Leek, Assistant Professor of Biostatistics
                            Johns Hopkins Bloomberg School of Public Health
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                  1/13
 
 9/3/13                                                                                                  Combining predictors
              Key ideas
                 · You can combine classifiers by averaging/voting
                 · Combining classifiers improves accuracy
                 · Combining classifiers reduces interpretability
                                                                                                                             2/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       2/13
 
 9/3/13                                                                                                  Combining predictors
              Netflix prize
              BellKor = Combination of 107 predictors
              http://www.netflixprize.com//leaderboard
                                                                                                                             3/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       3/13
 
 9/3/13                                                                                                  Combining predictors
              Heritage health prize - Progress Prize 1
              Market Makers
              Mestrom
                                                                                                                             4/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       4/13
 
 9/3/13                                                                                                  Combining predictors
              Basic intuition - majority vote
              Suppose we have 5 completely independent classifiers
              If accuracy is 70% for each:
                                       3           2                     4        2                5
                 ·  10 × (0.7 ) (0.3)                 + 5 × (0.7 ) (0.3 )            + (0.7 )
                 · 83.7% majority vote accuracy
              With 101 independent classifiers
                 · 99.9% majority vote accuracy
                                                                                                                             5/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       5/13
 
 9/3/13                                                                                                  Combining predictors
              Approaches for combining classifiers
                   1. Bagging (see previous lecture)
                   2. Boosting
                   3. Combining different classifiers
                                                                                                                             6/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       6/13
 
 9/3/13                                                                                                  Combining predictors
              Example
                 #library(devtools)
                 #install_github(""medley"",""mewo2"")
                 library(medley)
                 set.seed(453234)
                 y <- rnorm(1000)
                 x1 <- (y > 0); x2 <- y*rnorm(1000)
                 x3 <- rnorm(1000,mean=y,sd=1); x4 <- (y > 0) & (y < 3)
                 x5 <- rbinom(1000,size=4,prob=exp(y)/(1+exp(y)))
                 x6 <- (y < -2) | (y > 2)
                 data <- data.frame(y=y,x1=x1,x2=x2,x3=x3,x4=x4,x5=x5,x6=x6)
                 train <- sample(1:1000,size=500)
                 trainData <- data[train,]; testData <- data[-train,]
                                                                                                                             7/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       7/13
 
 9/3/13                                                                                                  Combining predictors
              Basic models
                 library(tree)
                 lm1 <- lm(y ~.,data=trainData)
                 rmse(predict(lm1,data=testData),testData$y)
                 tree1 <- tree(y ~.,data=trainData)
                 rmse(predict(tree1,data=testData),testData$y)
                 tree2 <- tree(y~.,data=trainData[sample(1:dim(trainData)[1]),])
                                                                                                                             8/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       8/13
 
 9/3/13                                                                                                  Combining predictors
              Combining models
                 combine1 <- predict(lm1,data=testData)/2 + predict(tree1,data=testData)/2
                 rmse(combine1,testData$y)
                 [1] 1.281
                 combine2 <- (predict(lm1,data=testData)/3 + predict(tree1,data=testData)/3
                                           + predict(tree2,data=testData)/3)
                 rmse(combine2,testData$y)
                 [1] 1.175
                                                                                                                             9/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                       9/13
 
 9/3/13                                                                                                  Combining predictors
              Medley package
                 #library(devtools)
                 #install_github(""medley"",""mewo2"")
                 library(medley)
                 library(e1071)
                 library(randomForests)
                 x <- trainData[,-1]
                 y <- trainData$y
                 newx <- testData[,-1]
              http://www.kaggle.com/users/10748/martin-o-leary
                                                                                                                             10/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                        10/13
 
 9/3/13                                                                                                  Combining predictors
              Blending models (part 1)
                 m <- create.medley(x, y, errfunc=rmse);
                 for (g in 1:10) {
                    m <- add.medley(m, svm, list(gamma=1e-3 * g));
                 }
                                                                                                                             11/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                        11/13
 
 9/3/13                                                                                                  Combining predictors
              Blending models (part 2)
                 for (mt in 1:2) {
                    m <- add.medley(m, randomForest, list(mtry=mt));
                 }
                 m <- prune.medley(m, 0.8);
                 rmse(predict(m,newx),testData$y)
                 Sampled... 96.00 %: 1 svm (gamma = 0.01)
                 1.00 %: 2 svm (gamma = 0.009)
                 1.00 %: 5 svm (gamma = 0.006)
                 1.00 %: 6 svm (gamma = 0.005)
                 1.00 %: 7 svm (gamma = 0.004)
                 CV error: 0.4956
                 [1] 0.4694
                                                                                                                             12/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                        12/13
 
 9/3/13                                                                                                  Combining predictors
              Notes and further resources
              Notes:
                 · Even simple blending can be useful
                 · Majority vote is typical model for binary/multiclass data
                 · Makes models hard to interpret
              Further resources:
                 · Bayesian model averaging
                 · Heritage health prize
                 · Netflix model blending
                                                                                                                             13/13
file://localhost/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week7/004combiningPredictors/index.html#1                        13/13
"
"./08_PracticalMachineLearning/lectures/001predictionMotivation.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/002whatIsPrediction.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/003relativeImportance.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/004inOutSampleErrors.pdf"," 
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/005predictionStudyDesign.pdf"," 
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/006typesOfErrors.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/007receiverOperatingCharacteristic.pdf"," 
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/008crossValidation.pdf"," 
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/009whatData.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/010caretPackage.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/011dataSlicing.pdf"," 
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/012trainOptions.pdf"," 
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/013plottingPredictors.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/014basicPreprocessing.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/015covariateCreation.pdf"," 
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/016preProcessingPCA.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/017predictingWithRegression.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/018predictingWithRegressionMC.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/019predictingWithTrees.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/020bagging.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/021randomForests.pdf"," 
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/022boosting.pdf"," 
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/023modelBasedPrediction.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/024regularizedRegression.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/025combiningPredictors.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/026unsupervisedPrediction.pdf"," 
  
  
  
  
  
  
 "
"./08_PracticalMachineLearning/lectures/027forecasting.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/classes-methods.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/googleVis.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/manipulate.pdf"," 
  
  
 "
"./09_DevelopingDataProducts/lectures/rCharts.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/rgl.pdf"," 
  
 "
"./09_DevelopingDataProducts/lectures/rMaps.pdf"," 
  
  
  
 "
"./09_DevelopingDataProducts/lectures/RPackages.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/shiny.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/shiny2.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
"./09_DevelopingDataProducts/lectures/slidify.pdf"," 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 "
